<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=WINDOWS-1252">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: ethics</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: ethics">
<meta name="Date" content="2004-05-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: ethics</h1>
<!-- received="Thu May 20 00:19:33 2004" -->
<!-- isoreceived="20040520061933" -->
<!-- sent="Wed, 19 May 2004 23:19:30 -0700" -->
<!-- isosent="20040520061930" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: ethics" -->
<!-- id="A7974A32-AA25-11D8-86D4-000A95B1AFDE@objectent.com" -->
<!-- charset="WINDOWS-1252" -->
<!-- inreplyto="40ABDA29.5050303@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20ethics"><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Thu May 20 2004 - 00:19:30 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8598.html">Samantha Atkins: "Re: ethics"</a>
<li><strong>Previous message:</strong> <a href="8596.html">Samantha Atkins: "Re: ethics"</a>
<li><strong>In reply to:</strong> <a href="8586.html">Eliezer S. Yudkowsky: "Re: ethics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8599.html">Marc Geddes: "Fascinating, fascinating"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8597">[ date ]</a>
<a href="index.html#8597">[ thread ]</a>
<a href="subject.html#8597">[ subject ]</a>
<a href="author.html#8597">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On May 19, 2004, at 3:05 PM, Eliezer S. Yudkowsky wrote:
<br>
<p><em>&gt; fudley wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; On Wed, 19 May 2004 01:42:12 -0400, &quot;Eliezer S. Yudkowsky&quot;
</em><br>
<em>&gt;&gt; &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20ethics">sentience@pobox.com</a>&gt; said:
</em><br>
<em>&gt;&gt;&gt; The problem is:  Sea Slugs can't do abstract reasoning!
</em><br>
<em>&gt;&gt; Well, Sea Slugs can respond to simple external stimuli but I agree 
</em><br>
<em>&gt;&gt; they
</em><br>
<em>&gt;&gt; have no understanding of Human Beings, just as a Human Being can have 
</em><br>
<em>&gt;&gt; no
</em><br>
<em>&gt;&gt; understanding of the psychology of a being with the brain the size of 
</em><br>
<em>&gt;&gt; a
</em><br>
<em>&gt;&gt; planet.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This was my ancient argument, and it turned out to be a flawed 
</em><br>
<em>&gt; metaphor - the rule simply doesn't carry over.  If you have no 
</em><br>
<em>&gt; understanding of the psychology of a being with the brain the size of 
</em><br>
<em>&gt; a planet, how do you know that no human can understand its psychology?
</em><br>
<p>Because of what we do know about the limits of human cognitive 
<br>
abilities?
<br>
<p><p><em>&gt;  This sounds like a flip question, but it's not; it's the source of my 
</em><br>
<em>&gt; original mistake - I tried to reason about the incomprehensibility of 
</em><br>
<em>&gt; superintelligence without understanding where the incomprehensibility 
</em><br>
<em>&gt; came from, or why.  Think of all the analogies from the history of 
</em><br>
<em>&gt; science; if something is a mystery to you, you do not know enough to 
</em><br>
<em>&gt; claim that science will never comprehend it.  I was foolish to make 
</em><br>
<em>&gt; statements about the incomprehensibility of intelligence before I 
</em><br>
<em>&gt; understood intelligence.
</em><br>
<em>&gt;
</em><br>
<p>You have also made statements that I have not seen you repudiate about 
<br>
the necessity of a sufficiently powerful intelligence to tackle various 
<br>
sizes of problems.   I have not yet seen an argument that makes be 
<br>
believe that fully dependably limiting the choice space of a SAI to 
<br>
preserve human existence is within the range of human mental abilities. 
<br>
&nbsp;&nbsp;For that matter, I have no strong reason to believe the problem is 
<br>
solvable in principle.
<br>
<p><p><em>&gt; Now I understand intelligence better, which is why I talk about 
</em><br>
<em>&gt; &quot;optimization processes&quot; rather than &quot;intelligence&quot;.
</em><br>
<em>&gt;
</em><br>
<p>But doesn't this sort of talk only work if the supergoal remains 
<br>
invariant and is not subject to reflection and possible &quot;optimization&quot; 
<br>
itself?   Is this a form of begging the question?
<br>
<p><em>&gt; The human ability to employ abstract reasoning is a threshold effect 
</em><br>
<em>&gt; that *potentially* enables a human to fully understand some 
</em><br>
<em>&gt; optimization processes, including, I think, optimization processes 
</em><br>
<em>&gt; with arbitrarily large amounts of computing power.
</em><br>
<p>Precisely.  &quot;I think&quot;, not &quot;I know&quot;.
<br>
<p><p><em>&gt; That is only *some* optimization processes, processes that flow within 
</em><br>
<em>&gt; persistent, humanly understandable invariants; others will be as 
</em><br>
<em>&gt; unpredictable as coinflips.
</em><br>
<em>&gt;
</em><br>
<p>Yes, good..
<br>
<p><em>&gt; Imagine a computer program that outputs the prime factorization of 
</em><br>
<em>&gt; large numbers.  For large enough numbers, the actual execution of the 
</em><br>
<em>&gt; program flow is not humanly visualizable, even in principle.  But we 
</em><br>
<em>&gt; can still understand an abstract property of the program, which is 
</em><br>
<em>&gt; that it outputs a set of primes that multiply together to yield the 
</em><br>
<em>&gt; input number.
</em><br>
<em>&gt;
</em><br>
<p>This is a grossly inadequate analogy to producing a super-intelligent 
<br>
mind.
<br>
<p><em>&gt; Now imagine a program that writes a program that outputs the prime 
</em><br>
<em>&gt; factorization of large numbers.  This is a more subtle problem, 
</em><br>
<em>&gt; because there's a more complex definition of utility involved - we are 
</em><br>
<em>&gt; looking for a fast program, and a program that doesn't crash or cause 
</em><br>
<em>&gt; other negative side effects, such as overwriting other programs' 
</em><br>
<em>&gt; memory.  But I think it's possible to build out an FAI dynamic that 
</em><br>
<em>&gt; reads out the complete set of side effects you care about.  More 
</em><br>
<em>&gt; simply, you could use deductive reasoning processes that guarantee no 
</em><br>
<em>&gt; side effects.  (Sandboxing a Java program generated by directed 
</em><br>
<em>&gt; evolution is bad, because you're directing enormous search power 
</em><br>
<em>&gt; toward finding a flaw in the sandboxing!)  Again, the exact form of 
</em><br>
<em>&gt; the generated program would be unpredictable to humans, but its effect 
</em><br>
<em>&gt; would be predictable from understanding the optimization criteria of 
</em><br>
<em>&gt; the generator; a fast, reliable factorizer with no side effects.
</em><br>
<em>&gt;
</em><br>
<p>The analogy again breaks down.  You are not talking about creating 
<br>
algorithms that create other algorithms within design constraints.  You 
<br>
are talking of creating algorithms that become capable of creating 
<br>
algorithms for any purpose the algorithmic entity desires to any depth 
<br>
and with the entity capable of full examination and modification of the 
<br>
very roots of its &quot;desires&quot;.   Otherwise you are only creating 
<br>
something that super-efficiently optimizes a few goals you begin with.  
<br>
&nbsp;&nbsp;That you cannot predict its execution space means that this 
<br>
optimization may well produce results that are not what you wished that 
<br>
you did not think of beforehand and thus did not sufficiently instruct 
<br>
the system to self-detect and avoid.
<br>
<p><p><em>&gt; A program that writes a program that outputs the prime factorization 
</em><br>
<em>&gt; of large numbers is still understandable, and still not visualizable.
</em><br>
<p>The same would be true if you evolved such a program btw.
<br>
<p><em>&gt;
</em><br>
<em>&gt; The essential law of Friendly AI is that you cannot build an AI to 
</em><br>
<em>&gt; accomplish any end for which you do not possess a well-specified 
</em><br>
<em>&gt; *abstract* description.  If you want moral reasoning, or (my current 
</em><br>
<em>&gt; model) a dynamic that extrapolates human volitions including the 
</em><br>
<em>&gt; extrapolation of moral reasoning, then you need a well-specified 
</em><br>
<em>&gt; abstract description of what that looks like.
</em><br>
<em>&gt;
</em><br>
<p>Can the AI itself decide to build an AI for such an end?    Are you 
<br>
sure the finite space of &quot;abstract&quot; descriptions we can generate and 
<br>
verify includes the solutions we seek?  I'm not.  I guess what you say 
<br>
is true enough for some values of &quot;abstract&quot;.
<br>
<p><em>&gt; In summary:  You may not need to know the exact answer, but you need 
</em><br>
<em>&gt; to know an exact question.  The question may generate another 
</em><br>
<em>&gt; question, but you still need an exact original question.  And you need 
</em><br>
<em>&gt; to understand everything you build well enough to know that it answers 
</em><br>
<em>&gt; that question.
</em><br>
<p>Not always possible.  Sometimes, especially if the question is pressing 
<br>
enough, you build the best you can that may be able to answer the 
<br>
question even knowing that you cannot prove that it will.
<br>
<p><em>&gt;
</em><br>
<em>&gt;&gt;&gt; Thus making them impotent to control optimization processes such as 
</em><br>
<em>&gt;&gt;&gt; Humans, just like natural selection, which also can't do abstract 
</em><br>
<em>&gt;&gt;&gt; reasoning.
</em><br>
<em>&gt;&gt; But if the “optimization processes” can also do abstract reasoning 
</em><br>
<em>&gt;&gt; things
</em><br>
<em>&gt;&gt; become more interesting; it may reason out why it always rushes to 
</em><br>
<em>&gt;&gt; aid a
</em><br>
<em>&gt;&gt; Sea Slug in distress even at the risk of its own life, and it may 
</em><br>
<em>&gt;&gt; reason
</em><br>
<em>&gt;&gt; that this is not in its best interest, and it may look for a way to
</em><br>
<em>&gt;&gt; change things.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Don't put &quot;optimization processes&quot; in quotes, please.  Your question 
</em><br>
<em>&gt; involves putting yourself into an FAI's shoes, and the shoes don't 
</em><br>
<em>&gt; fit, any more than the shoes of natural selection would fit.  You may 
</em><br>
<em>&gt; be thinking that &quot;intelligences&quot; have self-centered &quot;best interests&quot;.  
</em><br>
<em>&gt; Rather than arguing about intelligence, I would prefer to talk about 
</em><br>
<em>&gt; optimization processes, which (as the case of natural selection 
</em><br>
<em>&gt; illustrates) do not even need to be anything that humans comprehend as 
</em><br>
<em>&gt; a mind, let alone possess self-centered best interests.
</em><br>
<em>&gt;
</em><br>
<p>All intelligences of the complexity we are concerned with to date do 
<br>
have self-centered interests.  It is an assertion that the FAI will be 
<br>
an intelligence that does not.   I am suspicious that either you can 
<br>
build such an intelligence that is not really all that intelligent 
<br>
because thus constrained or you can build a true mind with greater than 
<br>
human intelligence that may decide to be friendly to humans but cannot 
<br>
be coerced into being so.   You can give a  tendency to such a mind, 
<br>
but no more.   I don't believe anything less will be sufficient.
<br>
<p><em>&gt; Optimization processes direct futures into small targets in phase 
</em><br>
<em>&gt; space. A Sea-Slug-rescuing optimization process, say a Bayesian 
</em><br>
<em>&gt; decision system controlled by a utility function that assigns higher 
</em><br>
<em>&gt; utility to Sea Slugs out of distress than Sea Slugs in distress, 
</em><br>
<em>&gt; doesn't have a &quot;self&quot; or a &quot;best interest&quot; as you know it.  Put too 
</em><br>
<em>&gt; much power behind the optimization process, and unless it involves a 
</em><br>
<em>&gt; full solution to the underlying Friendly AI challenge, it may 
</em><br>
<em>&gt; overwrite the solar system with quintillions of tiny toy Sea Slugs, 
</em><br>
<em>&gt; just large enough to meet its criterion for &quot;undistressed Sea Slug&quot;, 
</em><br>
<em>&gt; and no larger.  But it still won't be &quot;acting in its own 
</em><br>
<em>&gt; self-interest&quot;.  That was just the final state the optimization 
</em><br>
<em>&gt; process happened to seek out, given the goal binding.  As for it being 
</em><br>
<em>&gt; unpredictable, why, look, here I am predicting it.  It's only 
</em><br>
<em>&gt; unpredictable if you close your eyes and walk directly into the 
</em><br>
<em>&gt; whirling razor blades.  This is a popular option, but not a 
</em><br>
<em>&gt; universally admired one.
</em><br>
<em>&gt;
</em><br>
<p>What you have described is imho precisely what is wrong with 
<br>
substituting &quot;optimization processes&quot; for intelligence.
<br>
<p><em>&gt;&gt;&gt; That part about &quot;Humans were never able to figure out a way to 
</em><br>
<em>&gt;&gt;&gt; overcome them&quot; was a hint, since it implies the Humans, as an 
</em><br>
<em>&gt;&gt;&gt; optimization process, were somehow led to expend computing power 
</em><br>
<em>&gt;&gt;&gt; specifically on searching for a pathway whose effect (from the Sea 
</em><br>
<em>&gt;&gt;&gt; Slugs' perspective) was to break the rules.
</em><br>
<em>&gt;&gt; The only thing that hint is telling you is that sometimes a hugely
</em><br>
<em>&gt;&gt; complicated program will behave in ways the programmer neither 
</em><br>
<em>&gt;&gt; expected
</em><br>
<em>&gt;&gt; or wanted, the more complex the program the more likely the surprise, 
</em><br>
<em>&gt;&gt; and
</em><br>
<em>&gt;&gt; we’re talking about a brain the size of a planet.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Humans weren't generated by computer programmers.  Our defiance of 
</em><br>
<em>&gt; evolution isn't an &quot;emergent&quot; result of &quot;complexity&quot;.  It's the result 
</em><br>
<em>&gt; of natural selection tending to generate psychological goals that 
</em><br>
<em>&gt; aren't the same as natural selection's fitness criterion.
</em><br>
<em>&gt;
</em><br>
<p>In other words it is a result of certain aspects of the complexities of 
<br>
our context leading to an unanticipated result.
<br>
<p><em>&gt; An FAI ain't a &quot;hugely complicated program&quot;, or at least, not as 
</em><br>
<em>&gt; programmers know it.  In the case of a *young* FAI, yeah, I expect 
</em><br>
<em>&gt; unanticipated behaviors, but I plan to detect them, and make sure that 
</em><br>
<em>&gt; not too much power goes into them.  In the case of a mature FAI, I 
</em><br>
<em>&gt; don't expect any behaviors the FAI doesn't anticipate.
</em><br>
<em>&gt;
</em><br>
<p>You plan to detect them all heh?   I do wish you luck.   I don't 
<br>
believe the task is tractable for anything that would have sufficient 
<br>
power to enable humans to survive Singularity.
<br>
<p><em>&gt; &quot;Emergence&quot; and &quot;complexity&quot; are explanations of maximum entropy; they 
</em><br>
<em>&gt; produce the illusion of explanation, yet are incapable of producing 
</em><br>
<em>&gt; any specific ante facto predictions.
</em><br>
<em>&gt;
</em><br>
<p>I am no expert on complexity theory.  But I don't believe this is 
<br>
correct.
<br>
<p>- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8598.html">Samantha Atkins: "Re: ethics"</a>
<li><strong>Previous message:</strong> <a href="8596.html">Samantha Atkins: "Re: ethics"</a>
<li><strong>In reply to:</strong> <a href="8586.html">Eliezer S. Yudkowsky: "Re: ethics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8599.html">Marc Geddes: "Fascinating, fascinating"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8597">[ date ]</a>
<a href="index.html#8597">[ thread ]</a>
<a href="subject.html#8597">[ subject ]</a>
<a href="author.html#8597">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
