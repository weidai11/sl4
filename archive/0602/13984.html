<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AGI thoughts was[AGI Reproduction? (Safety)]</title>
<meta name="Author" content="Rick Geniale (rickgeniale@pibot.com)">
<meta name="Subject" content="Re: AGI thoughts was[AGI Reproduction? (Safety)]">
<meta name="Date" content="2006-02-04">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AGI thoughts was[AGI Reproduction? (Safety)]</h1>
<!-- received="Sat Feb  4 06:41:18 2006" -->
<!-- isoreceived="20060204134118" -->
<!-- sent="Sat, 04 Feb 2006 14:43:42 +0100" -->
<!-- isosent="20060204134342" -->
<!-- name="Rick Geniale" -->
<!-- email="rickgeniale@pibot.com" -->
<!-- subject="Re: AGI thoughts was[AGI Reproduction? (Safety)]" -->
<!-- id="43E4AF8E.7090509@pibot.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="BAY108-F149147470D6F4221256690910C0@phx.gbl" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Rick Geniale (<a href="mailto:rickgeniale@pibot.com?Subject=Re:%20AGI%20thoughts%20was[AGI%20Reproduction?%20(Safety)]"><em>rickgeniale@pibot.com</em></a>)<br>
<strong>Date:</strong> Sat Feb 04 2006 - 06:43:42 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="13985.html">Rick Geniale: "Sorry. The country of sun and sea (not see)"</a>
<li><strong>Previous message:</strong> <a href="13983.html">P K: "Re: AGI thoughts was[AGI Reproduction? (Safety)]"</a>
<li><strong>In reply to:</strong> <a href="13983.html">P K: "Re: AGI thoughts was[AGI Reproduction? (Safety)]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13986.html">P K: "Re: AGI thoughts"</a>
<li><strong>Reply:</strong> <a href="13986.html">P K: "Re: AGI thoughts"</a>
<li><strong>Reply:</strong> <a href="13992.html">Charles D Hixson: "Re: AGI thoughts was[AGI Reproduction? (Safety)]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13984">[ date ]</a>
<a href="index.html#13984">[ thread ]</a>
<a href="subject.html#13984">[ subject ]</a>
<a href="author.html#13984">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
P K wrote:
<br>
<p><em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; From: &quot;nuzz604&quot; &lt;<a href="mailto:nuzz604@gmail.com?Subject=Re:%20AGI%20thoughts%20was[AGI%20Reproduction?%20(Safety)]">nuzz604@gmail.com</a>&gt;
</em><br>
<em>&gt;&gt; Reply-To: <a href="mailto:sl4@sl4.org?Subject=Re:%20AGI%20thoughts%20was[AGI%20Reproduction?%20(Safety)]">sl4@sl4.org</a>
</em><br>
<em>&gt;&gt; To: &lt;<a href="mailto:sl4@sl4.org?Subject=Re:%20AGI%20thoughts%20was[AGI%20Reproduction?%20(Safety)]">sl4@sl4.org</a>&gt;
</em><br>
<em>&gt;&gt; Subject: Re: AGI Reproduction? (Safety)
</em><br>
<em>&gt;&gt; Date: Fri, 3 Feb 2006 20:15:20 -0800
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; I understand that you have good intentions with AGI.  My worry 
</em><br>
<em>&gt;&gt; involves -accidentally- creating an unfriendly AGI (And this worry 
</em><br>
<em>&gt;&gt; applies to anybody who builds an AGI).  You can have good intentions 
</em><br>
<em>&gt;&gt; and still create an unfriendly AI because of some design flaw (even 
</em><br>
<em>&gt;&gt; if it appears to be a great design).
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; I am worried because no one really knows how a Seed AI will function 
</em><br>
<em>&gt;&gt; when it is turned on, and whether it will be friendly or not.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; The odds of randomly stumbling upon a working AGI are extremely small. 
</em><br>
<em>&gt; AGI programming will most likely be a very deliberate process. In 
</em><br>
<em>&gt; other words, if and when AGI is created, the builder(s) will most 
</em><br>
<em>&gt; likely know exactly what they are doing.
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; There are so many things that can go wrong.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yes, but for an AGI to work, allot of things would have to go right. 
</em><br>
<em>&gt; Why would builder(s) capable of overcoming the enormous technical 
</em><br>
<em>&gt; challenges of making a working AGI succeed on all the other points and 
</em><br>
<em>&gt; fail on that particular point; friendliness. I'm amazed at how, in SF, 
</em><br>
<em>&gt; AGI creators are smart enough to build it but give goal systems so 
</em><br>
<em>&gt; stupidly flawed that...(I know SF is just for entertainment, I'm 
</em><br>
<em>&gt; trying to prove a point here.)
</em><br>
<em>&gt;
</em><br>
<em>&gt; The complexity of the task (AGI) is naturally selecting for builder(s) 
</em><br>
<em>&gt; that have a clue.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Do you have any particular reason to believe that the FAI problem is 
</em><br>
<em>&gt; more complex than the AGI problem? Most people seem to believe that 
</em><br>
<em>&gt; intuitively. This is due to two reasons.
</em><br>
<em>&gt; 1) It is easier to argue about FAI because it doesn't require as much 
</em><br>
<em>&gt; technical knowledge. It is easier to grasp the complexity of the 
</em><br>
<em>&gt; Friendliness problem first hand. It looks like a big thing to solve.
</em><br>
<em>&gt; 2) General intelligence seems kind of straightforward because we do it 
</em><br>
<em>&gt; all the time however; doing it is definitely not the same as coding 
</em><br>
<em>&gt; it. In fact, people systematically underestimate how complex AGI 
</em><br>
<em>&gt; really is. There have been many that claimed to have the AGI solution. 
</em><br>
<em>&gt; They have all failed todate. If you ever try coding an AGI you will 
</em><br>
<em>&gt; very likely realize it is more complex than you originally thought.
</em><br>
<em>&gt;
</em><br>
<em>&gt; These two reasons cause people to focus on the FAI problem more than 
</em><br>
<em>&gt; on the AGI problem. Which, in my opinion is a mistake at this stage.
</em><br>
<em>&gt;
</em><br>
<em>&gt; There is another twist to this. The FAI or UFAI concepts are mostly 
</em><br>
<em>&gt; useless without AGI however; working on AGI will very likely help 
</em><br>
<em>&gt; develop FAI theory.
</em><br>
<em>&gt; 1) AGI theory will give a clearer picture of how FAI can be 
</em><br>
<em>&gt; technically implemented.
</em><br>
<em>&gt; 2) AGI work can have semi-intelligent tools as offshoots that, when 
</em><br>
<em>&gt; combined with human intelligence, enhance it (ex: human + computer + 
</em><br>
<em>&gt; Internet &gt; human). We could then work on FAI theory more efficiently 
</em><br>
<em>&gt; (and AGI aswell).
</em><br>
<p>Finally somebody is hitting the target.
<br>
Also, the problem of the hard takeoff is fake. It has never existed. It 
<br>
pertains only to SF (I will explain better this point on our site).
<br>
<p><em>&gt;
</em><br>
<em>&gt;&gt; This is why I think that the system and its safety should be 
</em><br>
<em>&gt;&gt; analyzed, and go through at least several phases of testing before 
</em><br>
<em>&gt;&gt; activation is even considered.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; There will be plenty of testing all along the project. And there wont 
</em><br>
<em>&gt; be just a single activation where the coders put some jargon together, 
</em><br>
<em>&gt; compile and see what happens. (See above)
</em><br>
<em>&gt; Poof-&gt; FAI -&gt; YAY! -&gt;JK -&gt;UFAI -&gt;NOO! -&gt;R.I.P.
</em><br>
<p>Totally agree.
<br>
<p><em>&gt;
</em><br>
<em>&gt;&gt; I would also feel better if these tests are conducted by a team of 
</em><br>
<em>&gt;&gt; independent AGI researchers rather than just one firm, or RGE Corp. 
</em><br>
<em>&gt;&gt; by itself.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; The AGI coder(s) will be pre-selected for competence by the complexity 
</em><br>
<em>&gt; of the task. The point is moot for evil coder(s) since they wouldn't 
</em><br>
<em>&gt; agree for inspection anyway.
</em><br>
<em>&gt; How would the &quot;independent AGI researchers&quot; be selected? How would we 
</em><br>
<em>&gt; know they are trustworthy and competent? I think this introduces more 
</em><br>
<em>&gt; uncertainty than it removes.
</em><br>
<p>But what evil coders? There is no evil coders (never existed). We are 
<br>
simply a company formed by people that have done an hard work, that have 
<br>
sweated blood to make something positive, and that have invested much 
<br>
time, money and efforts to do that.
<br>
Regarding PIBOT, the first step is to see if it functions properly and 
<br>
how it functions (IOW, what PIBOT can do?). The second step is to do an 
<br>
assessment of the AGI scenario. The third step is to open and to share 
<br>
the technology.
<br>
Furthermore, we are Italians. Do you know Italy? The country of sun and 
<br>
see. We like good cuisine. We like to dress well. We like friendship. We 
<br>
live very well in our country.
<br>
<p><em>&gt;&gt;  You can have many shots at creating a Seed AI, but you only get one 
</em><br>
<em>&gt;&gt; shot at creating a friendly one.
</em><br>
<em>&gt;&gt;  If this is the Seed AI that is successful, then I say make that 
</em><br>
<em>&gt;&gt; friendly shot count.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; If you are shooting randomly, there is a small chance you will hit the 
</em><br>
<em>&gt; right target, a slightly larger chance you will hit the wrong target, 
</em><br>
<em>&gt; and an overwhelmingly huge chance you will never hit anything. If 
</em><br>
<em>&gt; you're one of the few who have some talent, opportunity, and 
</em><br>
<em>&gt; persistence, you can perfect your archery and hit targets at will. We 
</em><br>
<em>&gt; hope you aim at FAI for all.
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; Since you want to be open with the technology, I think that it is an 
</em><br>
<em>&gt;&gt; idea worth considering.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; I am somewhat suspicious of AGI claims. The more advanced an AI is the 
</em><br>
<em>&gt; less public proving it needs. A seed AI could recursively self 
</em><br>
<em>&gt; improve, start the singularity ... trust me we would know. A 
</em><br>
<em>&gt; superhuman AI that is a bit short of the Singularity (could happen) 
</em><br>
<em>&gt; could at least make its owners millionaires, just let it play with the 
</em><br>
<em>&gt; stock market. Even a slow AI could do some amazing things. At least it 
</em><br>
<em>&gt; would be immune to human biases. There might be some areas where it 
</em><br>
<em>&gt; could outperform humans. They would have investors chasing them not 
</em><br>
<em>&gt; them having to prove themselves. When I think of some workshop format 
</em><br>
<em>&gt; proving, what comes to mind is Eliza type AI. It looks smart at first 
</em><br>
<em>&gt; glance but is inferior to humans in practically every way. I'm not 
</em><br>
<em>&gt; trying to be rude, but the lack of a splash sort of indicates that 
</em><br>
<em>&gt; there isn't that much to see.
</em><br>
<em>&gt;
</em><br>
<em>&gt; _________________________________________________________________
</em><br>
<em>&gt; Take advantage of powerful junk e-mail filters built on patented 
</em><br>
<em>&gt; Microsoft® SmartScreen Technology. 
</em><br>
<em>&gt; <a href="http://join.msn.com/?pgmarket=en-ca&amp;page=byoa/prem&amp;xAPID=1994&amp;DI=1034&amp;SU=http://hotmail.com/enca&amp;HL=Market_MSNIS_Taglines">http://join.msn.com/?pgmarket=en-ca&amp;page=byoa/prem&amp;xAPID=1994&amp;DI=1034&amp;SU=http://hotmail.com/enca&amp;HL=Market_MSNIS_Taglines</a></em><br>
<em>&gt;  Start enjoying all the benefits of MSN® Premium right now and get the 
</em><br>
<em>&gt; first two months FREE*.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13985.html">Rick Geniale: "Sorry. The country of sun and sea (not see)"</a>
<li><strong>Previous message:</strong> <a href="13983.html">P K: "Re: AGI thoughts was[AGI Reproduction? (Safety)]"</a>
<li><strong>In reply to:</strong> <a href="13983.html">P K: "Re: AGI thoughts was[AGI Reproduction? (Safety)]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13986.html">P K: "Re: AGI thoughts"</a>
<li><strong>Reply:</strong> <a href="13986.html">P K: "Re: AGI thoughts"</a>
<li><strong>Reply:</strong> <a href="13992.html">Charles D Hixson: "Re: AGI thoughts was[AGI Reproduction? (Safety)]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13984">[ date ]</a>
<a href="index.html#13984">[ thread ]</a>
<a href="subject.html#13984">[ subject ]</a>
<a href="author.html#13984">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:55 MDT
</em></small></p>
</body>
</html>
