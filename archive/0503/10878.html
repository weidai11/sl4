<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Overconfidence and meta-rationality</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Overconfidence and meta-rationality">
<meta name="Date" content="2005-03-09">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Overconfidence and meta-rationality</h1>
<!-- received="Wed Mar  9 16:41:51 2005" -->
<!-- isoreceived="20050309234151" -->
<!-- sent="Wed, 09 Mar 2005 15:40:30 -0800" -->
<!-- isosent="20050309234030" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Overconfidence and meta-rationality" -->
<!-- id="422F896E.7030204@pobox.com" -->
<!-- charset="windows-1252" -->
<!-- inreplyto="6.2.1.2.2.20050226165910.02ba6b08@mail.gmu.edu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Overconfidence%20and%20meta-rationality"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed Mar 09 2005 - 16:40:30 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10879.html">Ben Goertzel: "RE: Cognitive neuroscience of consciousness"</a>
<li><strong>Previous message:</strong> <a href="10877.html">brannen: "Re: Cognitive neuroscience of consciousness"</a>
<li><strong>Maybe in reply to:</strong> <a href="../0502/10764.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10880.html">Marc Geddes: "Re: Overconfidence and meta-rationality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10878">[ date ]</a>
<a href="index.html#10878">[ thread ]</a>
<a href="subject.html#10878">[ subject ]</a>
<a href="author.html#10878">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Robin Hanson wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; You don't seem very interested in the formal analysis here.  You know, 
</em><br>
<em>&gt; math, theorems and all that.
</em><br>
<p>You did not ask.
<br>
<p>Helpful references:
<br>
<p>Robin's paper &quot;Are Disagreements Honest?&quot;
<br>
<a href="http://www.gmu.edu/jbc/Tyler/deceive.pdf">http://www.gmu.edu/jbc/Tyler/deceive.pdf</a>
<br>
<p>which builds on Aumann's Agreement Theorem:
<br>
<a href="http://www.princeton.edu/~bayesway/Dick.tex.pdf">http://www.princeton.edu/~bayesway/Dick.tex.pdf</a>
<br>
(not a good intro for the bewildered, maybe someone can find a better intro)
<br>
<p><em>&gt; The whole point of such analysis is to 
</em><br>
<em>&gt; identify which assumptions matter for what conclusions.  And as far as I 
</em><br>
<em>&gt; can tell your only argument which gets at the heart of the relevant 
</em><br>
<em>&gt; assumptions is your claim that those who make relatively more errors 
</em><br>
<em>&gt; can't see this fact while those who make relatively fewer errors can see 
</em><br>
<em>&gt; this fact.
</em><br>
<p>I don't think this argument (which you do concede for a factual premise? 
<br>
or was our agreement only that people who make relatively fewer errors 
<br>
do so in part because they are relatively better at estimating their 
<br>
probability of error on specific problems?) is what touches on the 
<br>
assumptions.
<br>
<p>Anyway, let's talk math.
<br>
<p>First, a couple of general principles that apply to discussions in which 
<br>
someone invokes math:
<br>
<p>1)  An argument from pure math, if it turns out to be wrong, must have 
<br>
an error in one or more premises or purportedly deductive steps.  If the 
<br>
deductive steps are all correct, this is a special kind of rigor which 
<br>
Ben Goertzel gave as his definition of the word &quot;technical&quot;; personally 
<br>
I would label this class of argument &quot;logical&quot;, reserving &quot;technical&quot; 
<br>
for hypotheses that sharply concentrate their probability mass.  (A la 
<br>
&quot;A Technical Explanation of Technical Explanation&quot;.)
<br>
<p>Pure math is a fragile thing.  An argument that is pure math except for 
<br>
one nonmathematical step is not pure math.  The chain of reasoning in 
<br>
&quot;Are Disagreements Honest?&quot; is not pure math.  The modesty argument uses 
<br>
Aumann's Agreement Theorem and AAT's extensions as plugins, but the 
<br>
modesty argument itself is not formal from start to finish.  I know of 
<br>
no *formal* extension of Aumann's Agreement Theorem such that its 
<br>
premises are plausibly applicable to humans.  I also expect that I know 
<br>
less than a hundredth as much about AAT's extensions as you do.  But if 
<br>
I am correct that there is no formal human extension of AAT, you cannot 
<br>
tell me: &quot;If you claim the theorem is wrong, then it is your 
<br>
responsibility to identify which of the deductive steps or empirical 
<br>
premises is wrong.&quot;  The modesty argument has not yet been formalized to 
<br>
that level.  It's still a modesty *argument* not a modesty *theorem*.
<br>
<p>Might the modesty argument readily formalize to a modesty theorem with a 
<br>
bit more work?  Later I will argue that this seems unlikely because the 
<br>
modesty argument has a different character from Aumann's Agreement Theorem.
<br>
<p>2)  Logical argument has no ability to coerce physics.  There's a 
<br>
variety of parables I tell to illustrate this point.  Here's one parable:
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Socrates raised the glass of hemlock to his lips.  &quot;Do you 
<br>
suppose,&quot; asked one of the onlookers, &quot;that even hemlock will not be 
<br>
enough to kill so wise and good a man?&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;No,&quot; replied another bystander, a student of philosophy; &quot;all men 
<br>
are mortal, and Socrates is a man; and if a mortal drink hemlock, surely 
<br>
he dies.&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;Well,&quot; said the onlooker, &quot;what if it happens that Socrates 
<br>
*isn't* mortal?&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;Nonsense,&quot; replied the student, a little sharply; &quot;all men are 
<br>
mortal *by definition*; it is part of what we mean by the word 'man'. 
<br>
All men are mortal, Socrates is a man, therefore Socrates is mortal.  It 
<br>
is not merely a guess, but a *logical certainty*.&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;I suppose that's right...&quot; said the onlooker.  &quot;Oh, look, Socrates 
<br>
already drank the hemlock while we were talking.&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;Yes, he should keel over any minute now,&quot; said the student.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;And they waited, and they waited, and they waited...
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;Socrates appears not to be mortal,&quot; said the onlooker.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;Then Socrates must not be a man,&quot; replied the student.  &quot;All men 
<br>
are mortal, Socrates is not mortal, therefore Socrates is not a man. 
<br>
And that is not merely a guess, but a *logical certainty*.&quot;
<br>
<p>The moral of this parable is that if all &quot;humans&quot; are mortal by 
<br>
definition, then I cannot know that Socrates is a &quot;human&quot; until after I 
<br>
have observed that Socrates is mortal.  If &quot;humans&quot; are defined as 
<br>
mortal language-users with ten fingers, then it does no good at all - 
<br>
under Aristotle's logic - to observe merely that Socrates speaks 
<br>
excellent Greek and count five of his fingers on each hand.  I cannot 
<br>
state that Socrates is a member of the class &quot;human&quot; until I observe all 
<br>
three properties of Socrates - language use, ten fingers, and mortality. 
<br>
&nbsp;&nbsp;Whatever information I put into an Aristotelian definition, I get 
<br>
exactly the same information back out - nothing more.  If you want 
<br>
actual cognitive categories instead of mere Aristotelian classes, 
<br>
categories that permit your mind to classify objects into empirical 
<br>
clusters and thereby guess observations you have not yet made, you have 
<br>
to resort to induction, not deduction.  Whatever is said to be true &quot;by 
<br>
definition&quot; usually isn't; writing in dictionaries has no ability to 
<br>
coerce physics.  You cannot change the writing in a dictionary and get a 
<br>
different outcome.
<br>
<p>Another parable:
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Once upon a time there was a court jester who dabbled in logic. 
<br>
The jester gave the king two boxes:  The first box inscribed &quot;Either 
<br>
this box contains an angry frog, or the box with a false inscription 
<br>
contains gold, but not both.&quot;  And the second box inscribed &quot;Either this 
<br>
box contains gold and the box with a false inscription contains an angry 
<br>
frog, or this box contains an angry frog and the box with a true 
<br>
inscription contains gold.&quot;  And the jester said:  &quot;One box contains an 
<br>
angry frog, the other box gold, and one and only one of the inscriptions 
<br>
is true.&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The king opened the wrong box, and was savaged by an angry frog.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;You see,&quot; the jester said, &quot;let us hypothesize that the first 
<br>
inscription is the true one.  Then suppose the first box contains an 
<br>
angry frog.  Then the other box would contain gold and this would 
<br>
contradict the first inscription which we hypothesized to be true.  Now 
<br>
suppose the first box contains gold.  The other box would contain an 
<br>
angry frog, which again contradicts the first inscription -&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The king ordered the jester thrown in the dungeons.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A day later, the jester was brought before the king in chains, and 
<br>
shown two boxes.  &quot;One box contains a key,&quot; said the king, &quot;to unlock 
<br>
your chains, and if you find the key you are free.  But the other box 
<br>
contains a dagger for your heart if you fail.&quot;  And the first box was 
<br>
inscribed:  &quot;Either both inscriptions are true or both inscriptions are 
<br>
false.&quot;  And the second box was inscribed:  &quot;This box contains the key.&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The jester reasoned thusly:  &quot;Suppose the first inscription is 
<br>
true.  Then the second inscription must also be true.  Now suppose the 
<br>
first inscription is false.  Then again the second inscription must be 
<br>
true.  Therefore the second box contains the key, whether the first 
<br>
inscription is true or false.&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The jester opened the second box and found a dagger.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;How?!&quot; cried the jester in horror, as he was dragged away.  &quot;It 
<br>
isn't possible!&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;It is quite possible,&quot; replied the king.  &quot;I merely wrote those 
<br>
inscriptions on two boxes, and then I put the dagger in the second one.&quot;
<br>
<p>In &quot;Are Disagreements Honest?&quot; you say that people should not have one 
<br>
standard in public and another standard in private; you say:  &quot;If people 
<br>
mostly disagree because they systematically violate the rationality 
<br>
standards that they profess, and hold up for others, then we will say 
<br>
that their disagreements are dishonest.&quot;  (I would disagree with your 
<br>
terminology; they might be dishonest *or* they might be self-deceived. 
<br>
Whether you think self-deception is a better excuse than dishonesty is 
<br>
between yourself and your morality.)  In any case, there is a moral and 
<br>
social dimension to the words you use in &quot;Are Disagreements Honest?&quot; 
<br>
You did in fact invoke moral forces to help justify some steps in your 
<br>
chain of reasoning, even if you come back later and say that the steps 
<br>
can stand on their own.
<br>
<p>Now suppose that I am looking at two boxes, one with gold, and one with 
<br>
an angry frog.  I have pondered these two boxes as best I may, and those 
<br>
signs and portents that are attached to boxes; and I believe that the 
<br>
first box contains the gold, with 67% probability.  And another person 
<br>
comes before me and says:  &quot;I believe that the first box contains an 
<br>
angry frog, with 99.9% probability.&quot;  Now you may say to me that I 
<br>
should not presume a priori that I am more rational than others; you may 
<br>
say that most people are self-deceived about their relative immunity to 
<br>
self-deception; you may say it would be logically inconsistent with my 
<br>
publicly professed tenets if we agree to disagree; you may say that it 
<br>
wouldn't be fair for me to insist that the other person change his 
<br>
opinion if I'm not willing to change mine.  So suppose that the two of 
<br>
us agree to compromise on a 99% probability that the first box contains 
<br>
an angry frog.  But this is not just a social compromise; it is an 
<br>
attempted statement about physical reality, determined by the modesty 
<br>
argument.  What if the first box, in defiance of our logic and 
<br>
reasonableness, turns out to contain gold instead?  Which premises of 
<br>
the modesty argument would turn out to be the flawed ones?  Which 
<br>
premises would have failed to reflect underlying, physical, empirical 
<br>
reality?
<br>
<p>The heart of your argument in &quot;Are Disagreements Honest?&quot; is Aumann's 
<br>
Agreement Theorem and the dozens of extensions that have been found for 
<br>
it.  But if Aumann's Agreement Theorem is wrong (goes wrong reliably in 
<br>
the long run, not just failing 1 time out of 100 when the consensus 
<br>
belief is 99% probability) then we can readily compare the premises of 
<br>
AAT against the dynamics of the agents, their updating, their prior 
<br>
knowledge, etc., and track down the mistaken assumption that caused AAT 
<br>
(or the extension of AAT) to fail to match physical reality.  In 
<br>
contrast, it seems harder to identify what would have gone wrong, 
<br>
probability-theoretically speaking, if I dutifully follow the modesty 
<br>
argument, humbly update my beliefs until there is no longer any 
<br>
disagreement between myself and the person standing next to me, and the 
<br>
other person is also fair and tries to do the same, and lo and behold 
<br>
our consensus beliefs turn out to be more poorly calibrated than my 
<br>
original guesses.
<br>
<p>Is this scenario a physical impossibility?  Not obviously, though I'm 
<br>
willing to hear you out if you think it is.  Let's suppose that the 
<br>
scenario is physically possible and that it occurs; then which of the 
<br>
premises of the modesty argument do you think would have been 
<br>
empirically wrong?  Is my sense of fairness factually incorrect?  Is the 
<br>
other person's humility factually incorrect?  Does the factually 
<br>
mistaken premise lie in our dutiful attempt to avoid agreeing to 
<br>
disagree because we know this implies a logical inconsistency?  To me 
<br>
this suggests that the modesty argument is not just *presently* 
<br>
informal, but that it would be harder to formalize than one might wish.
<br>
<p>There's another important difference between the modesty argument and 
<br>
Aumann's Agreement Theorem.  AAT has been excessively generalized; it's 
<br>
easy to generalize and a new generalization is always worth a published 
<br>
paper.  You attribute the great number of extensions of AAT to the 
<br>
following underlying reason:  &quot;His [Aumann's] results are robust because 
<br>
they are based on the simple idea that when seeking to estimate the 
<br>
truth, you should realize you might be wrong; others may well know 
<br>
things that you do not.&quot;
<br>
<p>I disagree; this is *not* what Aumann's results are based on.
<br>
<p>Aumann's results are based on the underlying idea that if other entities 
<br>
behave in a way understandable to you, then their observable behaviors 
<br>
are relevant Bayesian evidence to you.  This includes the behavior of 
<br>
assigning probabilities according to understandable Bayesian cognition.
<br>
<p>Suppose that A and B have a common prior probability for proposition X 
<br>
of 10%.  A sees a piece of evidence E1 and updates X's probability to 
<br>
90%; B sees a piece of evidence E2 and updates X's probability to 1%. 
<br>
Then A and B compare notes, exchanging no information except their 
<br>
probability assignments.  Aumann's Agreement Theorem easily permits us 
<br>
to construct scenarios in which A and B's consensus probability goes to 
<br>
0, 1, or any real number between.  (Or rather, simple extensions of AAT 
<br>
permit this; the version of AAT I saw is static, allowing only a single 
<br>
question and answer.)  Why?  Because it may be that A's posterior 
<br>
announcement, &quot;90%&quot;, is sufficient to uniquely identify E1 as A's 
<br>
observation, in that no other observed evidence would produce A's 
<br>
statement &quot;90%&quot;; likewise with B and E2.  The joint probability for 
<br>
E1&amp;E2 given X (or ~X) does not need to be the product of the 
<br>
probabilities E1|X and E2|X (E1|~X, E2|~X).  It might be that E1 and E2 
<br>
are only ever seen together when X, or only ever seen together when ~X. 
<br>
&nbsp;&nbsp;So A and B are *not* compromising between their previous positions; 
<br>
their consensus probability assignment is *not* a linear weighting of 
<br>
their previous assignments.
<br>
<p>If you tried to devise an extension of Aumann's Agreement Theorem in 
<br>
which A and B, e.g., deduce each other's likelihoods given their stated 
<br>
posteriors and then combine likelihoods, you would be assuming that A 
<br>
and B always see unrelated evidence - an assumption rather difficult to 
<br>
extend to human domains of argument; no two minds could ever take the 
<br>
same arguments into account.  Our individual attempts to cut through to 
<br>
the correct answer do not have the Markov property relative to one 
<br>
another; different rationalists make correlated errors.
<br>
<p>Under AAT, as A and B exchange information and become mutually aware of 
<br>
knowledge, they concentrate their models into an ever-smaller set of 
<br>
possible worlds.  (I dislike possible-worlds semantics for various 
<br>
reasons, but let that aside; the formalizations I've found of AAT are 
<br>
based on possible-worlds semantics.  Besides, I rather liked the way 
<br>
that possible-worlds semantics avoids the infinite recursion problem in 
<br>
&quot;common knowledge&quot;.)  If A and B's models are concentrating their 
<br>
probability densities into ever-smaller volumes, why, they must be 
<br>
learning something - they're reducing entropy, one might say, though 
<br>
only metaphorically.
<br>
<p>Now *contrast* this with the modesty argument, as its terms of human 
<br>
intercourse are usually presented.  I believe that the moon is made of 
<br>
green cheese with 80% probability.  Fred believes that the moon is made 
<br>
of blueberries with 90% probability.  This is all the information that 
<br>
we have of each other; we can exchange naked probability assignments but 
<br>
no other arguments.  By the math of AAT, *or* the intuitive terms of the 
<br>
modesty argument, this ought to force agreement.  In human terms, 
<br>
presumably I should take into account that I might be wrong and that 
<br>
Fred has also done some thinking about the subject, and compromise my 
<br>
beliefs with Fred's, so that we'll say, oh, hm, that the moon is made of 
<br>
green cheese with 40% probability and blueberries with 45% probability, 
<br>
that sounds about right.  Fred chews this over, decides I'm being fair, 
<br>
and nods agreement; Fred updates his verbally stated probability 
<br>
assignments accordingly.  Yay!  We agreed!  It is now theoretically 
<br>
possible that we are being verbally consistent with our professed 
<br>
beliefs about what is rational!
<br>
<p>But wait!  What do Fred and I know about the moon that we didn't know 
<br>
before?  If this were AAT, rather than a human conversation, then as 
<br>
Fred and I exchanged probability assignments our actual knowledge of the 
<br>
moon would steadily increase; our models would concentrate into an 
<br>
ever-smaller set of possible worlds.  So in this sense the dynamics of 
<br>
the modesty argument are most unlike the dynamics of Aumann's Agreement 
<br>
Theorem, from which the modesty argument seeks to derive its force.  AAT 
<br>
drives down entropy (sorta); the modesty argument doesn't.  This is a 
<br>
BIG difference.
<br>
<p>Furthermore, Fred and I can achieve the same mutual triumph of possible 
<br>
consistency - hence, public defensibility if someone tries to criticize 
<br>
us - by agreeing that the moon is equally likely to be made of green 
<br>
cheese or blueberries.  (Fred is willing to agree that I shouldn't be 
<br>
penalized for having been more modest about my discrimination 
<br>
capability.  Modesty is a virtue and shouldn't be penalized.)
<br>
<p>As far as any outside observer can tell according to the rules you have 
<br>
laid down for 'modesty', two disputants can publicly satisfy the moral 
<br>
demand of the modesty argument by any number of possible compromises. 
<br>
&nbsp;From _Are Disagreements Honest_:  &quot;It is perhaps unsurprising that most 
<br>
people do not always spend the effort required to completely overcome 
<br>
known biases.  What may be more surprising is that people do not simply 
<br>
stop disagreeing, as this would seem to take relatively little 
<br>
effort...&quot;  I haven't heard of an extension to AAT which (a) proves that 
<br>
'rational' agents will agree (b) explicitly permits multiple possible 
<br>
compromises to be equally 'rational' as the agent dynamics were defined.
<br>
<p>&nbsp;From _Are Disagreements Honest?_:
<br>
<p><em>&gt; One approach would be to try to never assume that you are more meta-rational than anyone else. But this cannot mean that you should agree with everyone, because you simply cannot do so when other people disagree among themselves. Alternatively, you could adopt a &quot;middle&quot; opinion. There are, however, many ways to define middle, and people can disagree about which middle is best (Barns 1998).  Not only are there disagreements on many topics, but there are also disagreements on how to best correct for one’s limited meta-rationality.
</em><br>
<p>The AATs I know are constructive; they don't just prove that agents will 
<br>
agree as they acquire common knowledge, they describe *exactly how* 
<br>
agents arrive at agreement.  (Including multiple agents.)  So that's 
<br>
another sense in which the modesty argument seems unlike a formalizable 
<br>
extension of AAT - the modesty argument doesn't tell us *how* to go 
<br>
about being modest.  Again, this is a BIG difference.
<br>
<p>&nbsp;From _Are Disagreements Honest?_:
<br>
<p><em>&gt; For example, people who feel free to criticize consistently complain when they notice someone making a sequence of statements that is inconsistent or incoherent. [...] These patterns of criticism suggest that people uphold rationality standards that prefer logical consistency...
</em><br>
<p>As I wrote in an unpublished work of mine:
<br>
<p>&quot;Is the Way to have beliefs that are consistent among themselves?  This 
<br>
is not the Way, though it is often mistaken for the Way by logicians and 
<br>
philosophers.  The object of the Way is to achieve a map that reflects 
<br>
the territory.  If I survey a city block five times and draw five 
<br>
accurate maps, the maps, being consistent with the same territory, will 
<br>
be consistent with each other.  Yet I must still walk through the city 
<br>
block and draw lines on paper that correspond to what I see.  If I sit 
<br>
in my living room and draw five maps that are mutually consistent, the 
<br>
maps will bear no relation whatsoever to the territory.  Accuracy of 
<br>
belief implies consistency of belief, but consistency does not imply 
<br>
accuracy.  Consistency of belief is only a sign of truth, and does not 
<br>
constitute truth in itself.&quot;
<br>
<p>&nbsp;From _ADH?_:
<br>
<p><em>&gt; In this paper we consider only truth-seeking at the individual level, and do not attempt a formal definition, in the hope of avoiding the murky philosophical waters of “justified belief.”
</em><br>
<p>I define the &quot;truth&quot; of a probabilistic belief system as its score 
<br>
according to the strictly proper Bayesian scoring criterion I laid down 
<br>
in &quot;Technical Explanation&quot; - a definition of truth which I should 
<br>
probably be attributing to someone else, but I have no idea who.
<br>
<p>(Incidentally, it seems to me that the notion of the Bayesian score cuts 
<br>
through a lot of gibberish about freedom of priors; the external 
<br>
goodness of a prior is its Bayesian score.  A lot of philosophers seem 
<br>
to think that, because there's disagreement where priors come from, they 
<br>
can pick any damn prior they please and none of those darned 
<br>
rationalists will be able to criticize them.  But there's actually a 
<br>
very clearly defined criterion for the external goodness of priors, the 
<br>
question is just how to maximize it using internally accessible 
<br>
decisions.  That aside...)
<br>
<p>According to one who follows the way of Bayesianity - a Bayesianitarian, 
<br>
one might say - it is better to have inconsistent beliefs with a high 
<br>
Bayesian score than to have consistent beliefs with a low Bayesian 
<br>
score.  Accuracy is prized above consistency.  I guess that this 
<br>
situation can never arise given logical omniscience or infinite 
<br>
computing power; but I guess it can legitimately arise under bounded 
<br>
rationality.  Maybe you could even detect an *explicit* inconsistency in 
<br>
your beliefs, while simultaneously having no way to reconcile it in a 
<br>
way that you expect to raise your Bayesian score.  I'm not sure about 
<br>
that, though.  It seems like the scenario would be hard to construct, no 
<br>
matter what bounds you put on the rationalist.  I would not be taken 
<br>
aback to see a proof of impossibility - though I would hope the 
<br>
impossibility proof to take the form of a simple constructive algorithm 
<br>
that can be followed by most plausible bounded rationalists in case they 
<br>
discover inconsistency.
<br>
<p>Even the simplest inconsistency resolution algorithm may take more 
<br>
time/computation than the simpler algorithm &quot;discard one belief at 
<br>
random&quot;.  And the simplest good resolution algorithm for resolving a 
<br>
human disagreement may take more time than one of the parties discarding 
<br>
their beliefs at random.  Would it be more rational to ignore this 
<br>
matter of the Bayesian score, which is to say, ignore the truth, and 
<br>
just agree as swiftly as possible with the other person?  No.  Would 
<br>
that behavior be more 'consistent' with Aumann's result and extensions? 
<br>
&nbsp;&nbsp;No, because the AATs I know, when applied to any specific 
<br>
conversation, constructively specify a precise, score-maximizing change 
<br>
of beliefs - which a random compromise is not.  All you'd be maximizing 
<br>
through rapid compromise is your immunity to social criticism for 
<br>
'irrationality' in the event of a public disagreement.
<br>
<p>Aumann's Agreement Theorem and its extensions do not say that 
<br>
rationalists *should* agree.  AATs prove that various rational agents 
<br>
*will* agree, not because they *want* to agree, but because that's how 
<br>
the dynamics work out.  But that mathematical result doesn't mean that 
<br>
you can become more rational by pursuing agreement.  It doesn't mean you 
<br>
can find your Way by trying to imitate this surface quality of AAT 
<br>
agents, that they agree with one another; because that cognitive 
<br>
behavior is itself quite unlike what AAT agents do.  You cannot tack an 
<br>
imperative toward agreement onto the Way.  The Way is only the Way of 
<br>
cutting through to the correct answer, not the Way of cutting through to 
<br>
the correct answer + not disagreeing with others.  If agreement arises 
<br>
from that, fine; if not, it doesn't mean that you can patch the Way by 
<br>
tacking a requirement for agreement onto the Way.
<br>
<p>The essence of the modesty argument is that we can become more rational 
<br>
by *trying* to agree with one another; but that is not how AAT agents 
<br>
work in their internals.  Though my reply doesn't rule out the 
<br>
possibility that the modesty rule might prove pragmatically useful when 
<br>
real human beings try to use it.
<br>
<p>The modesty argument is important in one respect.  I agree that when two 
<br>
humans disagree and have common knowledge of each other's opinion (or a 
<br>
human approximation of common knowledge which does not require logical 
<br>
omniscience), *at least one* human must be doing something wrong.  The 
<br>
modesty argument doesn't tell us immediately what is wrong or how to fix 
<br>
it.  I have argued that the *behavior* of modesty is not a solution 
<br>
theorem, though it might *pragmatically* help.  But the modesty 
<br>
*argument* does tell us that something is wrong.  We shouldn't ignore 
<br>
things when they are visibly wrong - even if modesty is not a solution.
<br>
<p>One possible underlying fact of the matter might be that one person is 
<br>
right and the other person is wrong and that is all there ever was to 
<br>
it.  This is not an uncommon state of human affairs.  It happens every 
<br>
time a scientific illiterate argues with a scientific literate about 
<br>
natural selection.  From my perspective, the scientific literate is 
<br>
doing just fine and doesn't need to change anything.  The scientific 
<br>
illiterate, if he ever becomes capable of facing the truth, will end up 
<br>
needing to sacrifice some of his most deeply held beliefs while not 
<br>
receiving any compromise or sacrifice-of-belief in return, not even the 
<br>
smallest consolation prize.  That's just the Way things are sometimes. 
<br>
And in AAT also, sometimes when you learn the other's answer you will 
<br>
simply discard your own, while the other changes his probability 
<br>
assignment not a jot.  Aumann agents aren't always humble and compromising.
<br>
<p>But then we come to the part of the problem that pits meta-rationality 
<br>
against self-deception.  How does the scientific literate guess that he 
<br>
is in the right, when he, being scientifically literate, is also aware 
<br>
of studies of human overconfidence and of consistent biases toward 
<br>
self-overestimation of relative competence?
<br>
<p>As far as I know, neither meta-rationality nor self-deception have been 
<br>
*formalized* in a way plausibly applicable to humans even as an 
<br>
approximation.  (Or maybe it would be better to say that I have not yet 
<br>
encountered a satisfactory formalism.  For who among us has read the 
<br>
entire Literature?)
<br>
<p>Trying to estimate your own rationality or meta-rationality involves 
<br>
severe theoretical problems because of the invocation of reflectivity, a 
<br>
puzzle that I'm still trying to solve in my own FAI work.  My puzzle 
<br>
appears, not as a puzzle of estimating *self*-rationality as such, but 
<br>
the puzzle of why a Bayesian attaches confidence to a purely abstract 
<br>
system that performs Bayesian reasoning, without knowing the specifics 
<br>
of the domain.  &quot;Beliefs&quot; and &quot;likelihoods&quot; and &quot;Bayesian justification&quot; 
<br>
and even &quot;subjective probability&quot; are not ontological parts of our 
<br>
universe, which contains only a mist of probability amplitudes.  The 
<br>
probability theory I know can only apply to &quot;beliefs&quot; by translating 
<br>
them into ordinary causal signals about the domain, not treating them 
<br>
sympathetically *as beliefs*.
<br>
<p>Suppose I assign a subjective probability of 40% to some one-time event, 
<br>
and someone else says he assigns a subjective probability of 80% to the 
<br>
same one-time event.  This is all I know of him; I don't know the other 
<br>
person's priors, nor what evidence he has seen, nor the likelihood 
<br>
ratio.  There is no fundamental mathematical contradiction between two 
<br>
well-calibrated individuals with different evidence assigning different 
<br>
subjective probabilities to the same one-time event.  We can still 
<br>
suppose both individuals are calibrated in the long run - when one says 
<br>
&quot;40%&quot; it happens 40% of the time, and when one says &quot;80%&quot; it happens 80% 
<br>
of the time.  In this specific case, either the one-time event will 
<br>
happen or it won't.  How are two well-calibrated systems to update when 
<br>
they know the other's estimate, assuming they each believe the other to 
<br>
be well-calibrated, but know nothing else about one another? 
<br>
Specifically, they don't know the other's priors, just that those priors 
<br>
are well-calibrated - they can't deduce likelihood of evidence seen by 
<br>
examining the posterior probability.  (If they could deduce likelihoods, 
<br>
they could translate beliefs to causal signals by translating:  &quot;His 
<br>
prior odds in P were 1:4, and his posterior odds in P are 4:1, so he 
<br>
must have seen evidence about P of likelihood 16:1&quot; to &quot;The fact of his 
<br>
saying aloud '80%' has a likelihood ratio of 16:1 with respect to P/~P, 
<br>
even though I don't know the conditional probabilities.&quot;)
<br>
<p>How are these two minds to integrate the other's subjective probability 
<br>
into their calculations, if they can't convert the other's spoken words 
<br>
into some kind of witnessable causal signal that bears a known 
<br>
evidential relationship to the actual phenomenon?  How can Bayesian 
<br>
reasoning take into account other agents' beliefs *as beliefs*, not just 
<br>
as causal phenomena?
<br>
<p>Maybe if you know the purely abstract fact that the other entity is a 
<br>
Bayesian reasoner (implements a causal process with a certain Bayesian 
<br>
structure), this causes some type of Bayesian evidence to be inferrable 
<br>
from the pure abstract report &quot;70%&quot;?  Well, first of all, how do you 
<br>
integrate it?  If there's a mathematical solution it ought to be 
<br>
constructive.  Second, attaching this kind of *abstract* confidence to 
<br>
the output of a cognitive system runs into formal problems.  Consider 
<br>
Lob's Theorem in mathematical logic.  Lob's Theorem says that if you can 
<br>
prove that a proof of T implies T, you can prove T; |- ([]T =&gt; T) 
<br>
implies |- T.  Now the idea of attaching confidence to a Bayesian system 
<br>
seems to me to translate into the idea that if a Bayesian system says 
<br>
'X', that implies X.  I'm still trying to sort out this confused issue 
<br>
to the point where I will run over it in my mind one day and find out 
<br>
that Lob is not actually a problem.
<br>
<p>Is there an AAT extension that doesn't involve converting the other's 
<br>
beliefs into causal signals with known evidentiary relationships to the 
<br>
specific data?  Is there a formal AAT extension that works on the 
<br>
*abstract* knowledge of the other person's probable rationality, without 
<br>
being able to relate specific beliefs to specific states of the world? 
<br>
Suppose that I say 30%, and my friend says 70%, and we know of each 
<br>
other only the pure abstract fact that we are calibrated in the long 
<br>
run; in fact, we don't even know what our argument is about 
<br>
specifically.  Should we be able to reach an agreement on our 
<br>
probability assignments even though we have no idea what we're arguing 
<br>
about?  How?  What's the exact number?
<br>
<p>That's the problem I run into when I try to formalize a pure abstract 
<br>
belief about another person's 'rationality'.  (If this has already been 
<br>
formalized, do please let me know.)  Now obviously human beings do make 
<br>
intuitive estimates of each other's rationality.  I'm just saying that I 
<br>
don't know how to formalize this in a way free from paradox - humans do 
<br>
a lot of thinking that is useful and powerful but also sloppy and 
<br>
subject to paradox.  I think that if this human thinking is reliably 
<br>
useful, then there must be some structure to it that explains the 
<br>
usefulness, a structure that can be extracted and used in an FAI 
<br>
architecture while leaving all the sloppiness and paradox behind.  But I 
<br>
have not yet figured out how to build a reflective cognitive system that 
<br>
attaches equal evidential force to (a) its own estimates as they are 
<br>
produced in the system or (b) a mental model of an abstract process that 
<br>
is an accurate copy of itself, plus the abstract knowledge (without 
<br>
knowing the specific evidence) that this Bayesian process arrived at the 
<br>
same specific probability output.  I want this condition so the 
<br>
cognitive system is consistent under reflection; it attaches the same 
<br>
force to its own thoughts whether they are processed as thoughts or as 
<br>
causal signals.  But how do I prevent a system like that from falling 
<br>
prey to Lob's Theorem when it tries the same thing in mathematical 
<br>
logic?  That's something I'm presently pondering.  I think there's 
<br>
probably a straightforward solution, I just don't have it yet.
<br>
<p>Then we come to self-deception.  If it were not for self-deception, 
<br>
meta-rationality would be much more straightforward.  Grant some kind of 
<br>
cognitive framework for estimating self-rationality and 
<br>
other-rationality.  There would be some set of signals standing in a 
<br>
Bayesian relation to the quantities of &quot;rationality&quot;, some signals 
<br>
publicly accessible and some privately accessible.  Each party would 
<br>
honestly report their self-estimate of rationality (the public signals 
<br>
being privately accessible as well), and this estimate would have no 
<br>
privileged bias.  Instead, though, we have self-deceptive phenomena such 
<br>
as biased retrieval of signals favorable to self-rationality, and biased 
<br>
non-retrieval of signals prejudicial to self-rationality.
<br>
<p>It seems to me that you have sometimes argued that I should foreshorten 
<br>
my chain of reasoning, saying, &quot;But why argue and defend yourself, and 
<br>
give yourself a chance to deceive yourself?  Why not just accept the 
<br>
modesty argument?  Just stop fighting, dammit!&quot;  I am a human, and a 
<br>
human is a system with known biases like selective retrieval of 
<br>
favorable evidence.  Each additional step in an inferential chain 
<br>
introduces a new opportunity for the biases to enter.  Therefore I 
<br>
should grant greater credence to shorter chains of inference.
<br>
<p>This again has a certain human plausibility, and it even seems as if it 
<br>
might be formalizable.
<br>
<p>*But*, trying to foreshorten our chains of inference contradicts the 
<br>
character of ordinary probability theory.
<br>
<p>E. T. Jaynes (who is dead but not forgotten), in _Probability Theory: 
<br>
The Logic of Science_, Chapter 1, page 1.14, verse 1-23, speaking of a 
<br>
'robot' programmed to carry out Bayesian reasoning:
<br>
<p>1-23b:  &quot;The robot always takes into account all of the evidence it has 
<br>
relevant to a question.  It does not arbitrarily ignore some of the 
<br>
information, basing its conclusions only on what remains.  In other 
<br>
words, the robot is completely non-ideological.&quot;
<br>
<p>Jaynes quoted this dictum when he railed against ad-hoc devices of 
<br>
orthodox statistics that would throw away relevant information.  The 
<br>
modesty argument argues that I should foreshorten my chain of reasoning, 
<br>
*not* take into account everything I can retrieve as evidence, and stick 
<br>
to modesty - without using my biased retrieval mechanisms to try and 
<br>
recall evidence regarding my relative competence.  Now this has a 
<br>
pragmatic human plausibility, but it's very un-Jaynesian.  According to 
<br>
the religion of Bayesianity, what might perhaps be called 
<br>
Bayesianitarianism, I should be trying to kiss the truth, pressing my 
<br>
map as close to the territory as possible, maximizing my Bayesian score 
<br>
by every inch and fraction I can muster, using every bit of evidence I 
<br>
can find.
<br>
<p>I think that's the point which, from my perspective, cuts closest to the 
<br>
heart of the matter.  Biases can be overcome.  You can fight bias, and 
<br>
win.  You can't do that if you cut short the chain of reasoning at its 
<br>
beginning.  I don't spend as much time as I once did thinking about my 
<br>
relative rationality, mostly because I estimate myself as being so way 
<br>
the hell ahead that *relative* rationality is no longer interesting. 
<br>
The problems that worry me are whether I'm rational enough to deal with 
<br>
a given challenge from Nature.  But, yes, I try to estimate my 
<br>
rationality in detail, instead of using unchanged my mean estimate for 
<br>
the rationality of an average human.  And maybe an average person who 
<br>
tries to do that will fail pathetically.  Doesn't mean *I'll* fail, cuz, 
<br>
let's face it, I'm a better-than-average rationalist.  There will be 
<br>
costs, if I dare to estimate my own rationality.  There will be errors. 
<br>
&nbsp;&nbsp;But I think I can do better by thinking.
<br>
<p>While you might think that I'm not as good as I think, you probably do 
<br>
think that I'm a more skilled rationalist than an average early 
<br>
21st-century human, right?  According to the foreshortening version of 
<br>
the modesty argument, would I be forbidden to notice even that?  Where 
<br>
do I draw the line?  If you, Robin Hanson, go about saying that you have 
<br>
no way of knowing that you know more about rationality than a typical 
<br>
undergraduate philosophy student because you *might* be deceiving 
<br>
yourself, then you have argued yourself into believing the patently 
<br>
ridiculous, making your estimate correct.
<br>
<p>The indexical argument about how you could counterfactually have been 
<br>
born as someone else gets into deep anthropic issues, but I don't think 
<br>
that's really relevant given the arguments I already stated.
<br>
<p>And now I'd better terminate this letter before it goes over 40K and 
<br>
mailing lists start rejecting it.  I think that was most of what I had 
<br>
to say about the math, leaving out the anthropic stuff for lack of space.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10879.html">Ben Goertzel: "RE: Cognitive neuroscience of consciousness"</a>
<li><strong>Previous message:</strong> <a href="10877.html">brannen: "Re: Cognitive neuroscience of consciousness"</a>
<li><strong>Maybe in reply to:</strong> <a href="../0502/10764.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10880.html">Marc Geddes: "Re: Overconfidence and meta-rationality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10878">[ date ]</a>
<a href="index.html#10878">[ thread ]</a>
<a href="subject.html#10878">[ subject ]</a>
<a href="author.html#10878">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:22:54 MST
</em></small></p>
</body>
</html>
