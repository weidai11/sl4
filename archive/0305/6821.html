<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=X-UNKNOWN">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: SIAI's flawed friendliness analysis</title>
<meta name="Author" content="Bill Hibbard (test@demedici.ssec.wisc.edu)">
<meta name="Subject" content="RE: SIAI's flawed friendliness analysis">
<meta name="Date" content="2003-05-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: SIAI's flawed friendliness analysis</h1>
<!-- received="Mon May 26 15:43:56 2003" -->
<!-- isoreceived="20030526214356" -->
<!-- sent="Mon, 26 May 2003 16:43:42 -0500 (CDT)" -->
<!-- isosent="20030526214342" -->
<!-- name="Bill Hibbard" -->
<!-- email="test@demedici.ssec.wisc.edu" -->
<!-- subject="RE: SIAI's flawed friendliness analysis" -->
<!-- id="Pine.GSO.4.44.0305261641340.20116-100000@demedici.ssec.wisc.edu" -->
<!-- charset="X-UNKNOWN" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJKECAFAAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Bill Hibbard (<a href="mailto:test@demedici.ssec.wisc.edu?Subject=RE:%20SIAI's%20flawed%20friendliness%20analysis"><em>test@demedici.ssec.wisc.edu</em></a>)<br>
<strong>Date:</strong> Mon May 26 2003 - 15:43:42 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6822.html">Ben Goertzel: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>Previous message:</strong> <a href="6820.html">Samantha Atkins: "Re: Failure of AI so far"</a>
<li><strong>In reply to:</strong> <a href="6809.html">Ben Goertzel: "RE: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6822.html">Ben Goertzel: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6822.html">Ben Goertzel: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6823.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6821">[ date ]</a>
<a href="index.html#6821">[ thread ]</a>
<a href="subject.html#6821">[ subject ]</a>
<a href="author.html#6821">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
This discussion has split into many threads, and I'll bring
<br>
them together into this single response. Ben's comments are
<br>
a good starting point for this, and I'll address all the
<br>
recent questions.
<br>
<p>On Fri, 23 May 2003, Ben Goertzel wrote:
<br>
<p><em>&gt; There are a lot of good points and interesting issues mixed up here, but I
</em><br>
<em>&gt; think the most key point is the division between
</em><br>
<em>&gt;
</em><br>
<em>&gt; -- those who believe a hard takeoff is reasonably likely, based on a radical
</em><br>
<em>&gt; insight in AI design coupled with a favorable trajectory of self-improvemetn
</em><br>
<em>&gt; of a particular AI system
</em><br>
<em>&gt;
</em><br>
<em>&gt; -- those who believe in a soft takeoff, in which true AI is approached
</em><br>
<em>&gt; gradually [in which case government regulation, careful peer review and so
</em><br>
<em>&gt; forth are potentially relevant]
</em><br>
<em>&gt;
</em><br>
<em>&gt; The soft takeoff brings with it many obvious possibilities for safeguarding,
</em><br>
<em>&gt; which are not offered in the hard takeoff scenario.  These possibilities are
</em><br>
<em>&gt; the ones Bill Hibbard is exploring, I think.  A lot of what SIAI is saying
</em><br>
<em>&gt; is more relevant to the hard takeoff scenario, on the other hand.
</em><br>
<em>&gt;
</em><br>
<em>&gt; My own projection is a semi-hard takeoff, which doesn't really bring much
</em><br>
<em>&gt; reassurance...
</em><br>
<p>I think we'll eventually get to a time (the singularity) when
<br>
intelligence increases very quickly to very high levels. But I
<br>
think it will take a long time to get there, during a sort of
<br>
soft takeoff. In particular it will be years or even decades
<br>
from the first intelligent machines until the true singularity,
<br>
and it could be decades from now until the first intelligent
<br>
machines. I agree with Donald Norman that people tend to
<br>
overestimate the short-term progress of technological change,
<br>
and underestimate the long-term effects.
<br>
<p>I think real intelligence is decades away because no current
<br>
research is making any real progress on the grounding problem,
<br>
which is the problem of grounding symbols in sensory
<br>
experience and grounding reasoning and planning in learning.
<br>
That is, you cannot reason intelligently about horses unless
<br>
the word horse is connected to sight, sound, smell and touch
<br>
experiences with horses. Solving the grounding problem will
<br>
require much faster computers than are being used for current
<br>
AI research.
<br>
<p>I think there will be years or decades from the first real
<br>
machine intelligence until the singularity because of the
<br>
likelyhood of difficult technical problems even after the
<br>
first signs of machine intelligence, and because of the
<br>
amount of learning for intelligence to acheive its true
<br>
potential. Applying intelligence effectively (we might call
<br>
this wisdom) requires many fine value judgements that can
<br>
only be learned from experience. Humans require decades of
<br>
learning for their intelligence to mature. A super-intelligent
<br>
machine may learn faster, but it may also need a lot more
<br>
experience for its super-intelligence to mature (just as
<br>
higher animals generally take longer to mature than lower
<br>
animals).
<br>
<p>There is some chance that the first intelligent machines will
<br>
be hidden from the public. But probably not for long, because
<br>
they will be built in a wealthy and open society like the U.S.,
<br>
with lots of whistle blowers and where exciting news has a way
<br>
of getting out. Furthermore, a machine designed as I advocate,
<br>
with values for human happiness, or a machine designed as the
<br>
SIAI advocates, with a friendliness super-goal, would create
<br>
the singularity openly rather than hiding it from humans. It
<br>
is hard to imagine a safe singularity created in secret.
<br>
<p>There are three broad public policy choices for AI:
<br>
<p>1. Prohibit it, as advocated by Bill Joy in his April 2000 Wired
<br>
article &quot;Why the Future Doesn't Need Us&quot;.
<br>
<p>2. Allow it without regulation, as advocated by the SIAI and
<br>
most members of the SL4 mailing list.
<br>
<p>3. Allow it but regulate it, as I advocate.
<br>
<p>I think prohibiting AI is technically impossible and politically
<br>
unlikely, and unregulated AI is politically impossible and will
<br>
almost certainly be unsafe for humans. So we have no alternative
<br>
but to find our way through the difficulties of regulating AI.
<br>
In more detail:
<br>
<p>1. Prohibit AI.
<br>
<p>In his article, Bill Joy is pessimistic about prohibiting AI
<br>
because people will want the benefits. It will be politically
<br>
difficult to decide the right point to stop a technology whose
<br>
development continually creates wealth and relieves people of
<br>
the need to work.
<br>
<p>As several people have pointed out, it will be technically
<br>
impossible to prevent people from building outlaw AIs,
<br>
especially as technology matures. The only way to do it
<br>
would be to stop technological progress world wide, which
<br>
won't happen.
<br>
<p>2. Allow AI without regulation.
<br>
<p>Ben's question about timing is relevant here. If you think
<br>
that the singularity will happen so quickly that the public
<br>
and the government won't have time to act to control the
<br>
singularity once they realize that machines are becoming
<br>
intelligent, then you don't have to worry about regulation
<br>
because it will be too late.
<br>
<p>If the public and the government have enough time to react,
<br>
they will. People have been well primed for the dangers of
<br>
AI by science fiction books and movies. When machines start
<br>
surprising them with their intelligence, many people will be
<br>
freightened and then politicians will get excited. They will
<br>
be no more likely to allow unregulated AI than they are to
<br>
allow unregulated nuclear power. The only question is whether
<br>
they will try to prohibit or regulate AI.
<br>
<p>Wealthy and powerful institutions will have motives to build
<br>
unsafe AIs. Even generally well-meaning institutions may
<br>
fatally compromise safety for mildly selfish motives. Without
<br>
broad public insistence on aggressive safety regulation, one
<br>
of these unsafe AIs will likely be the seed for the
<br>
singularity.
<br>
<p>3. Allow AI with regulation.
<br>
<p>Ben's question about timing is relevant here too. The need
<br>
and political drive for regulation won't be serious until
<br>
mchines start exhibiting real intelligence, and that is
<br>
decades away. Even if you disagree about the timing, it is
<br>
still true that regulation won't interfere with current
<br>
research until some project acheives an AI breakthrough. At
<br>
the current stage of development, with lots of experiments
<br>
but nothing approaching real intelligence, regulation would
<br>
be counter-productive.
<br>
<p>Like so many things in politics, regulation is the best
<br>
choice among a set of bad alternatives. Here is a list of
<br>
objections, with my answers:
<br>
<p>&nbsp;&nbsp;a. Regulation cannot work because no one can understand my
<br>
&nbsp;&nbsp;designs. Government employees are too stupid to understand
<br>
&nbsp;&nbsp;designs.
<br>
<p>Government employees include lots of very smart people, like
<br>
those who worked on the Manhattan Project and those who are
<br>
finding cures for diseases. While it is healthy for citizens
<br>
to be skeptical of politicians and government, thinking that
<br>
all politicians and government employees are stupid is just
<br>
an ignorant prejudice.
<br>
<p>The regulators will understand designs because the burden
<br>
will be on the designers to satisfy regulators (many of whom
<br>
will be very smart) of the safety of their designs, as with
<br>
any dangerous technology.
<br>
<p>Even if some smart designers don't want to cooperate with
<br>
regulators, other designers just as smart will cooperate.
<br>
<p>&nbsp;&nbsp;b. Regulation will hobble cooperating projects, enabling
<br>
&nbsp;&nbsp;non-cooperating unsafe AI projects create the singularity
<br>
&nbsp;&nbsp;first.
<br>
<p>Non-cooperating projects will be hobbled by the need to hide
<br>
their resource use (large computers, smart designers, network
<br>
access, etc).
<br>
<p>As long as regulation is aggressively enforced, major
<br>
corporations and government agencies will cooperate and
<br>
bring their huge resources to the effort for safe AI.
<br>
<p>The government will have access to very smart people who can
<br>
help more than hinder the designers they are inspecting.
<br>
<p>Given the importance of AI, it is plausible that the U.S.
<br>
government itself will create a project like the Manhattan
<br>
Project for developing safe AI, with resources way beyond
<br>
those available to non-cooperating groups. Currently, the
<br>
U.S. GDP is about $10 trillion, the federal government
<br>
budget is about $2.3 trillion, the defense budget is $0.4
<br>
trillion, and global spending on information technology is
<br>
$3 trillion. When the public sees intelligent machines and
<br>
starts asking their elected representatives to do something
<br>
about it, and those representatives hear from experts
<br>
about the dangers of the singularity, it is easy to imagine
<br>
a federal safe AI project with a budget on the scale of
<br>
these numbers.
<br>
<p>&nbsp;&nbsp;c. A non-cooperating project may destroy the world by
<br>
&nbsp;&nbsp;using AI to create a nano-technology &quot;grey goo&quot; attack.
<br>
<p>This is possible. But even without AI, there may be a world
<br>
destroying attack using nano-technology or genetically
<br>
engineered micro-organisms. My judgement is that the
<br>
probability of unsafe AI from a lack of regulation (I think
<br>
this is close to 1.0) is greater than the marginal increase
<br>
in the probability of a nano-technology attack caused by
<br>
regulation of AI (as explained in my answer to the previous
<br>
objection, active government regulation won't necessarily
<br>
slow safe AI down relative to unsafe AI).
<br>
<p>&nbsp;&nbsp;d. Even if AI is regulated in most countries, there may
<br>
&nbsp;&nbsp;be others where it is not.
<br>
<p>This is a disturbing problem. However, the non-democracies
<br>
are gradually disappearing, and the democracies are
<br>
gradually learning to work together. Hopefully the world
<br>
will be more cooperative by the time the singularity
<br>
arrives.
<br>
<p>Democratic countries are wealthier than non-democracies,
<br>
so may create a safe singularity before an unsafe
<br>
singularity can be created elsewhere.
<br>
<p>&nbsp;&nbsp;e. We can't trust an AI because we can't know what its
<br>
&nbsp;&nbsp;thinking. An AI will continue to develop and design
<br>
&nbsp;&nbsp;other AIs that are beyond the ability of human
<br>
&nbsp;&nbsp;regulators to understand.
<br>
<p>There is no way to trace or predict the detailed thoughts
<br>
of an AI, but we can make the general prediction that it
<br>
will try to satisfy its reinforcement values. The safety
<br>
of an AI is primarily determined by its values (its
<br>
learning and simulation algorithms also need to be
<br>
accurate).
<br>
<p>I would trust an AI designed by another safe AI, with
<br>
reinforcement values for human happiness. It may decide
<br>
that we would be happier if its design was checked by
<br>
another independently-designed safe AI, and so seek such
<br>
peer review.
<br>
<p>&nbsp;&nbsp;f. The intelligence of AIs will be limited by the
<br>
&nbsp;&nbsp;ability of human regulators to understand their designs.
<br>
<p>This is related to the previous objection. Once we have
<br>
safe AIs, we can trust them to design other safe AIs with
<br>
greater intelligence, and to verify the safety of each
<br>
other's designs.
<br>
<p><p>** There are other objections to the specific form of
<br>
regulation that I advocate, rather then regulation in
<br>
general:
<br>
<p>&nbsp;&nbsp;g. You advocate regulations on reinforcement values, but
<br>
&nbsp;&nbsp;some designes don't rely on them.
<br>
<p>Based on knowledge of human brains, and on the Solomonoff
<br>
Induction model of intelligence, I think the essence of
<br>
intelligence is reinforcement learning. Reinforcement
<br>
learning is very hard to do effectively in general situations
<br>
(like those faced by humans), which leads to all sorts of
<br>
design optimizations (e.g., human consciousness) that don't
<br>
look much like reinforcement learning. But at base they are
<br>
all trying to learn behaviors for satisfying some values.
<br>
<p>&nbsp;&nbsp;h. An AI based on reinforcement values for human happiness
<br>
&nbsp;&nbsp;can't be any more intelligent than humans.
<br>
<p>Values and intelligence are independent. As long as there
<br>
is no fixed-length algorithm that optimally satisfies the
<br>
values (i.e., values are not just winning at tic-tac-toe or
<br>
chess) there is no limit to how much intelligence can be
<br>
brought to bear to satisfying the values. In particular,
<br>
values for human happiness can drive unlimited intelligence,
<br>
given the insatiable nature of human aspirations.
<br>
<p>&nbsp;&nbsp;i. Reinforcement values for human happiness are too
<br>
&nbsp;&nbsp;specific to humans. An AI should have universal altruism.
<br>
<p>Universally altruistic values can only be defined in terms
<br>
of symbols (i.e., statements in human language) which must
<br>
be grounded in sensory experience before they have real
<br>
meaning. An AI will have grounding for language only after
<br>
it has done a lot of reinforcement learning, but values
<br>
are necessary for such learning. The third point of my
<br>
critique of the SIAI friendliness analysis was the lack of
<br>
values to reinforce its learning until the meaning of its
<br>
friendliness supergoal could be learned.
<br>
<p>Reinforcement values for human happiness can be implemented
<br>
using current or near-future machine learning technology
<br>
for recognizing emotions in human facial expresssions,
<br>
voices and body language. These values have grounded
<br>
definitions.
<br>
<p>I think that a number of current AI efforts underestimate
<br>
the importance of solving the grounding problem. This
<br>
applies not only to grounding symbols in sensory experience,
<br>
but grounding reason and planning in learning. Speculation
<br>
about AI values that can only be expressed in language also
<br>
fails to appreciate the grounding problem.
<br>
<p>There are always trade-offs, with winners and losers, that
<br>
must be faced by any set of values, even universal altruism.
<br>
That is, in this world there is no behavior that always
<br>
gives everyone what they want. I think it is likely that
<br>
&quot;universal altruism&quot; is one of those language constructs
<br>
that has no realization (like &quot;the set of all sets that do
<br>
not contain themselves&quot;).
<br>
<p>Any set of values that tries to protect interests broader
<br>
than human wellfare may motivate an AI behavior that has
<br>
negative consequences for humans. In the extreme, the AI
<br>
may destroy humanity because of its innate xenophobia or
<br>
violence. Some people think this may be the right thing
<br>
to do, but I cannot advocate any AI with such a possible
<br>
consequence. I only trust values that are grounded in human
<br>
wellfare, as expressed by human happiness.
<br>
<p>Using human happiness for AI reinforcement values equates
<br>
AI values with human values, and keeps humans &quot;in the loop&quot;
<br>
of AI thoughts. Human values do gradually evolve, as for
<br>
example xenophobia declines (its bad, but not as bad as it
<br>
used to be). My own hope is that super-intelligent AIs with
<br>
reinforcement values for human happiness will accelerate
<br>
the pace of evolution of human values. For example, the AI
<br>
will learn that tolerant people are happier than intolerant
<br>
people, and promote tolerance in human society.
<br>
<p><p>** Summary
<br>
<p>I am sure some people won't accept my answers to these
<br>
objections, and be skeptical of regulation. I admit that
<br>
regulation is not guaranteed to produce a safe singularity.
<br>
But I think the alternatives are worse. In my opinion,
<br>
prohibiting AI is impossible, and unregulated AI makes an
<br>
unsafe singularity almost certain.
<br>
<p>----------------------------------------------------------
<br>
Bill Hibbard, SSEC, 1225 W. Dayton St., Madison, WI  53706
<br>
<a href="mailto:test@demedici.ssec.wisc.edu?Subject=RE:%20SIAI's%20flawed%20friendliness%20analysis">test@demedici.ssec.wisc.edu</a>  608-263-4427  fax: 608-263-6738
<br>
<a href="http://www.ssec.wisc.edu/~billh/vis.html">http://www.ssec.wisc.edu/~billh/vis.html</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6822.html">Ben Goertzel: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>Previous message:</strong> <a href="6820.html">Samantha Atkins: "Re: Failure of AI so far"</a>
<li><strong>In reply to:</strong> <a href="6809.html">Ben Goertzel: "RE: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6822.html">Ben Goertzel: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6822.html">Ben Goertzel: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6823.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6821">[ date ]</a>
<a href="index.html#6821">[ thread ]</a>
<a href="subject.html#6821">[ subject ]</a>
<a href="author.html#6821">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
