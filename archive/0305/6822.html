<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: SIAI's flawed friendliness analysis</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: SIAI's flawed friendliness analysis">
<meta name="Date" content="2003-05-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: SIAI's flawed friendliness analysis</h1>
<!-- received="Mon May 26 16:41:33 2003" -->
<!-- isoreceived="20030526224133" -->
<!-- sent="Mon, 26 May 2003 18:41:52 -0400" -->
<!-- isosent="20030526224152" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: SIAI's flawed friendliness analysis" -->
<!-- id="LAEGJLOGJIOELPNIOOAJIEGEFAAA.ben@goertzel.org" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="Pine.GSO.4.44.0305261641340.20116-100000@demedici.ssec.wisc.edu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20SIAI's%20flawed%20friendliness%20analysis"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Mon May 26 2003 - 16:41:52 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6823.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Previous message:</strong> <a href="6821.html">Bill Hibbard: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>In reply to:</strong> <a href="6821.html">Bill Hibbard: "RE: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6823.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6822">[ date ]</a>
<a href="index.html#6822">[ thread ]</a>
<a href="subject.html#6822">[ subject ]</a>
<a href="author.html#6822">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Bill,
<br>
<p>Thanks for the very detailed and well-thought out response.
<br>
<p>My differences of opinion from you all stem from differences of intuitive
<br>
judgment, which cannot be resolved easily through argumentation.
<br>
<p>For instance:
<br>
<p>1) I think the chance of a safe singularity from an unregulated source is
<br>
higher than you do [there are both responsible and irresponsible AI groups
<br>
out there]
<br>
<p>2) I think the chance of an unsafe singularity coming out of government
<br>
research or sponsorship is higher than you do [government has screwed a lot
<br>
of things up in the past]
<br>
<p>3) I think the Novamente design embodies a workable solution to the symbol
<br>
grounding and reinforcement learning problems; hence I suspect that the
<br>
beginnings of artificial general intelligence are not decades off.  Even if
<br>
a lot of things go wrong and our funding remains minimal, we should get
<br>
there before a decade has expired.  If I'm right about my AI design, of
<br>
course ;-)
<br>
<p>4) I think that the government and public may NOT wake up to the real
<br>
potential of AGI during the interval between the first real AGI and the
<br>
Singularity.  Their attention may well be distracted by concurrent
<br>
developments in biotech that are found morally disturbing by mainstream
<br>
society.  AI does not have the emotional and cultural gripping power of
<br>
human cloning, genetic engineering, human-computer hybridization, etc.
<br>
<p>-- Ben
<br>
<p><em>&gt; -----Original Message-----
</em><br>
<em>&gt; From: <a href="mailto:owner-sl4@sl4.org?Subject=RE:%20SIAI's%20flawed%20friendliness%20analysis">owner-sl4@sl4.org</a> [mailto:<a href="mailto:owner-sl4@sl4.org?Subject=RE:%20SIAI's%20flawed%20friendliness%20analysis">owner-sl4@sl4.org</a>]On Behalf Of Bill
</em><br>
<em>&gt; Hibbard
</em><br>
<em>&gt; Sent: Monday, May 26, 2003 5:44 PM
</em><br>
<em>&gt; To: <a href="mailto:sl4@sl4.org?Subject=RE:%20SIAI's%20flawed%20friendliness%20analysis">sl4@sl4.org</a>
</em><br>
<em>&gt; Subject: RE: SIAI's flawed friendliness analysis
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; This discussion has split into many threads, and I'll bring
</em><br>
<em>&gt; them together into this single response. Ben's comments are
</em><br>
<em>&gt; a good starting point for this, and I'll address all the
</em><br>
<em>&gt; recent questions.
</em><br>
<em>&gt;
</em><br>
<em>&gt; On Fri, 23 May 2003, Ben Goertzel wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; There are a lot of good points and interesting issues mixed up
</em><br>
<em>&gt; here, but I
</em><br>
<em>&gt; &gt; think the most key point is the division between
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; -- those who believe a hard takeoff is reasonably likely, based
</em><br>
<em>&gt; on a radical
</em><br>
<em>&gt; &gt; insight in AI design coupled with a favorable trajectory of
</em><br>
<em>&gt; self-improvemetn
</em><br>
<em>&gt; &gt; of a particular AI system
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; -- those who believe in a soft takeoff, in which true AI is approached
</em><br>
<em>&gt; &gt; gradually [in which case government regulation, careful peer
</em><br>
<em>&gt; review and so
</em><br>
<em>&gt; &gt; forth are potentially relevant]
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; The soft takeoff brings with it many obvious possibilities for
</em><br>
<em>&gt; safeguarding,
</em><br>
<em>&gt; &gt; which are not offered in the hard takeoff scenario.  These
</em><br>
<em>&gt; possibilities are
</em><br>
<em>&gt; &gt; the ones Bill Hibbard is exploring, I think.  A lot of what
</em><br>
<em>&gt; SIAI is saying
</em><br>
<em>&gt; &gt; is more relevant to the hard takeoff scenario, on the other hand.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; My own projection is a semi-hard takeoff, which doesn't really
</em><br>
<em>&gt; bring much
</em><br>
<em>&gt; &gt; reassurance...
</em><br>
<em>&gt;
</em><br>
<em>&gt; I think we'll eventually get to a time (the singularity) when
</em><br>
<em>&gt; intelligence increases very quickly to very high levels. But I
</em><br>
<em>&gt; think it will take a long time to get there, during a sort of
</em><br>
<em>&gt; soft takeoff. In particular it will be years or even decades
</em><br>
<em>&gt; from the first intelligent machines until the true singularity,
</em><br>
<em>&gt; and it could be decades from now until the first intelligent
</em><br>
<em>&gt; machines. I agree with Donald Norman that people tend to
</em><br>
<em>&gt; overestimate the short-term progress of technological change,
</em><br>
<em>&gt; and underestimate the long-term effects.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I think real intelligence is decades away because no current
</em><br>
<em>&gt; research is making any real progress on the grounding problem,
</em><br>
<em>&gt; which is the problem of grounding symbols in sensory
</em><br>
<em>&gt; experience and grounding reasoning and planning in learning.
</em><br>
<em>&gt; That is, you cannot reason intelligently about horses unless
</em><br>
<em>&gt; the word horse is connected to sight, sound, smell and touch
</em><br>
<em>&gt; experiences with horses. Solving the grounding problem will
</em><br>
<em>&gt; require much faster computers than are being used for current
</em><br>
<em>&gt; AI research.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I think there will be years or decades from the first real
</em><br>
<em>&gt; machine intelligence until the singularity because of the
</em><br>
<em>&gt; likelyhood of difficult technical problems even after the
</em><br>
<em>&gt; first signs of machine intelligence, and because of the
</em><br>
<em>&gt; amount of learning for intelligence to acheive its true
</em><br>
<em>&gt; potential. Applying intelligence effectively (we might call
</em><br>
<em>&gt; this wisdom) requires many fine value judgements that can
</em><br>
<em>&gt; only be learned from experience. Humans require decades of
</em><br>
<em>&gt; learning for their intelligence to mature. A super-intelligent
</em><br>
<em>&gt; machine may learn faster, but it may also need a lot more
</em><br>
<em>&gt; experience for its super-intelligence to mature (just as
</em><br>
<em>&gt; higher animals generally take longer to mature than lower
</em><br>
<em>&gt; animals).
</em><br>
<em>&gt;
</em><br>
<em>&gt; There is some chance that the first intelligent machines will
</em><br>
<em>&gt; be hidden from the public. But probably not for long, because
</em><br>
<em>&gt; they will be built in a wealthy and open society like the U.S.,
</em><br>
<em>&gt; with lots of whistle blowers and where exciting news has a way
</em><br>
<em>&gt; of getting out. Furthermore, a machine designed as I advocate,
</em><br>
<em>&gt; with values for human happiness, or a machine designed as the
</em><br>
<em>&gt; SIAI advocates, with a friendliness super-goal, would create
</em><br>
<em>&gt; the singularity openly rather than hiding it from humans. It
</em><br>
<em>&gt; is hard to imagine a safe singularity created in secret.
</em><br>
<em>&gt;
</em><br>
<em>&gt; There are three broad public policy choices for AI:
</em><br>
<em>&gt;
</em><br>
<em>&gt; 1. Prohibit it, as advocated by Bill Joy in his April 2000 Wired
</em><br>
<em>&gt; article &quot;Why the Future Doesn't Need Us&quot;.
</em><br>
<em>&gt;
</em><br>
<em>&gt; 2. Allow it without regulation, as advocated by the SIAI and
</em><br>
<em>&gt; most members of the SL4 mailing list.
</em><br>
<em>&gt;
</em><br>
<em>&gt; 3. Allow it but regulate it, as I advocate.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I think prohibiting AI is technically impossible and politically
</em><br>
<em>&gt; unlikely, and unregulated AI is politically impossible and will
</em><br>
<em>&gt; almost certainly be unsafe for humans. So we have no alternative
</em><br>
<em>&gt; but to find our way through the difficulties of regulating AI.
</em><br>
<em>&gt; In more detail:
</em><br>
<em>&gt;
</em><br>
<em>&gt; 1. Prohibit AI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; In his article, Bill Joy is pessimistic about prohibiting AI
</em><br>
<em>&gt; because people will want the benefits. It will be politically
</em><br>
<em>&gt; difficult to decide the right point to stop a technology whose
</em><br>
<em>&gt; development continually creates wealth and relieves people of
</em><br>
<em>&gt; the need to work.
</em><br>
<em>&gt;
</em><br>
<em>&gt; As several people have pointed out, it will be technically
</em><br>
<em>&gt; impossible to prevent people from building outlaw AIs,
</em><br>
<em>&gt; especially as technology matures. The only way to do it
</em><br>
<em>&gt; would be to stop technological progress world wide, which
</em><br>
<em>&gt; won't happen.
</em><br>
<em>&gt;
</em><br>
<em>&gt; 2. Allow AI without regulation.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Ben's question about timing is relevant here. If you think
</em><br>
<em>&gt; that the singularity will happen so quickly that the public
</em><br>
<em>&gt; and the government won't have time to act to control the
</em><br>
<em>&gt; singularity once they realize that machines are becoming
</em><br>
<em>&gt; intelligent, then you don't have to worry about regulation
</em><br>
<em>&gt; because it will be too late.
</em><br>
<em>&gt;
</em><br>
<em>&gt; If the public and the government have enough time to react,
</em><br>
<em>&gt; they will. People have been well primed for the dangers of
</em><br>
<em>&gt; AI by science fiction books and movies. When machines start
</em><br>
<em>&gt; surprising them with their intelligence, many people will be
</em><br>
<em>&gt; freightened and then politicians will get excited. They will
</em><br>
<em>&gt; be no more likely to allow unregulated AI than they are to
</em><br>
<em>&gt; allow unregulated nuclear power. The only question is whether
</em><br>
<em>&gt; they will try to prohibit or regulate AI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Wealthy and powerful institutions will have motives to build
</em><br>
<em>&gt; unsafe AIs. Even generally well-meaning institutions may
</em><br>
<em>&gt; fatally compromise safety for mildly selfish motives. Without
</em><br>
<em>&gt; broad public insistence on aggressive safety regulation, one
</em><br>
<em>&gt; of these unsafe AIs will likely be the seed for the
</em><br>
<em>&gt; singularity.
</em><br>
<em>&gt;
</em><br>
<em>&gt; 3. Allow AI with regulation.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Ben's question about timing is relevant here too. The need
</em><br>
<em>&gt; and political drive for regulation won't be serious until
</em><br>
<em>&gt; mchines start exhibiting real intelligence, and that is
</em><br>
<em>&gt; decades away. Even if you disagree about the timing, it is
</em><br>
<em>&gt; still true that regulation won't interfere with current
</em><br>
<em>&gt; research until some project acheives an AI breakthrough. At
</em><br>
<em>&gt; the current stage of development, with lots of experiments
</em><br>
<em>&gt; but nothing approaching real intelligence, regulation would
</em><br>
<em>&gt; be counter-productive.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Like so many things in politics, regulation is the best
</em><br>
<em>&gt; choice among a set of bad alternatives. Here is a list of
</em><br>
<em>&gt; objections, with my answers:
</em><br>
<em>&gt;
</em><br>
<em>&gt;   a. Regulation cannot work because no one can understand my
</em><br>
<em>&gt;   designs. Government employees are too stupid to understand
</em><br>
<em>&gt;   designs.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Government employees include lots of very smart people, like
</em><br>
<em>&gt; those who worked on the Manhattan Project and those who are
</em><br>
<em>&gt; finding cures for diseases. While it is healthy for citizens
</em><br>
<em>&gt; to be skeptical of politicians and government, thinking that
</em><br>
<em>&gt; all politicians and government employees are stupid is just
</em><br>
<em>&gt; an ignorant prejudice.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The regulators will understand designs because the burden
</em><br>
<em>&gt; will be on the designers to satisfy regulators (many of whom
</em><br>
<em>&gt; will be very smart) of the safety of their designs, as with
</em><br>
<em>&gt; any dangerous technology.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Even if some smart designers don't want to cooperate with
</em><br>
<em>&gt; regulators, other designers just as smart will cooperate.
</em><br>
<em>&gt;
</em><br>
<em>&gt;   b. Regulation will hobble cooperating projects, enabling
</em><br>
<em>&gt;   non-cooperating unsafe AI projects create the singularity
</em><br>
<em>&gt;   first.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Non-cooperating projects will be hobbled by the need to hide
</em><br>
<em>&gt; their resource use (large computers, smart designers, network
</em><br>
<em>&gt; access, etc).
</em><br>
<em>&gt;
</em><br>
<em>&gt; As long as regulation is aggressively enforced, major
</em><br>
<em>&gt; corporations and government agencies will cooperate and
</em><br>
<em>&gt; bring their huge resources to the effort for safe AI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The government will have access to very smart people who can
</em><br>
<em>&gt; help more than hinder the designers they are inspecting.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Given the importance of AI, it is plausible that the U.S.
</em><br>
<em>&gt; government itself will create a project like the Manhattan
</em><br>
<em>&gt; Project for developing safe AI, with resources way beyond
</em><br>
<em>&gt; those available to non-cooperating groups. Currently, the
</em><br>
<em>&gt; U.S. GDP is about $10 trillion, the federal government
</em><br>
<em>&gt; budget is about $2.3 trillion, the defense budget is $0.4
</em><br>
<em>&gt; trillion, and global spending on information technology is
</em><br>
<em>&gt; $3 trillion. When the public sees intelligent machines and
</em><br>
<em>&gt; starts asking their elected representatives to do something
</em><br>
<em>&gt; about it, and those representatives hear from experts
</em><br>
<em>&gt; about the dangers of the singularity, it is easy to imagine
</em><br>
<em>&gt; a federal safe AI project with a budget on the scale of
</em><br>
<em>&gt; these numbers.
</em><br>
<em>&gt;
</em><br>
<em>&gt;   c. A non-cooperating project may destroy the world by
</em><br>
<em>&gt;   using AI to create a nano-technology &quot;grey goo&quot; attack.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This is possible. But even without AI, there may be a world
</em><br>
<em>&gt; destroying attack using nano-technology or genetically
</em><br>
<em>&gt; engineered micro-organisms. My judgement is that the
</em><br>
<em>&gt; probability of unsafe AI from a lack of regulation (I think
</em><br>
<em>&gt; this is close to 1.0) is greater than the marginal increase
</em><br>
<em>&gt; in the probability of a nano-technology attack caused by
</em><br>
<em>&gt; regulation of AI (as explained in my answer to the previous
</em><br>
<em>&gt; objection, active government regulation won't necessarily
</em><br>
<em>&gt; slow safe AI down relative to unsafe AI).
</em><br>
<em>&gt;
</em><br>
<em>&gt;   d. Even if AI is regulated in most countries, there may
</em><br>
<em>&gt;   be others where it is not.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This is a disturbing problem. However, the non-democracies
</em><br>
<em>&gt; are gradually disappearing, and the democracies are
</em><br>
<em>&gt; gradually learning to work together. Hopefully the world
</em><br>
<em>&gt; will be more cooperative by the time the singularity
</em><br>
<em>&gt; arrives.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Democratic countries are wealthier than non-democracies,
</em><br>
<em>&gt; so may create a safe singularity before an unsafe
</em><br>
<em>&gt; singularity can be created elsewhere.
</em><br>
<em>&gt;
</em><br>
<em>&gt;   e. We can't trust an AI because we can't know what its
</em><br>
<em>&gt;   thinking. An AI will continue to develop and design
</em><br>
<em>&gt;   other AIs that are beyond the ability of human
</em><br>
<em>&gt;   regulators to understand.
</em><br>
<em>&gt;
</em><br>
<em>&gt; There is no way to trace or predict the detailed thoughts
</em><br>
<em>&gt; of an AI, but we can make the general prediction that it
</em><br>
<em>&gt; will try to satisfy its reinforcement values. The safety
</em><br>
<em>&gt; of an AI is primarily determined by its values (its
</em><br>
<em>&gt; learning and simulation algorithms also need to be
</em><br>
<em>&gt; accurate).
</em><br>
<em>&gt;
</em><br>
<em>&gt; I would trust an AI designed by another safe AI, with
</em><br>
<em>&gt; reinforcement values for human happiness. It may decide
</em><br>
<em>&gt; that we would be happier if its design was checked by
</em><br>
<em>&gt; another independently-designed safe AI, and so seek such
</em><br>
<em>&gt; peer review.
</em><br>
<em>&gt;
</em><br>
<em>&gt;   f. The intelligence of AIs will be limited by the
</em><br>
<em>&gt;   ability of human regulators to understand their designs.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This is related to the previous objection. Once we have
</em><br>
<em>&gt; safe AIs, we can trust them to design other safe AIs with
</em><br>
<em>&gt; greater intelligence, and to verify the safety of each
</em><br>
<em>&gt; other's designs.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; ** There are other objections to the specific form of
</em><br>
<em>&gt; regulation that I advocate, rather then regulation in
</em><br>
<em>&gt; general:
</em><br>
<em>&gt;
</em><br>
<em>&gt;   g. You advocate regulations on reinforcement values, but
</em><br>
<em>&gt;   some designes don't rely on them.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Based on knowledge of human brains, and on the Solomonoff
</em><br>
<em>&gt; Induction model of intelligence, I think the essence of
</em><br>
<em>&gt; intelligence is reinforcement learning. Reinforcement
</em><br>
<em>&gt; learning is very hard to do effectively in general situations
</em><br>
<em>&gt; (like those faced by humans), which leads to all sorts of
</em><br>
<em>&gt; design optimizations (e.g., human consciousness) that don't
</em><br>
<em>&gt; look much like reinforcement learning. But at base they are
</em><br>
<em>&gt; all trying to learn behaviors for satisfying some values.
</em><br>
<em>&gt;
</em><br>
<em>&gt;   h. An AI based on reinforcement values for human happiness
</em><br>
<em>&gt;   can't be any more intelligent than humans.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Values and intelligence are independent. As long as there
</em><br>
<em>&gt; is no fixed-length algorithm that optimally satisfies the
</em><br>
<em>&gt; values (i.e., values are not just winning at tic-tac-toe or
</em><br>
<em>&gt; chess) there is no limit to how much intelligence can be
</em><br>
<em>&gt; brought to bear to satisfying the values. In particular,
</em><br>
<em>&gt; values for human happiness can drive unlimited intelligence,
</em><br>
<em>&gt; given the insatiable nature of human aspirations.
</em><br>
<em>&gt;
</em><br>
<em>&gt;   i. Reinforcement values for human happiness are too
</em><br>
<em>&gt;   specific to humans. An AI should have universal altruism.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Universally altruistic values can only be defined in terms
</em><br>
<em>&gt; of symbols (i.e., statements in human language) which must
</em><br>
<em>&gt; be grounded in sensory experience before they have real
</em><br>
<em>&gt; meaning. An AI will have grounding for language only after
</em><br>
<em>&gt; it has done a lot of reinforcement learning, but values
</em><br>
<em>&gt; are necessary for such learning. The third point of my
</em><br>
<em>&gt; critique of the SIAI friendliness analysis was the lack of
</em><br>
<em>&gt; values to reinforce its learning until the meaning of its
</em><br>
<em>&gt; friendliness supergoal could be learned.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Reinforcement values for human happiness can be implemented
</em><br>
<em>&gt; using current or near-future machine learning technology
</em><br>
<em>&gt; for recognizing emotions in human facial expresssions,
</em><br>
<em>&gt; voices and body language. These values have grounded
</em><br>
<em>&gt; definitions.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I think that a number of current AI efforts underestimate
</em><br>
<em>&gt; the importance of solving the grounding problem. This
</em><br>
<em>&gt; applies not only to grounding symbols in sensory experience,
</em><br>
<em>&gt; but grounding reason and planning in learning. Speculation
</em><br>
<em>&gt; about AI values that can only be expressed in language also
</em><br>
<em>&gt; fails to appreciate the grounding problem.
</em><br>
<em>&gt;
</em><br>
<em>&gt; There are always trade-offs, with winners and losers, that
</em><br>
<em>&gt; must be faced by any set of values, even universal altruism.
</em><br>
<em>&gt; That is, in this world there is no behavior that always
</em><br>
<em>&gt; gives everyone what they want. I think it is likely that
</em><br>
<em>&gt; &quot;universal altruism&quot; is one of those language constructs
</em><br>
<em>&gt; that has no realization (like &quot;the set of all sets that do
</em><br>
<em>&gt; not contain themselves&quot;).
</em><br>
<em>&gt;
</em><br>
<em>&gt; Any set of values that tries to protect interests broader
</em><br>
<em>&gt; than human wellfare may motivate an AI behavior that has
</em><br>
<em>&gt; negative consequences for humans. In the extreme, the AI
</em><br>
<em>&gt; may destroy humanity because of its innate xenophobia or
</em><br>
<em>&gt; violence. Some people think this may be the right thing
</em><br>
<em>&gt; to do, but I cannot advocate any AI with such a possible
</em><br>
<em>&gt; consequence. I only trust values that are grounded in human
</em><br>
<em>&gt; wellfare, as expressed by human happiness.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Using human happiness for AI reinforcement values equates
</em><br>
<em>&gt; AI values with human values, and keeps humans &quot;in the loop&quot;
</em><br>
<em>&gt; of AI thoughts. Human values do gradually evolve, as for
</em><br>
<em>&gt; example xenophobia declines (its bad, but not as bad as it
</em><br>
<em>&gt; used to be). My own hope is that super-intelligent AIs with
</em><br>
<em>&gt; reinforcement values for human happiness will accelerate
</em><br>
<em>&gt; the pace of evolution of human values. For example, the AI
</em><br>
<em>&gt; will learn that tolerant people are happier than intolerant
</em><br>
<em>&gt; people, and promote tolerance in human society.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; ** Summary
</em><br>
<em>&gt;
</em><br>
<em>&gt; I am sure some people won't accept my answers to these
</em><br>
<em>&gt; objections, and be skeptical of regulation. I admit that
</em><br>
<em>&gt; regulation is not guaranteed to produce a safe singularity.
</em><br>
<em>&gt; But I think the alternatives are worse. In my opinion,
</em><br>
<em>&gt; prohibiting AI is impossible, and unregulated AI makes an
</em><br>
<em>&gt; unsafe singularity almost certain.
</em><br>
<em>&gt;
</em><br>
<em>&gt; ----------------------------------------------------------
</em><br>
<em>&gt; Bill Hibbard, SSEC, 1225 W. Dayton St., Madison, WI  53706
</em><br>
<em>&gt; <a href="mailto:test@demedici.ssec.wisc.edu?Subject=RE:%20SIAI's%20flawed%20friendliness%20analysis">test@demedici.ssec.wisc.edu</a>  608-263-4427  fax: 608-263-6738
</em><br>
<em>&gt; <a href="http://www.ssec.wisc.edu/~billh/vis.html">http://www.ssec.wisc.edu/~billh/vis.html</a>
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6823.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Previous message:</strong> <a href="6821.html">Bill Hibbard: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>In reply to:</strong> <a href="6821.html">Bill Hibbard: "RE: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6823.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6822">[ date ]</a>
<a href="index.html#6822">[ thread ]</a>
<a href="subject.html#6822">[ subject ]</a>
<a href="author.html#6822">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
