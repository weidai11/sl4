<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=Windows-1252">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Friendly AI in &quot;Positive Transcension&quot;</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Friendly AI in &quot;Positive Transcension&quot;">
<meta name="Date" content="2004-02-15">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Friendly AI in &quot;Positive Transcension&quot;</h1>
<!-- received="Sun Feb 15 11:12:31 2004" -->
<!-- isoreceived="20040215181231" -->
<!-- sent="Sun, 15 Feb 2004 13:19:14 -0500" -->
<!-- isosent="20040215181914" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Friendly AI in &quot;Positive Transcension&quot;" -->
<!-- id="BMECIIDGKPGNFPJLIDNPMEMICMAA.ben@goertzel.org" -->
<!-- charset="Windows-1252" -->
<!-- inreplyto="402ED90B.3080504@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Friendly%20AI%20in%20&quot;Positive%20Transcension&quot;"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Sun Feb 15 2004 - 11:19:14 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7940.html">Ben Goertzel: "RE: Encouraging a Positive Transcension"</a>
<li><strong>Previous message:</strong> <a href="7938.html">Mikko Särelä: "RE: Encouraging a Positive Transcension"</a>
<li><strong>In reply to:</strong> <a href="7922.html">Eliezer S. Yudkowsky: "Friendly AI in &quot;Positive Transcension&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7941.html">Eliezer S. Yudkowsky: "Re: Friendly AI in &quot;Positive Transcension&quot;"</a>
<li><strong>Reply:</strong> <a href="7941.html">Eliezer S. Yudkowsky: "Re: Friendly AI in &quot;Positive Transcension&quot;"</a>
<li><strong>Reply:</strong> <a href="7944.html">Jef Allbright: "Positive Transcension"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7939">[ date ]</a>
<a href="index.html#7939">[ thread ]</a>
<a href="subject.html#7939">[ subject ]</a>
<a href="author.html#7939">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi,
<br>
<p>I've lost taste for lengthy point-by-point email arguments, but I'll respond
<br>
to a few of your major themes and points.
<br>
<p>Before responding at all I'm going to make a request of you.  Please
<br>
summarize,
<br>
<p>FIRST, in a single, clear, not-too-long sentence
<br>
<p>SECOND, in a single, clear, not-too-long paragraph
<br>
<p>your own view of your theory of Friendly AI.  I will then paraphrase your
<br>
own summaries, in my own words, in my revised essay.  I have no desire to
<br>
misrepresent your ideas, of course.
<br>
<p>I'm sure that I don't fully understand your ideas and intentions when you
<br>
wrote CFAI, nor your current intentions and ideas.  However, I have read
<br>
them and talked to you about them more than once.  The problem is not so
<br>
much that I'm not aware of the details, but that I think the details are
<br>
consistent with certain summaries that you think they're inconsistent with
<br>
;-)
<br>
<p>The summary you gave in your response email was almost clear, but not
<br>
quite..
<br>
<p>In line with your request, I will add a brief disclaimer to the essay noting
<br>
that you feel I didn't correctly interpret your ideas, and that the reader
<br>
should turn to your writings directly to form their own opinion.
<br>
<p>Next, to clarify one small point, you say
<br>
<p><em>&gt; f)  Friendly AI is frickin' complicated, so please stop summarizing it as
</em><br>
<em>&gt; &quot;hardwiring benevolence to humans&quot;.  I ain't bloody Asimov.
</em><br>
<p>In fact what I said was &quot;programming or otherwise inculcating&quot; benevolence
<br>
to humans -- i.e. by saying &quot;inculcating&quot; I meant to encompass teaching not
<br>
just programming, or combinations of teaching and programming, etc.
<br>
<p>So far as I can tell, the biggest difference you see between my rendition of
<br>
your views, and your actual views, is that instead of &quot;programming or
<br>
otherwise inculcating benevolence to humans&quot;, you'd rather speak about
<br>
&quot;programming or otherwise inculcating humane morality in an AI&quot;.  And you
<br>
consider this an approximation for &quot;an architecture that explicitly treats
<br>
itself as an approximation to the AI that would be constructed by a humane
<br>
morality.&quot;  I can see the difference you're pointing out but I don't see it
<br>
as such a big difference -- I guess it all depends on how you ground the
<br>
term &quot;benevolence.&quot;  One could fairly interpret &quot;benevolence to humans&quot; as
<br>
&quot;acting toward humans in accordance with humane morality.&quot;  In that case,
<br>
what my formulation misses is mainly that you want an AI that acts toward
<br>
other things with humane morality as well, not just toward humans.
<br>
<p>I still have the basic complaint that &quot;humane morality&quot; is a very narrow
<br>
thing to be projecting throughout the cosmos.  It's also a rather funky and
<br>
uncomfortable abstraction, given the immense diversity of human ethical
<br>
systems throughout history and across the globe.
<br>
<p>I do see your point that my more abstract concepts like growth, choice and
<br>
joy are grounded -- in my mind -- in a lot of human thoughts and feelings.
<br>
Very true, very deep.  But that doesn't mean that I can't pick and choose
<br>
from among the vast contradictory morass of &quot;humane morality,&quot; certain
<br>
aspects that I think are worthy of projecting across the cosmos, because
<br>
they have more fundamental importance than the other aspects, less
<br>
narrowness of meaning.
<br>
<p>Now, on to a couple nitpicks (I could give many more, but lack the
<br>
time/motivation):
<br>
<p>At one point in your message you seem to be accusing me of thinking the
<br>
creation of ethically-positive superhuman AI systems is an &quot;easy&quot; problem.
<br>
Of course, that is not the case.  I explicitly state that I consider it a
<br>
very difficult problem that requires a huge amount of research -- and in
<br>
fact, I even wonder whether it will be necessary to create a
<br>
relatively-technologically-static global police state in order to allow this
<br>
research to go on systematically and in peace!
<br>
<p>I guess you badly misunderstood some of the more philosophical points in my
<br>
essay.  For instance, you respond to something or other I said by telling me
<br>
that &quot;the universe is not on [my] side&quot; -- but I certainly didn't mean to
<br>
say anything implying that the universe is &quot;on my side&quot; in any reasonable
<br>
sense...
<br>
<p>-- Ben G
<br>
<p><p><em>&gt; -----Original Message-----
</em><br>
<em>&gt; From: <a href="mailto:owner-sl4@sl4.org?Subject=RE:%20Friendly%20AI%20in%20&quot;Positive%20Transcension&quot;">owner-sl4@sl4.org</a> [mailto:<a href="mailto:owner-sl4@sl4.org?Subject=RE:%20Friendly%20AI%20in%20&quot;Positive%20Transcension&quot;">owner-sl4@sl4.org</a>]On Behalf Of Eliezer
</em><br>
<em>&gt; S. Yudkowsky
</em><br>
<em>&gt; Sent: Saturday, February 14, 2004 9:27 PM
</em><br>
<em>&gt; To: <a href="mailto:sl4@sl4.org?Subject=RE:%20Friendly%20AI%20in%20&quot;Positive%20Transcension&quot;">sl4@sl4.org</a>
</em><br>
<em>&gt; Subject: Friendly AI in &quot;Positive Transcension&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Ben Goertzel wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Here is a revised version of my recent futurist essay, improved
</em><br>
<em>&gt; &gt; significantly in parts due to the feedback of several readers
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; www.goertzel.org/papers/PositiveTranscension.htm
</em><br>
<em>&gt;
</em><br>
<em>&gt; Ben, looking over this paper, I would like you to attach a disclaimer that
</em><br>
<em>&gt; runs something like this:
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;Note:  Eliezer Yudkowsky has looked over the sections of this paper
</em><br>
<em>&gt; dealing with Friendly AI, and says that not only did I not understand FAI,
</em><br>
<em>&gt; but I wasn't even in the remote ballpark - Yudkowsky says he couldn't even
</em><br>
<em>&gt; have figured out I was talking about his theory if I hadn't mentioned it
</em><br>
<em>&gt; by name, and that all of my representations are absolutely antithetical to
</em><br>
<em>&gt; spirit of Friendly AI as he meant it.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; No offense is meant.  Okay, so you utterly failed to get FAI; so did
</em><br>
<em>&gt; everyone else.  Anyway, here are some examples of specific problems:
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; For Eliezer Yudkowsky, the preservation of human life and human will is
</em><br>
<em>&gt; &gt;  of almost supreme importance.
</em><br>
<em>&gt;
</em><br>
<em>&gt; What I wish to preserve is sentient life and humane morality.  Presently,
</em><br>
<em>&gt; humans are the only sentients (I don't think it includes chimps, but I'm
</em><br>
<em>&gt; not sure), and it is *only* humans that embody humane morality.  Lose the
</em><br>
<em>&gt; humans and you lose everything that you value.  I know you don't believe
</em><br>
<em>&gt; this, but it's why I place the emphasis.
</em><br>
<em>&gt;
</em><br>
<em>&gt; You know how economists are always talking about utility functions?  If
</em><br>
<em>&gt; you wanted to take a very very VERY rough stab at FAI, it would be
</em><br>
<em>&gt; something like this:
</em><br>
<em>&gt;
</em><br>
<em>&gt; 1)  This is a human.
</em><br>
<em>&gt;
</em><br>
<em>&gt; 2)  The human being embodies a utility function, preferences,
</em><br>
<em>&gt; things-that-output-choices.  (No, not really, but we're taking
</em><br>
<em>&gt; rough stabs
</em><br>
<em>&gt; here.)
</em><br>
<em>&gt;
</em><br>
<em>&gt; 3)  This is what the human being's preferences would be, taking the limit
</em><br>
<em>&gt; as knowledge approaches a perfect model of reality, and computing power
</em><br>
<em>&gt; available goes to infinity.  Call this a &quot;volition&quot;.  The limit is not
</em><br>
<em>&gt; even remotely well-defined.  On to step 4.
</em><br>
<em>&gt;
</em><br>
<em>&gt; 4)  Let the volition examine itself.  This is the renormalized volition,
</em><br>
<em>&gt; or the volition under reflection.  Take the limit to logical omniscience
</em><br>
<em>&gt; on this too.  Now it's even less well-defined then before.
</em><br>
<em>&gt;
</em><br>
<em>&gt; 5)  Instead of using the renormalized volition from a specific human, use
</em><br>
<em>&gt; a &quot;typical&quot; human starting point derived from the evolutionary psychology
</em><br>
<em>&gt; specific to the species.  This is a &quot;humane&quot; morality.  Oh, and
</em><br>
<em>&gt; the limit?
</em><br>
<em>&gt;   It ain't getting any better-defined.
</em><br>
<em>&gt;
</em><br>
<em>&gt; 6)  Take all the confusion involved in taking the limit of humaneness and
</em><br>
<em>&gt; call it &quot;entropy&quot;.  This is much more impressive.
</em><br>
<em>&gt;
</em><br>
<em>&gt; 7) Actually calculate the amount of entropy in the system.  Be more
</em><br>
<em>&gt; reluctant to guess when the entropy is high.
</em><br>
<em>&gt;
</em><br>
<em>&gt; 8)  Jump back to systems that are not logically omniscient by
</em><br>
<em>&gt; reintroducing probabilism into the calculations.  This kind of
</em><br>
<em>&gt; probabilistic uncertainty also adds to the entropy.  We are now
</em><br>
<em>&gt; approximating humaneness.
</em><br>
<em>&gt;
</em><br>
<em>&gt; 9)  Let an ideally Friendly AI be the AI that would be constructed by a
</em><br>
<em>&gt; humane morality.
</em><br>
<em>&gt;
</em><br>
<em>&gt; 10) Build an approximately Friendly AI with an architecture that
</em><br>
<em>&gt; explicitly treats itself as an approximation to the AI that would be
</em><br>
<em>&gt; constructed by a humane morality.  If it were me doing the first
</em><br>
<em>&gt; approximation, I'd start by guessing that this involved embodying
</em><br>
<em>&gt; a humane
</em><br>
<em>&gt; morality within the FAI itself, i.e., a humane FAI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I wish these were the old days, so I could look over what I just wrote
</em><br>
<em>&gt; with satisfaction, rather than the dreadful sinking knowledge that
</em><br>
<em>&gt; everything I just said sounded like complete gibberish to anyone honest
</em><br>
<em>&gt; enough to admit it.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Some key derived concepts are these:
</em><br>
<em>&gt;
</em><br>
<em>&gt; a)  A Friendly AI improving itself approaches as a limit the AI
</em><br>
<em>&gt; you'd have
</em><br>
<em>&gt; built if you knew what you were doing, *provided that* you *did*
</em><br>
<em>&gt; know what
</em><br>
<em>&gt; you were doing when you defined the limiting process.
</em><br>
<em>&gt;
</em><br>
<em>&gt; b)  A Friendly AI does not look old and busted when the civilization that
</em><br>
<em>&gt; created it has grown up a few million years.  FAIs grow up too - very
</em><br>
<em>&gt; rapidly, where the course is obvious (entropy low), in other
</em><br>
<em>&gt; areas waiting
</em><br>
<em>&gt; for the civilization to actually make its choices.
</em><br>
<em>&gt;
</em><br>
<em>&gt; c)  If you are in the middle of constructing an FAI, and you make a
</em><br>
<em>&gt; mistake about what you really wanted, but you got the fundamental
</em><br>
<em>&gt; architecture right, you can say &quot;Oops&quot; and the FAI listens.  This is
</em><br>
<em>&gt; really really REALLY nontrivial.  It requires practically the entire
</em><br>
<em>&gt; discipline of FAI to do this one thing.
</em><br>
<em>&gt;
</em><br>
<em>&gt; d)  Friendly AI doesn't run on verbal principles or moral philosophies.
</em><br>
<em>&gt; If you said to an FAI, &quot;Joyous Growth&quot;, its architecture would attempt to
</em><br>
<em>&gt; suck out the warm fuzzy feeling that &quot;Joyous Growth&quot; gives you and is the
</em><br>
<em>&gt; actual de facto reason you feel fond of &quot;Joyous Growth&quot;.
</em><br>
<em>&gt;
</em><br>
<em>&gt; e)  The architecture that does this cool stuff is where the
</em><br>
<em>&gt; technical meat
</em><br>
<em>&gt; comes in, and is the interesting part of Friendly AI that goes
</em><br>
<em>&gt; beyond bull
</em><br>
<em>&gt; sessions about what kind of perfect world we'd like.  The bull sessions
</em><br>
<em>&gt; are useless.  You'll know what you want when you know how to do it.  All
</em><br>
<em>&gt; the really *good* options are phrased in the deep language of FAI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; f)  Friendly AI is frickin' complicated, so please stop summarizing it as
</em><br>
<em>&gt; &quot;hardwiring benevolence to humans&quot;.  I ain't bloody Asimov.  This isn't
</em><br>
<em>&gt; even near the galaxy of the solar system that has the planet where the
</em><br>
<em>&gt; ballpark is located.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; He goes even further than most Singularity believers, postulating a
</em><br>
<em>&gt; &gt; “hard takeoff” in which a self-modifying AI program moves from
</em><br>
<em>&gt; &gt; near-human to superhuman intelligence within hours or minutes – instant
</em><br>
<em>&gt; &gt; Singularity!
</em><br>
<em>&gt;
</em><br>
<em>&gt; Ben, you *know* I've read my Tversky and Kahneman.  You *know* I'm not
</em><br>
<em>&gt; stupid enough to say that as a specific prediction of real-world events.
</em><br>
<em>&gt;
</em><br>
<em>&gt; What I do say is that a hard takeoff looks to be theoretically possible
</em><br>
<em>&gt; and is a real practical possibility as well.  Moreover, I say that it is
</em><br>
<em>&gt; both reasonable, and a necessary exercise in AI craftsmanship, to work as
</em><br>
<em>&gt; if a hard takeoff was an immediate possibility at all times - it forces
</em><br>
<em>&gt; you to do things that are the right things in any case, and to address
</em><br>
<em>&gt; necessary theoretical issues.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; With this in mind, he prioritizes the creation of “Friendly AI’s” –
</em><br>
<em>&gt; &gt; artificial intelligence programs with “beneficence to human life”
</em><br>
<em>&gt; &gt; programmed in or otherwise inculcated as a primary value.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Absolutely wrong.  Ben, you never understood Friendly AI from the
</em><br>
<em>&gt; beginning.  I say it without malice, because I never understood Novamente
</em><br>
<em>&gt; from the beginning, and it took me too many interchanges with you before I
</em><br>
<em>&gt; realized it.  Communicating about AI is fscking hard.  I don't understand
</em><br>
<em>&gt; Novamente's parts, design philosophy, or where the work gets done; as far
</em><br>
<em>&gt; as I can tell the parts should sit there and brood and not do a damn
</em><br>
<em>&gt; thing, which is presumably not the case.  If you actually wanted to
</em><br>
<em>&gt; communicate to me how Novamente works, it would probably take hands-on
</em><br>
<em>&gt; experience with the system, or maybe a university course with textbooks,
</em><br>
<em>&gt; experiments, teaching assistants, and experienced professors.  That's what
</em><br>
<em>&gt; it takes to teach most subjects, and AI is fscking hard.  Friendly AI is
</em><br>
<em>&gt; two orders of magnitude fscking harder.  If there were college courses on
</em><br>
<em>&gt; the subject, people still wouldn't get it, because FAI is too fscking
</em><br>
<em>&gt; hard.  So I mean no malice by saying that you didn't get Friendly
</em><br>
<em>&gt; AI at all.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The idea of programming the *output* of a human's moral philosophy into an
</em><br>
<em>&gt; FAI as a &quot;primary value&quot; is absolutely antithetical to the spirit of
</em><br>
<em>&gt; Friendly AI, because you lose all the information and dynamics the human
</em><br>
<em>&gt; used to decide that &quot;beneficence to human life&quot; (for example) was even a
</em><br>
<em>&gt; good idea to begin with.  You therefore lose the content of &quot;beneficence&quot;,
</em><br>
<em>&gt; the dynamics that would decide what constituted &quot;life&quot;, and so on.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; The creation of Friendly AI, he proposes, is the path most likely to
</em><br>
<em>&gt; &gt; lead to a human-friendly post-Singularity world.[11]
</em><br>
<em>&gt;
</em><br>
<em>&gt; Smells vaguely of tautology, given definition (9) above.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; This perspective raises a major issue regarding the notion of AI
</em><br>
<em>&gt; &gt; Friendliness.  Perhaps “Be nice to humans” or “Obey your human masters”
</em><br>
<em>&gt; &gt; are simply too concrete and low-level ethical prescriptions to be
</em><br>
<em>&gt; &gt; expected to survive the Transcension.  Perhaps it’s more reasonable to
</em><br>
<em>&gt; &gt; expect highly abstract ethical principles to survive.  Perhaps it’s
</em><br>
<em>&gt; &gt; more sensible to focus on ensuring the Principle of Voluntary Joyous
</em><br>
<em>&gt; &gt; Growth to survive the Transcension, than to focus on specific ethical
</em><br>
<em>&gt; &gt; rules (which have meaning only within specific ethical systems, which
</em><br>
<em>&gt; &gt; are highly context and culture bound).
</em><br>
<em>&gt;
</em><br>
<em>&gt; FAI does not make specific ethical rules the basis of hardwired
</em><br>
<em>&gt; programming.  I seem to recall addressing you on this specific point
</em><br>
<em>&gt; long since, actually... so for heaven's sake, if you stop nothing else,
</em><br>
<em>&gt; please stop this particular misrepresentation.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The problem with expecting &quot;highly abstract ethical principles&quot; to survive
</em><br>
<em>&gt; is that even if they only contain 30 bits of Kolmogorov complexity, which
</em><br>
<em>&gt; is far too little, the odds of them arising by chance are still a billion
</em><br>
<em>&gt; to one.  Your instincts give you a totally off estimate of the actual
</em><br>
<em>&gt; complexity of your &quot;highly abstract&quot; ethical principles.  You say
</em><br>
<em>&gt; something like &quot;be beneficent to human life&quot;, and your mind recalls Buddha
</em><br>
<em>&gt; and Gandhi, and all the cognitive complexity you've ever developed
</em><br>
<em>&gt; associated with &quot;beneficence&quot; over a lifetime of living with human
</em><br>
<em>&gt; emotions, a human limbic system, a human empathic architecture for
</em><br>
<em>&gt; predicting your conspecifics by using yourself as a model, a human
</em><br>
<em>&gt; sympathetic architecture for evaluating fairness and other
</em><br>
<em>&gt; brainware-supported emotional concepts by putting yourself in other
</em><br>
<em>&gt; people's shoes.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; So, my essential complaint against Yudkowsky’s Friendly AI notion is
</em><br>
<em>&gt; &gt; that – quite apart from ethical issues regarding the wisdom of using
</em><br>
<em>&gt; &gt; mass-energy on humans rather than some other form of existence -- I
</em><br>
<em>&gt; &gt; strongly suspect that it’s impossible to create AGI’s that will
</em><br>
<em>&gt; &gt; progressively radically self-improve and yet retain belief in the “Be
</em><br>
<em>&gt; &gt; nice to and preserve humans” maxim.  I think this “Friendly AI”
</em><br>
<em>&gt; &gt; principle is just too concrete and too non-universal to survive the
</em><br>
<em>&gt; &gt; successive radical-self-improvement process and the Transcension.  On
</em><br>
<em>&gt; &gt; the other hand, I think a more abstract and universally-attractive
</em><br>
<em>&gt; &gt; principle like Voluntary Joyous Growth might well make it.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The core of Friendly AI is the sort of thing you seem to call &quot;abstract&quot;,
</em><br>
<em>&gt; indeed, far more abstract than &quot;Joyous Growth&quot; (though FAI must become
</em><br>
<em>&gt; entirely concrete in my thinking, if it is ever to come into existence).
</em><br>
<em>&gt; Yudkowsky's Friendly AI notion does indeed say that you have to
</em><br>
<em>&gt; understand
</em><br>
<em>&gt; how to transfer large amounts of moral complexity from point A to
</em><br>
<em>&gt; point B.
</em><br>
<em>&gt;   If you do not understand how to transfer &quot;concrete&quot; and &quot;non-universal&quot;
</em><br>
<em>&gt; complexity, you will fail, because absolutely everything you want to do
</em><br>
<em>&gt; has concrete non-universal complexity in it.  &quot;Voluntary Joyous Growth&quot;,
</em><br>
<em>&gt; as you understand that and would apply it, has kilobits and kilobits of
</em><br>
<em>&gt; complexity bound up in it.  You're evaluating the simplicity of this
</em><br>
<em>&gt; concept using a brain that makes things like empathy and sympathy and
</em><br>
<em>&gt; benevolence into emotional primitives that can be chunked and manipulated
</em><br>
<em>&gt; as if they were ontologically basic, and they're not.  You're estimating
</em><br>
<em>&gt; the complexity of things like &quot;Voluntary Joyous Growth&quot; as if it were
</em><br>
<em>&gt; three words long, when actually they're words that call up
</em><br>
<em>&gt; complexity-rich
</em><br>
<em>&gt; concepts that key into your entire existing emotional architecture and
</em><br>
<em>&gt; have implications &quot;obvious&quot; under that emotional architecture and that
</em><br>
<em>&gt; emotional architecture only.  Try explaining your Voluntary Joyous Growth
</em><br>
<em>&gt; to a !Kung tribesman, who's *got* all your brainware already, and you'll
</em><br>
<em>&gt; get a better picture of the complexity inherent in it.  Then try
</em><br>
<em>&gt; explaining it to a chimpanzee.  Then try explaining it to a Motie.  And
</em><br>
<em>&gt; then maybe you'll be ready to explain it to silicon.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Friendly AI is farging difficult!  If you cannot do farging difficult
</em><br>
<em>&gt; things, you cannot do Friendly AI!  Why is this so bloody hard to explain
</em><br>
<em>&gt; to people?  Why does everyone expect this to be some kind of cakewalk?
</em><br>
<em>&gt; Why does everyone turn around and flee at the merest hint that
</em><br>
<em>&gt; any kind of
</em><br>
<em>&gt; real effort might be involved?
</em><br>
<em>&gt;
</em><br>
<em>&gt; I was guilty of this too, by the way, which is why I'm now so intolerant
</em><br>
<em>&gt; of it.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; On the other hand, the Friendly AI principle does not seem to harmonize
</em><br>
<em>&gt; &gt; naturally with the evolutionary nature of the universe at all.
</em><br>
<em>&gt; &gt; Rather, it seems to contradict a key aspect of the nature of the
</em><br>
<em>&gt; &gt; universe -- which is that the old gives way to the new when the time
</em><br>
<em>&gt; &gt; has come for this to occur.
</em><br>
<em>&gt;
</em><br>
<em>&gt; And all societies inevitably progress toward Communism.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This sort of thinking is guaranteed to fail.  Everyone would like
</em><br>
<em>&gt; to think
</em><br>
<em>&gt; the universe is on their side.  It is the naturalistic fallacy committed
</em><br>
<em>&gt; with respect to mystical gibberish.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The universe I live in is neither for me nor against me.  When I
</em><br>
<em>&gt; am lucky,
</em><br>
<em>&gt; it presents to me an acceptable outcome as an accessible option.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; It’s an interesting question whether speciecide contradicts the
</em><br>
<em>&gt; &gt; universal-attractor nature of Compassion.  Under the Voluntary Joyous
</em><br>
<em>&gt; &gt; Growth principle, it’s not favored to extinguish beings without their
</em><br>
<em>&gt; &gt; permission.  But if a species wants to annihilate itself, because it
</em><br>
<em>&gt; &gt; feels its mass-energy can be used for something better, then it’s
</em><br>
<em>&gt; &gt; perfectly Compassionate to allow it to do so.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yeah, yeah, been there, done that, wrote the damn book, tried to have all
</em><br>
<em>&gt; the copies of the damn book burned after I actually figured out what the
</em><br>
<em>&gt; hell I was talking about.  See <a href="http://hanson.gmu.edu/vc.html">http://hanson.gmu.edu/vc.html</a> for an
</em><br>
<em>&gt; example of who Eliezer used to be.
</em><br>
<em>&gt;
</em><br>
<em>&gt; There is no light in this world except that embodied in humanity.
</em><br>
<em>&gt;  Even my
</em><br>
<em>&gt; old thoughts of species self-sacrifice were things that only a
</em><br>
<em>&gt; human would
</em><br>
<em>&gt; ever have thought.  If you lose the information bound up in humanity, you
</em><br>
<em>&gt; lose everything of any value, and it won't ever come back.  Everything we
</em><br>
<em>&gt; care about is specific to humanity, even as our (rather odd) moral
</em><br>
<em>&gt; instincts drive us to argue that it is universal.  When I
</em><br>
<em>&gt; understood that,
</em><br>
<em>&gt; I shut up about Shiva-Singularities.  See, I even gave it a name, back in
</em><br>
<em>&gt; my wild and reckless youth.  You've sometimes presumed to behave
</em><br>
<em>&gt; toward me
</em><br>
<em>&gt; in a sage and elderly fashion, Ben, so allow me to share one of the
</em><br>
<em>&gt; critical lessons from my own childhood:  No, you do not want humanity to
</em><br>
<em>&gt; go extinct.  Trust me on this, because I've been there, and I know from
</em><br>
<em>&gt; experience that it isn't obvious.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; In either case: Hope for the best!
</em><br>
<em>&gt;
</em><br>
<em>&gt; I think I covered this at length a few weeks ago.
</em><br>
<em>&gt;
</em><br>
<em>&gt; --
</em><br>
<em>&gt; Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
</em><br>
<em>&gt; Research Fellow, Singularity Institute for Artificial Intelligence
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7940.html">Ben Goertzel: "RE: Encouraging a Positive Transcension"</a>
<li><strong>Previous message:</strong> <a href="7938.html">Mikko Särelä: "RE: Encouraging a Positive Transcension"</a>
<li><strong>In reply to:</strong> <a href="7922.html">Eliezer S. Yudkowsky: "Friendly AI in &quot;Positive Transcension&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7941.html">Eliezer S. Yudkowsky: "Re: Friendly AI in &quot;Positive Transcension&quot;"</a>
<li><strong>Reply:</strong> <a href="7941.html">Eliezer S. Yudkowsky: "Re: Friendly AI in &quot;Positive Transcension&quot;"</a>
<li><strong>Reply:</strong> <a href="7944.html">Jef Allbright: "Positive Transcension"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7939">[ date ]</a>
<a href="index.html#7939">[ thread ]</a>
<a href="subject.html#7939">[ subject ]</a>
<a href="author.html#7939">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:45 MDT
</em></small></p>
</body>
</html>
