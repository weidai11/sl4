<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Overconfidence and meta-rationality</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Overconfidence and meta-rationality">
<meta name="Date" content="2005-02-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Overconfidence and meta-rationality</h1>
<!-- received="Mon Feb 21 16:28:25 2005" -->
<!-- isoreceived="20050221232825" -->
<!-- sent="Mon, 21 Feb 2005 15:27:10 -0800" -->
<!-- isosent="20050221232710" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Overconfidence and meta-rationality" -->
<!-- id="421A6E4E.4010308@pobox.com" -->
<!-- charset="windows-1252" -->
<!-- inreplyto="6.2.0.14.2.20050116110929.02ef4de0@mail.gmu.edu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Overconfidence%20and%20meta-rationality"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Mon Feb 21 2005 - 16:27:10 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10765.html">Tennessee Leeuwenburg: "Re: AGI Prototying Project"</a>
<li><strong>Previous message:</strong> <a href="10763.html">Tennessee Leeuwenburg: "Re: AGI Prototying Project"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10767.html">Tennessee Leeuwenburg: "Re: Overconfidence and meta-rationality"</a>
<li><strong>Reply:</strong> <a href="10767.html">Tennessee Leeuwenburg: "Re: Overconfidence and meta-rationality"</a>
<li><strong>Reply:</strong> <a href="10773.html">Tennessee Leeuwenburg: "Re: Overconfidence and meta-rationality"</a>
<li><strong>Maybe reply:</strong> <a href="../0503/10878.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<li><strong>Maybe reply:</strong> <a href="../0503/10880.html">Marc Geddes: "Re: Overconfidence and meta-rationality"</a>
<li><strong>Maybe reply:</strong> <a href="../0503/10884.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<li><strong>Maybe reply:</strong> <a href="../0503/10889.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<li><strong>Maybe reply:</strong> <a href="../0503/10901.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<li><strong>Maybe reply:</strong> <a href="../0503/10927.html">Marc Geddes: "Re: Overconfidence and meta-rationality"</a>
<li><strong>Maybe reply:</strong> <a href="../0503/10945.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<li><strong>Maybe reply:</strong> <a href="../0503/10949.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10764">[ date ]</a>
<a href="index.html#10764">[ thread ]</a>
<a href="subject.html#10764">[ subject ]</a>
<a href="author.html#10764">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
(Resuming this after moving to the Bay Area.)
<br>
<p>Robin Hanson wrote:
<br>
<em>&gt; On 1/16/2005, Eliezer S. Yudkowsky wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; The consequences of my accepting the modesty argument would be 
</em><br>
<em>&gt;&gt; extremely large, because if I came to believe it as a fact, a large
</em><br>
<em>&gt;&gt;  number of extremely important beliefs would change.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I'm glad the topic is important to you.  It is important to me too -
</em><br>
<em>&gt; it is a central topic of my research, and I'm seriously considering
</em><br>
<em>&gt; writing a book on it.
</em><br>
<p>I hope that my challenges help to solidify your thinking.  (And that
<br>
your thinking solidifies in a correct direction, rather than, say,
<br>
polarizing with spins opposite to incoming challenges; a major past bias
<br>
of mine, one that still scares me, and a reason I still try to avoid
<br>
heated arguments about any question that might prove to be actually
<br>
important.)
<br>
<p><em>&gt;&gt; The only person I know of who seems to really accept the modesty 
</em><br>
<em>&gt;&gt; argument is Hal Finney.  You and I, having opinions that are not
</em><br>
<em>&gt;&gt; the academic consensus, and keeping them despite all arguments we
</em><br>
<em>&gt;&gt; &quot;take into account&quot;, are not on that list.  The modesty argument is
</em><br>
<em>&gt;&gt; not a consensus opinion in science as to how science should be
</em><br>
<em>&gt;&gt; conducted, but you think you know better.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You might be right, but I'm not sure.
</em><br>
<p>Modest of you, but how do your actions differ because of your notsurety?
<br>
&nbsp;&nbsp;A doubt that does not participate in steering my life is no doubt at
<br>
all.  I think that some - not a majority but some - educated Christians
<br>
would say that there's a chance that the atheists might be right; but,
<br>
having dutifully bowed to modesty, how does their life change?  When
<br>
people instinctively judge that, in their social environment, to believe
<br>
wholeheartedly in clear skies will make them appear overconfident, they
<br>
are instinctively careful to be seen publicly doubting the sun.  When
<br>
people anticipate rain, they take umbrellas.
<br>
<p>And I know this, and try to correct for that bug in my thinking.  So if
<br>
I accept the modesty argument, it has a *very large* effect on what I
<br>
anticipate and how I behave.
<br>
<p>I would have to somehow twist my brain into anticipating that AI was
<br>
fifty years off, that cryonics was a crackpot endeavor, that molecular
<br>
machines were a strictly science-fictional nightmare, and that clinical
<br>
psychology was medicine.  Where do I draw the line?
<br>
<p>Hal Finney is the only person I know of who would advocate that I not
<br>
draw a line - and I'm not sure that Finney realizes the logical
<br>
implication of his modesty.  But also this:  Finney is undeniably
<br>
correct that if most people followed his advice and accepted all
<br>
contemporary science without daring to draw lines, they would be better
<br>
off.  Better off, that is, in the matter of their maps conforming to the
<br>
territory.  Their personal development would be impeded, and they could
<br>
not build further upon the edifice of science; they would have accepted
<br>
an Authority.  Yet the mere morals of science matter not to Bayesianity.
<br>
&nbsp;&nbsp;Their score went up, and there Judgment ends.
<br>
<p>I don't think my own score would go up if I thought that AI were fifty
<br>
years off.  I am not most people, even taking into account that most
<br>
people don't believe themselves to be most people.
<br>
<p>I cannot accept the modesty argument lightly, because I know better than
<br>
to bow in that direction and then keep on with what I was doing.  If I
<br>
sound immodest, it is because my behavior is immodest however it is
<br>
disguised, and I have learned better than to disguise it.
<br>
<p><em>&gt; There is often a difference between the opinions of the typical
</em><br>
<em>&gt; person, the typical academic, and the typical academic who publishes
</em><br>
<em>&gt; near the topic in question (or could do so).   And there is often a
</em><br>
<em>&gt; difference between what they say in publications and what they say in
</em><br>
<em>&gt; private.  It is what those who publish near say in private that I
</em><br>
<em>&gt; most try to be near.
</em><br>
<p>I don't accuse you of committing the sin yourself; but if that is how
<br>
you advise others, they will pick and choose their sources.
<br>
<p><em>&gt; And just because I write favorably regarding a position doesn't mean
</em><br>
<em>&gt; I assign it over 50% probability.
</em><br>
<p>What's special about the 50% threshold?  If a calibrated confidence
<br>
would be 1% and the Bayesian wannabe says 10%, is he not damned just as
<br>
much as for 6 and 60?
<br>
<p>50% has a special status in discourse, but it stems from a naive
<br>
psychophysics of rationality, not probability theory.  50% is the
<br>
special number that lets you say aloud:  &quot;I assign a probability of less
<br>
than 50%, therefore I 'don't believe in it' - I doubt it just like you,
<br>
and pursue it only for the sake of investigating the issue, as a
<br>
scientist must...&quot;  Let us leave aside that a Bayesian may be damned for
<br>
saying 10% if the number be too high.  Do you *anticipate* less than 50%
<br>
those positions you write about?  How little must you *anticipate*
<br>
something before you no longer really deep-down expect returns on
<br>
pursuing it?  I've seen people claim to assign 1% probabilities to
<br>
solutions they pursue; and I think they have no emotional grasp on what
<br>
the phrase &quot;1% probability&quot; means.  They say &quot;1% probability&quot; and
<br>
multiply their emotional anticipation of reward by a factor of, oh, I
<br>
don't know, if I had to pull a number out of my ass, I'd say no better
<br>
than 80% in terms of neurotransmitter intensity or some other measure of
<br>
psychophysics or behavior.
<br>
<p>If the people around you (or the people you define as &quot;near to the
<br>
subject&quot;) seem to think 0.01%, and you want to say aloud &quot;ten percent&quot;
<br>
and care enough emotionally to write papers, then you still violate the
<br>
tenet of the modesty argument.  There's nothing special about the 50%
<br>
threshold.
<br>
<p><em>&gt;&gt; The modesty argument runs as follows:  Maybe 20% of the 
</em><br>
<em>&gt;&gt; as-yet-unproven answers, of the researchers of the caliber selected
</em><br>
<em>&gt;&gt;  for _Edge_, will turn out to be correct.  Or maybe 40%, or 5%; 
</em><br>
<em>&gt;&gt; whatever. ...  Yet most of the _Edge_ respondents would indicate 
</em><br>
<em>&gt;&gt; higher confidence ... The modesty argument is that if all
</em><br>
<em>&gt;&gt; researchers in the _Edge_ group changed their stated confidence
</em><br>
<em>&gt;&gt; (and actual decision-governing anticipation) to 20%, they would
</em><br>
<em>&gt;&gt; achieve better aggregate calibration.  ... The (hypothetical) 20%
</em><br>
<em>&gt;&gt; aggregate score is not due to the _Edge_ scientists randomly
</em><br>
<em>&gt;&gt; selecting one of five possible answers on a multiple-choice test.
</em><br>
<em>&gt;&gt; The _Edge_ scientists who selected a correct answer almost
</em><br>
<em>&gt;&gt; certainly had more evidence than they needed just to find that
</em><br>
<em>&gt;&gt; hypothesis in theoryspace; their confidence is justified.  The
</em><br>
<em>&gt;&gt; _Edge_ scientists who answered correctly must have done so from
</em><br>
<em>&gt;&gt; thinking with at least some rationality-structure, applying good
</em><br>
<em>&gt;&gt; reasons for confidence.  And in turn, those _Edge_ scientists who
</em><br>
<em>&gt;&gt; answered incorrectly must have mistaken what are &quot;good reasons&quot; for
</em><br>
<em>&gt;&gt; belief, or deliberately departed the Way. ... But from the internal
</em><br>
<em>&gt;&gt; perspective of any _Edge_ scientist who gave a correct answer, they
</em><br>
<em>&gt;&gt; must have more Bayesian evidence than just that. Otherwise it would
</em><br>
<em>&gt;&gt; have been impossible for them to pluck that correct hypothesis out
</em><br>
<em>&gt;&gt; of theoryspace.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I'm just not following your logic here.  Sure, if some people made
</em><br>
<em>&gt; more cognitive errors than other people, those who made fewer errors
</em><br>
<em>&gt; are more likely to be right.  So conversely, those who are right made
</em><br>
<em>&gt; fewer errors.  The argument they made turns out to have been a good
</em><br>
<em>&gt; argument with fewer errors.  And because they pick the right answer,
</em><br>
<em>&gt; they better reasons available to them.  But each person does not know
</em><br>
<em>&gt; if he made more or fewer errors, so none of this helps him.
</em><br>
<p>Let me start by asking this admittedly easier question: am I as an
<br>
outside observer, helpless to distinguish the _Edge_ responses into two
<br>
groups, A and B, such that A contains a significantly larger proportion
<br>
of correct answers than B?
<br>
<p>The _Edge_ Question was this:
<br>
<p>&quot;What do you believe is true even though you cannot prove it?&quot;
<br>
<p>David Buss, a preeminent evolutionary psychologist, answers:
<br>
<p><em>&gt;&gt;&gt;&gt; True love.
</em><br>
<em>&gt;&gt;&gt;&gt; 
</em><br>
<em>&gt;&gt;&gt;&gt; I've spent two decades of my professional life studying human
</em><br>
<em>&gt;&gt;&gt;&gt; mating. In that time, I've documented phenomena ranging from
</em><br>
<em>&gt;&gt;&gt;&gt; what men and women desire in a mate to the most diabolical
</em><br>
<em>&gt;&gt;&gt;&gt; forms of sexual treachery. I've discovered the astonishingly
</em><br>
<em>&gt;&gt;&gt;&gt; creative ways in which men and women deceive and manipulate
</em><br>
<em>&gt;&gt;&gt;&gt; each other. I've studied mate poachers, obsessed stalkers,
</em><br>
<em>&gt;&gt;&gt;&gt; sexual predators, and spouse murderers. But throughout this
</em><br>
<em>&gt;&gt;&gt;&gt; exploration of the dark dimensions of human mating, I've
</em><br>
<em>&gt;&gt;&gt;&gt; remained unwavering in my belief in true love.
</em><br>
<em>&gt;&gt;&gt;&gt; 
</em><br>
<em>&gt;&gt;&gt;&gt; While love is common, true love is rare, and I believe that few
</em><br>
<em>&gt;&gt;&gt;&gt; people are fortunate enough to experience it. The roads of
</em><br>
<em>&gt;&gt;&gt;&gt; regular love are well traveled and their markers are well
</em><br>
<em>&gt;&gt;&gt;&gt; understood by many—the mesmerizing attraction, the ideational
</em><br>
<em>&gt;&gt;&gt;&gt; obsession, the sexual afterglow, profound self-sacrifice, and
</em><br>
<em>&gt;&gt;&gt;&gt; the desire to combine DNA. But true love takes its own course
</em><br>
<em>&gt;&gt;&gt;&gt; through uncharted territory. It knows no fences, has no
</em><br>
<em>&gt;&gt;&gt;&gt; barriers or boundaries. It's difficult to define, eludes modern
</em><br>
<em>&gt;&gt;&gt;&gt; measurement, and seems scientifically wooly. But I know true
</em><br>
<em>&gt;&gt;&gt;&gt; love exists. I just can't prove it.
</em><br>
<p>Am I limited to assigning to David Buss's response the same confidence
<br>
that I assign to each and every other response to the _Edge_ questionnaire?
<br>
<p>Suppose that instead Buss had said:
<br>
<p>Subjunctive Buss:  &quot;I would say 'true love'.  By 'true love', I'm trying
<br>
to give a name - the name most people would apply, I think - to a
<br>
handful of relationships I've seen over the years; relationships that
<br>
are rock-solid, where both spouses are still 'in love', not just loving
<br>
but in love, after ten years or twenty years.  True love is very rare, I
<br>
haven't seen enough cases to launch a controlled study of true love, so
<br>
I can't tell you from observation what the preconditions are for true
<br>
love - although, anecdotally, the two are usually very compatible in
<br>
most ways that I've learned to measure compatibility.  I do have an
<br>
evolutionary hypothesis - more of a 'just-so story' at this point, but
<br>
still something that suggests possible experimental tests - that there
<br>
is a point in the Darwinian game where it is a winning strategy to pick
<br>
a mate and stick with that choice; where the probability of doing better
<br>
by straying or betraying, times the probable reward, is just
<br>
significantly less than the reproductive reward of a rock-solid
<br>
relationship.  And if so, people might have a 'true love' mode, and it
<br>
might also be more common in hunter-gatherer societies than modern
<br>
societies.  I don't think that you'll see many cases of 'true love' in a
<br>
society that advertises promiscuous women on every billboard, or
<br>
publishes stories about bachelor celebrity husbands - it makes the
<br>
reward of straying look too high.  And the opposing hypothesis, of which
<br>
I am well aware, is that 'true love' is something that people imagine,
<br>
like dragons and unicorns, and not something that exists in the real
<br>
world.  I'm aware of the seductiveness of this theory, and I try not to
<br>
be seduced.  But there are relationships I've seen that seem to call for
<br>
that explanation.  When I consider the tremendous, demonstrable benefits
<br>
of a rock-solid relationship, it isn't out of the question
<br>
evolutionarily - even if it sounds too good to be true.&quot;
<br>
<p>But instead, Buss actually did say:
<br>
<p>&quot;I've spent two decades of my professional life studying human mating.
<br>
In that time, I've documented phenomena ranging from what men and women
<br>
desire in a mate to the most diabolical forms of sexual treachery. I've
<br>
discovered the astonishingly creative ways in which men and women
<br>
deceive and manipulate each other. I've studied mate poachers, obsessed
<br>
stalkers, sexual predators, and spouse murderers. But throughout this
<br>
exploration of the dark dimensions of human mating, I've remained
<br>
unwavering in my belief in true love.&quot;
<br>
<p>In the first case, the subjunctive Buss is interpreting the _Edge_
<br>
question to mean:
<br>
<p>&quot;What do you believe even though you have not yet produced solid
<br>
experimental results that satisfy your fellow scientists?&quot;
<br>
<p>Maybe I accuse Buss too harshly and if I interrogated Buss more closely,
<br>
something like the above justification would emerge.  But Buss seems to
<br>
have actually interpreted the _Edge_ question to mean:
<br>
<p>&quot;What do you believe even though there is strong evidence against it?&quot;
<br>
<p>If John Brockman had asked the first version of the question explicitly,
<br>
I think he would have gotten a substantially better population of
<br>
correct answers among those who responded.  If John Brockman had asked
<br>
the second version of the question explicitly, many would have
<br>
indignantly refused to answer, and among those who cheerfully did so -
<br>
why, there might not be a single correct answer in the bunch, except by
<br>
sheer coincidence on questions with a small solution space.
<br>
<p>I think that if you applied the simple test of looking at which _Edge_
<br>
respondents put disclaimers on the &quot;wrong question&quot; they were given, as
<br>
many respondents did, then even that test would separate into
<br>
populations A and B with significantly different proportions of correct
<br>
answers.
<br>
<p>Unless the _Edge_ responders became aware of the test, in which case it
<br>
would become much less effective.  And this gets us into your much more
<br>
complicated question, not how we could distinguish among _Edge_
<br>
respondents, but how they could distinguish themselves.  Your phrasing was:
<br>
<p><em>&gt; I'm just not following your logic here.  Sure, if some people made
</em><br>
<em>&gt; more cognitive errors than other people, those who made fewer errors
</em><br>
<em>&gt; are more likely to be right.  So conversely, those who are right made
</em><br>
<em>&gt; fewer errors.  The argument they made turns out to have been a good
</em><br>
<em>&gt; argument with fewer errors.  And because they pick the right answer,
</em><br>
<em>&gt; they better reasons available to them.  But each person does not know
</em><br>
<em>&gt; if he made more or fewer errors, so none of this helps him.
</em><br>
<p>Errors of rationality are not independent random variables!  They are
<br>
not randomly distributed among reasoners.  Any human being possesses
<br>
some ability to check their own cognition for errors.  (And every human
<br>
being is therefore impressed with their own rationality, since they spot
<br>
the errors they know how to spot, and miss the errors they don't know
<br>
how to spot; from their perspective, they sure are catching a lot of
<br>
errors!)  But, leaving that aside, the ability to check oneself for
<br>
errors is part of the standard operating procedure of the brain,
<br>
contributing to human intelligence.  You say, &quot;But each person does not
<br>
know if he made more or fewer errors&quot;.  No; people will try to guess,
<br>
and do well or poorly, according to their mastery of the art.  But
<br>
errors of cognition are not concealed random variables.  People possess
<br>
information, both direct and indirect, about errors they may have made;
<br>
they may not know how to interpret the clues, but they have the clues.
<br>
<p>Now, I do apologize to Buss if I accuse him falsely, but I look at:
<br>
<p>&quot;True love takes its own course through uncharted territory. It knows no
<br>
fences, has no barriers or boundaries. It's difficult to define, eludes
<br>
modern measurement, and seems scientifically woolly. But I know true
<br>
love exists. I just can't prove it.&quot;
<br>
<p>You, or I, can hear the sound of Buss's brain clicking off, the metallic
<br>
clunk as the train ratchets off the rationality track.  Buss didn't hear
<br>
it, but you and I do.
<br>
<p>Stephen Gould's concept of a &quot;separate magisterium&quot; comes in handy here.
<br>
&nbsp;&nbsp;There are no separate magisteriums in reality, which is a single
<br>
unified whole, and all distinctions a human conceit.  But people do
<br>
maintain separate magisteriums in their thinking.  There is a mundane
<br>
magisterium, where people reason by evidence and Occam's Razor, or by a
<br>
fragile intuitive grasp on the statistics of mundane things.  And there
<br>
is another magisterium, which may be called the sacred magisterium, the
<br>
magical realm, the spiritual, the unmundane; by any name it is the realm
<br>
of thought (not reality) where woolly reasoning is believed by the
<br>
reasoner to be permitted and acceptable, for whatever reason.  The
<br>
sacred magisterium may be as lofty and unattainable as Heaven, or as
<br>
easily purchased as a lottery ticket.  But the rules of thinking change
<br>
there; that is what defines the sacred magisterium as it exists in the
<br>
human imagination.
<br>
<p>Buss is, by all that I have heard of him, an excellent evolutionary
<br>
scientist.  He fails on this question of 'true love' because it occupies
<br>
a separate magisterium to him.
<br>
<p>A master of traditional rationality (and how few people have mastered
<br>
even that, let alone Bayescraft?) does not permit any sacred
<br>
magisterium.  For by the ancient traditions of rationality, woolly
<br>
thinking is a sin.  By the morals of traditional rationality, there is a
<br>
precious and sacred thing, reason and evidence, to which the warm
<br>
comfort of woolly thinking must be nobly sacrificed, whatever the
<br>
emotional cost.  Buss was not willing to make the sacrifice.  So he failed.
<br>
<p>I am one who would create AI, a wannabe mechanic of minds.  Intelligence
<br>
is no longer a sacred mystery unto me.  For me the difference between
<br>
Bayesian probability theory and woolly reasoning is as clear as the
<br>
difference between an internal combustion engine and a heap of jello.  I
<br>
use rationality instead of wishful thinking because to me these are not
<br>
different labels but different engines of cognition; I can *see* why the
<br>
first cognitive dynamic works and the second one doesn't.  And, seeing
<br>
this, I know also that there is no magisterium where this is not so.  To
<br>
me there is no question of being 'allowed' to escape the naughty
<br>
constraints of reason into some sacred magisterium where I am finally
<br>
allowed to relax with some comfortable nonsense.  I would just be
<br>
swapping in an engine that didn't work.
<br>
<p>As a practitioner of the Way, I also don't hold with the traditional
<br>
notion of rationality being a great, noble, and difficult sacrifice.
<br>
That is not a smoothly running engine.  When you know from which quarter
<br>
the winds of evidence blow, when you realize which direction of incoming
<br>
evidence you are having to resist, then switch beliefs.  No fuss, no
<br>
drama, no agonizing before and no self-congratulation afterward, just
<br>
shut up and do it.
<br>
<p>People say - they repeat the satisfying anecdote - that scientists and
<br>
geniuses often do poorly in everyday life, or on questions outside their
<br>
field.  That does not satisfy me; if I am to succeed on purpose, instead
<br>
of by accident, I should be able to succeed reliably.  A well-designed
<br>
engine just works.  Buss failed embarassingly, in a way that would be
<br>
visible even to a traditional rationalist.  Buss *knew* better, on some
<br>
level, and still he made the mistake; he said himself that true love
<br>
seemed scientifically woolly.  Buss had already accomplished the hard
<br>
part of the problem, finding signs to distinguish truth from falsehood;
<br>
he just ignored his own intelligence.  If scientists fail on questions
<br>
outside their field, perhaps it is because they apply a different method
<br>
of thinking.  Why would scientists apply a different method of thinking
<br>
outside their art?  Because they believe it is permitted them; because
<br>
in our society they can get away with it; because no one holds them to a
<br>
higher standard.  They have not a mind-mechanic's art to look at their
<br>
thoughts and see engines of success and engines of failure.  They do not
<br>
hear the clunk as the train of reason derails.
<br>
<p>To ask how the _Edge_ correspondents could distinguish *themselves*,
<br>
catch their *own* errors, is a more complicated question than to ask how
<br>
I can distinguish among them.  When I audit someone from the outside, I
<br>
catch mistakes that they don't know how to conceal.  Suppose Buss-1 were
<br>
a sufficiently strong traditional rationalist to know how mistaken
<br>
Buss's actual answer sounded, but Buss-1 still possessed a strong
<br>
emotional attachment to 'true love'.  Buss-1 might fixate on the same
<br>
answer because of the same cognitive forces, while producing a
<br>
rationalization - &quot;reasons&quot; - like Buss-1's statement above.  And then I
<br>
as an auditor might be fooled, placing Buss-1's answer in the A
<br>
population, even though Buss-1 gave just the same answer as Buss.  It is
<br>
much harder to learn not to rationalize than to learn what sounds like a
<br>
rational answer to your fellows.  The second task is ancestral, the
<br>
first task is not.
<br>
<p>But it's not a coincidence that Buss, who made the mistake, did not know
<br>
to conceal that mistake from an outside auditor.  Errors of rationality
<br>
are not independent random variables.  I would expect those _Edge_
<br>
scientists who achieved a correct answer to have done so through
<br>
rational reasoning.  One cause for their reasoning being rational was
<br>
that they managed to make fewer errors or overcome the errors they did
<br>
make.  The cause for their making fewer errors was not mostly randomness
<br>
but mostly skill.  The ratio of possible errors to possible successes is
<br>
very high, and it only takes one solid error to unlink a chain of
<br>
reasoning.  When people get things right, on questions with large
<br>
solution spaces, it is not by coincidence.
<br>
<p>I would expect those _Edge_ scientists who answered correctly once to be
<br>
significantly more likely to answer correctly again, relative to the
<br>
population who answered incorrectly.
<br>
<p>I would expect this to be caused, in large part, by the correct
<br>
answerers having more veridical self-estimates of when they have
<br>
committed errors.
<br>
<p>And yes, those who answer incorrectly are free to reply smugly, &quot;But now
<br>
I'm going to make a very high estimate of my own veridicality in
<br>
detecting errors!  How about that?  Huh?  Huh?&quot;  But since they can't
<br>
get the right answer on scientific questions, why should we expect them
<br>
to get the right answer on reflective self-estimates?  The people who
<br>
answer incorrectly, well, what can you do with them?  I'd rather
<br>
concentrate my efforts on the people who already have some skill at
<br>
traditional rationality.  Those are the people whom I am likely to be
<br>
able to teach better and more detailed skills of self-estimation.  And I
<br>
want their self-estimates to be as accurate as possible - to develop
<br>
further that skill they have already used.  The modesty argument doesn't
<br>
strike me as helpful in doing so.  It just lumps them in with people who
<br>
are blatantly less skillful, and refuses to cluster further or provide
<br>
detailed predictions.
<br>
<p>In the end, my verdict is still:  &quot;When we dream, we do not know we
<br>
dream; but when we wake, we know we are awake.&quot;
<br>
<p>Fairness in argument is a foundational motive force upholding
<br>
traditional rationality, but the Way has only one single rule, to cut
<br>
through to the correct answer.  If I permit myself to think about
<br>
whether my arguments sound fair according to human political instincts,
<br>
even for a fraction of a second, I am no longer following the Way.  I
<br>
may be following the tradition of traditional rationality, which has
<br>
wreaked much good in its time; but I am no longer following the Way of
<br>
trying my absolute best to perfect my individual map to the territory.
<br>
<p>I can only improve my individual map of my individual errors by
<br>
rendering it a detailed description of myself, not by turning it into a
<br>
set of categorical imperatives that sound like fair political arguments,
<br>
for to say what is true of everyone loses the individual detail.  I can
<br>
describe a specific chair in more detail than I can describe all chairs.
<br>
&nbsp;&nbsp;Is this dangerous?  Yes!  So be it!  The Way is often dangerous, and to
<br>
get a sufficiently good prediction you may need to essay risky skills of
<br>
thought.  There are risks!  But pointing out that risk doesn't end the
<br>
discussion.  I accept the risk because I want a detailed skill of
<br>
assessing my probability of specific error on specific problems; and
<br>
that requires that I follow different rules than estimating the average
<br>
error probability of the entire human species and then giving that same
<br>
number for myself.  It would be fair to give the same number for
<br>
everyone.  And if everyone followed that strategy, we might be better
<br>
off than we are right now.  But it would not be the Way that leads to
<br>
the very highest score I can wring from my human brain.  And I am not
<br>
everyone, even taking into account that everyone thinks they are not
<br>
everyone.
<br>
<p>Though it *is* necessary to know the observed error rate of the subjects
<br>
in those cognitive psychology experiments, and to come to terms with
<br>
what that means for you as a fellow human, and as a result substantially
<br>
change your ways of thinking for specific problems instead of bowing in
<br>
the direction of modesty on random general occasions.  To do better than
<br>
those subjects, it is necessary to admit the possibility that you can do
<br>
better, and plan to achieve it.
<br>
<p><em>&gt;&gt;&gt; If the SI has rock solid evidence that its redundant
</em><br>
<em>&gt;&gt;&gt; incorruptible internal audit systems cannot succumb to the usual
</em><br>
<em>&gt;&gt;&gt; human biases of overconfidence, well good for it.
</em><br>
<em>&gt;&gt; 
</em><br>
<em>&gt;&gt; Why should an SI require &quot;rock solid evidence&quot; to make up for
</em><br>
<em>&gt;&gt; *your* doubts?  Why should an SI assemble evidence to overcome a
</em><br>
<em>&gt;&gt; doubt for which it has no evidence?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Until it has evidence that it is in fact an SI, it just knows that it
</em><br>
<em>&gt; is an intelligence.  And if it knows that most intelligences have
</em><br>
<em>&gt; been found to be overconfident, it must worry about that possibility
</em><br>
<em>&gt; for itself.
</em><br>
<p>I don't see how I could possibly make a mindlike process with &quot;super&quot;
<br>
competence - deserving of the name superintelligence - that didn't have
<br>
superhumanly detailed and veridical estimates of self-competence on
<br>
specific problems.  The kind of abstract argument you're using is very
<br>
high-level and agent-oriented and may well be human-specific.  I'm still
<br>
pondering how to translate the primitive terms in that argument you just
<br>
stated into elements in an engine of cognition.
<br>
<p>An SI, or at least an SI such as I would build, does not come into
<br>
existence as a full-fledged but empty mind floating in a vacuum
<br>
pondering &quot;cogito ergo sum&quot;.  A seed AI has to build itself into
<br>
existence.  By the time it knows that most humans possess an evolved
<br>
bias toward overconfidence, it already has an extremely detailed
<br>
self-model that tells it it's not human.
<br>
<p>At this point, I can't definitely rule out a foundation for Bayesian
<br>
probability theory built around a categorical imperative that applies to
<br>
all rational reasoners and an anthropic random selection from among all
<br>
possible observer-instants, but more probably *not*.  Those concepts
<br>
strike me as too complicated to belong in a basic ontology.
<br>
<p><em>&gt;&gt; What prevents a schizophrenic from reasoning likewise?  &quot;I am God,
</em><br>
<em>&gt;&gt;  therefore I can correctly estimate whether I am God.&quot;  It seems to
</em><br>
<em>&gt;&gt; me that the schizophrenic is just screwed, and the SI should not
</em><br>
<em>&gt;&gt; take into account that the schizophrenic is just screwed in
</em><br>
<em>&gt;&gt; deciding its own confidence.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Real schizophrenics have ample social evidence that they are in fact
</em><br>
<em>&gt;  broken.  Other people around them consistently tell them they are 
</em><br>
<em>&gt; broken.  If they follow my advice and listen carefully to social 
</em><br>
<em>&gt; evidence, they will do better than following your advice to listen
</em><br>
<em>&gt; less to such evidence.
</em><br>
<p>Schizophrenics have little or no chance of following your advice.  If we
<br>
started out with a population evenly distributed over IQ, the people who
<br>
are most likely to take your advice are least likely to need it or be
<br>
actively damaged by it.  Only the fact that a large prior bias exists
<br>
toward average IQ lends any hope at all to your cause.  And we must
<br>
consider that the most important persuadees, the centers of utility to
<br>
science, are those who would be most damaged if they accepted the
<br>
modesty argument.  The modesty argument is an argument for conformity
<br>
and conformity is sludge in the gears of science.  That last sentence is
<br>
an argument from a mere moral virtue of traditional rationality - what
<br>
helps science as a whole is not necessarily what improves your Bayesian
<br>
score - but it also impacts on the Way.  If something slows down social
<br>
arrival to the correct answer or impedes social ability to distinguish
<br>
truth from falsehood, it probably isn't the *very best* Way an
<br>
individual could possibly think.
<br>
<p>Most people would be better off if they followed Hal Finney's advice,
<br>
but is it the *best* advice they could follow?
<br>
<p>Many people already have some grasp on rationality, and they fail
<br>
because they decide not to use it, not to listen to what their own
<br>
intelligence tells them.  If you tell them to be modest, that will just
<br>
be another tool of rationality, which of course does not apply to the
<br>
sacred magisterium.  At least that's my guess as to how the psychology
<br>
will work.
<br>
<p><em>&gt;&gt; Being forced counts for nothing.  ...  Speak to me of people who 
</em><br>
<em>&gt;&gt; voluntarily invest effort and study in the explicit cognitive
</em><br>
<em>&gt;&gt; science of rationality, and look up the references of their own
</em><br>
<em>&gt;&gt; will and curiosity.  Sample size too small?  I'm not surprised. ...
</em><br>
<em>&gt;&gt; Being thrown into a state of genuine uncertainty and curiosity
</em><br>
<em>&gt;&gt; about the outcome of your own reasoning counts for *much* more.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I fear your intent is to narrow your claim so that you become the
</em><br>
<em>&gt; only known example which could confirm or deny your hypothesis.  If
</em><br>
<em>&gt; that doesn't seem suspiciously like excuse making, nothing does.
</em><br>
<p>Then so be it.  I have learned that fearing suspicion is not the Way.
<br>
Otherwise I would have thought up some clever excuse for my claim, some
<br>
way to disguise it and make it seem more 'rational' to a traditional
<br>
rationalist.  The hell with that.  I know how my thoughts arrived at my
<br>
answer; let them stand up front and naked.  No, I do not know of any
<br>
other specific person whom I place in my statistical cluster.  If they
<br>
are true rationalists with eyes that see reality, why are they living
<br>
comfortable academic lives instead of using their skills to prevent the
<br>
world from destruction?  If they aren't trying to build AI, how would
<br>
they acquire knowledge of Bayescraft?  You will accept that argument or
<br>
not, more probably not, as you choose, but it is how things look to me.
<br>
Why should I hail someone as Bayescrafter if their supposed skills
<br>
change their life so little, or are used to little ends?
<br>
<p><em>&gt; And again, even if they do disagree less, the fact that they do
</em><br>
<em>&gt; disagree says that they are still overconfident.
</em><br>
<p>It says that at least one of them is overconfident.
<br>
<p><em>&gt;&gt;&gt;&gt; Consider someone who buys a lottery ticket and wins.  The odds
</em><br>
<em>&gt;&gt;&gt;&gt; of winning the Mega Millions lottery are around 125,000,000 to
</em><br>
<em>&gt;&gt;&gt;&gt; 1.  Now consider alternate hypotheses, such as &quot;The world is a
</em><br>
<em>&gt;&gt;&gt;&gt; computer simulation and I am the main character&quot; or &quot;I have
</em><br>
<em>&gt;&gt;&gt;&gt; psychic powers&quot;. Is the prior probability of these statements
</em><br>
<em>&gt;&gt;&gt;&gt; really less than 1e-8?
</em><br>
<em>&gt;&gt;&gt; 
</em><br>
<em>&gt;&gt;&gt; I agree that if you high enough priors on these theories that
</em><br>
<em>&gt;&gt;&gt; have a much higher likelihood for the data you see, you may well
</em><br>
<em>&gt;&gt;&gt; need to take them seriously.
</em><br>
<em>&gt;&gt; 
</em><br>
<em>&gt;&gt; Consider the startling consequence: the lottery winner may need to
</em><br>
<em>&gt;&gt;  agree to disagree with a third-party observer, even if they are
</em><br>
<em>&gt;&gt; both meta-rational and agree perfectly on estimates of their
</em><br>
<em>&gt;&gt; respective rationality.  (I am still not sure how to resolve this
</em><br>
<em>&gt;&gt; puzzle.)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Under the hypothesis the &quot;people&quot; you would be disagreeing with
</em><br>
<em>&gt; wouldn't actually be people at all.  They would be the impression of
</em><br>
<em>&gt; people given to you by a simulator. These impressions would not be
</em><br>
<em>&gt; simulated in enough detail for them to internally generate opinions.
</em><br>
<p>And if, in reality, both people are real?  Is it okay to agree to
<br>
disagree if you both doubt whether the other person might be a
<br>
simulation (or otherwise has less measure than yours)?  Is it okay to
<br>
discount another person's opinions, if you think the other person is an
<br>
accurate simulation of a meta-rational Bayesian reasoner rather than a
<br>
real meta-rational Bayesian reasoner?
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10765.html">Tennessee Leeuwenburg: "Re: AGI Prototying Project"</a>
<li><strong>Previous message:</strong> <a href="10763.html">Tennessee Leeuwenburg: "Re: AGI Prototying Project"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10767.html">Tennessee Leeuwenburg: "Re: Overconfidence and meta-rationality"</a>
<li><strong>Reply:</strong> <a href="10767.html">Tennessee Leeuwenburg: "Re: Overconfidence and meta-rationality"</a>
<li><strong>Reply:</strong> <a href="10773.html">Tennessee Leeuwenburg: "Re: Overconfidence and meta-rationality"</a>
<li><strong>Maybe reply:</strong> <a href="../0503/10878.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<li><strong>Maybe reply:</strong> <a href="../0503/10880.html">Marc Geddes: "Re: Overconfidence and meta-rationality"</a>
<li><strong>Maybe reply:</strong> <a href="../0503/10884.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<li><strong>Maybe reply:</strong> <a href="../0503/10889.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<li><strong>Maybe reply:</strong> <a href="../0503/10901.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<li><strong>Maybe reply:</strong> <a href="../0503/10927.html">Marc Geddes: "Re: Overconfidence and meta-rationality"</a>
<li><strong>Maybe reply:</strong> <a href="../0503/10945.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<li><strong>Maybe reply:</strong> <a href="../0503/10949.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10764">[ date ]</a>
<a href="index.html#10764">[ thread ]</a>
<a href="subject.html#10764">[ subject ]</a>
<a href="author.html#10764">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:22:53 MST
</em></small></p>
</body>
</html>
