<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: [sl4] [Hplusroadmap] [SL4] Re: Paper: Artificial Intelligence will Kill our Grandchildren</title>
<meta name="Author" content="Bryan Bishop (kanzure@gmail.com)">
<meta name="Subject" content="[sl4] [Hplusroadmap] [SL4] Re: Paper: Artificial Intelligence will Kill our Grandchildren">
<meta name="Date" content="2008-06-24">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>[sl4] [Hplusroadmap] [SL4] Re: Paper: Artificial Intelligence will Kill our Grandchildren</h1>
<!-- received="Tue Jun 24 23:03:49 2008" -->
<!-- isoreceived="20080625050349" -->
<!-- sent="Wed, 25 Jun 2008 00:05:41 -0500" -->
<!-- isosent="20080625050541" -->
<!-- name="Bryan Bishop" -->
<!-- email="kanzure@gmail.com" -->
<!-- subject="[sl4] [Hplusroadmap] [SL4] Re: Paper: Artificial Intelligence will Kill our Grandchildren" -->
<!-- id="200806250005.42605.kanzure@gmail.com" -->
<!-- charset="utf-8" -->
<!-- inreplyto="7.0.1.0.2.20080614115641.051075b0@SpreadsheetDetective.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Bryan Bishop (<a href="mailto:kanzure@gmail.com?Subject=Re:%20[sl4]%20[Hplusroadmap]%20[SL4]%20Re:%20Paper:%20Artificial%20Intelligence%20will%20Kill%20our%20Grandchildren"><em>kanzure@gmail.com</em></a>)<br>
<strong>Date:</strong> Tue Jun 24 2008 - 23:05:41 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="19070.html">Stuart Armstrong: "Re: [sl4] Re: More silly but friendly ideas"</a>
<li><strong>Previous message:</strong> <a href="19068.html">Thomas McCabe: "Re: [sl4] Re: Signaling after a singularity"</a>
<li><strong>In reply to:</strong> <a href="18982.html">Anthony Berglas: "Paper: Artificial Intelligence will Kill our Grandchildren"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19072.html">Anthony Berglas: "Re: [sl4] [Hplusroadmap] Paper: Artificial Intelligence will Kill our Grandchildren"</a>
<li><strong>Reply:</strong> <a href="19072.html">Anthony Berglas: "Re: [sl4] [Hplusroadmap] Paper: Artificial Intelligence will Kill our Grandchildren"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19069">[ date ]</a>
<a href="index.html#19069">[ thread ]</a>
<a href="subject.html#19069">[ subject ]</a>
<a href="author.html#19069">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Friday 13 June 2008, Anthony Berglas wrote:
<br>
<em>&gt; <a href="http://berglas.org/Articles/AIKillGrandchildren/AIKillGrandchildren.h">http://berglas.org/Articles/AIKillGrandchildren/AIKillGrandchildren.h</a>
</em><br>
<em>&gt;tml
</em><br>
<p>Let's do this.
<br>
<p><em>&gt; There have been many exaggerated claims as to the power of Artificial
</em><br>
<em>&gt; Intelligence (AI), but there has also been real progress.  Computers
</em><br>
<p>This entirely depends on what progress you are expecting ai to come in 
<br>
the form of. This might be due to the fact that many people are taking 
<br>
peculiar routes to getting programmable intelligence, taking other 
<br>
loopholes and whatever, assuming definitions of intelligence, and so 
<br>
on. Difficult issues there, but otherwise if you look more closely 
<br>
there might be some real progress in other areas. RSI, maybe.
<br>
<p><em>&gt; can drive cars across rough desert tracks, understand speech, and
</em><br>
<em>&gt; prove complex mathematical theorems.  It is difficult to predict
</em><br>
<em>&gt; future progress, but if a computer ever became about as good at
</em><br>
<em>&gt; programming computers as people are, then it could program a copy of
</em><br>
<p>No, it could just copy bits and bytes, nothing about programming is 
<br>
needed for copying from one machine to another.
<br>
<p><em>&gt; itself.  This would lead to an exponential rise in intelligence (now
</em><br>
<p>No, this is not true. The ground problem inherently limits the available 
<br>
hardware on which the software can 'exponentially' expand on. Really 
<br>
the numbers are going to look like something that keeps hitting the top 
<br>
of available hardware capacity, unless they are physically grounded 
<br>
with manufacturing processes that allow it to exponentially make the 
<br>
machinery to make the machinery to ... &lt;do what it does&gt;.
<br>
<p><em>&gt; often referred to as the Singularity).  And evolution suggests that a
</em><br>
<em>&gt; sufficiently powerful AI would probably destroy humanity.  This paper
</em><br>
<p>Wha? How does 'evolution' suggest this? Evolution doesn't come to our 
<br>
door and actually say this to us, so I fail to see what you're trying 
<br>
to say here. Evolution isn't a good friend of ours (and I'm not saying 
<br>
it's a bad friend). Merely that you need some elaboration and 
<br>
clarification.
<br>
<p><em>&gt; reviews progress in Artificial Intelligence and some philosophical
</em><br>
<em>&gt; issues and objections.  It then describes the danger and proposes a
</em><br>
<em>&gt; radical solution, namely to limit the production of ever more
</em><br>
<em>&gt; powerful computers and so try to starve any AI of processing power. 
</em><br>
<p>That's peculiar. That's like shooting yourself in the foot. Especially 
<br>
considering the ancient post to SL4 stating &quot;I am an ai anyway&quot;.
<br>
<p><em>&gt; This is urgent, as computers are already almost powerful enough to
</em><br>
<em>&gt; host an artificial intelligence.
</em><br>
<p>Maybe the solution is that, instead of making silly regulations, you 
<br>
should try to make redundant those things that you value the most and 
<br>
want to save from that potential destruction that you forsee? Whether 
<br>
or not there's enough computational power isn't going to stop much from 
<br>
happening. Hell, we used to make computers in our basements out of 
<br>
freaking vacuum tubes. These things can be made by hand if they had to. 
<br>
And it's relatively easy to automate. As for the semiconductor 
<br>
manufacturing industry &lt; <a href="http://heybryan.org/semiconductor.html">http://heybryan.org/semiconductor.html</a> &gt;, 
<br>
those guys used to make circuits by photomasks and lenses they bought 
<br>
from the stores down the street from their parents' garages. So 
<br>
regulations aren't really going to stop that sort of thing.
<br>
<p><em>&gt; Hardware has certainly become much, much faster, but software has
</em><br>
<em>&gt; just become much, much slower to compensate.  We think we understand
</em><br>
<em>&gt; computers and the sort of things they can do.
</em><br>
<p>Are you a programmer and have you any idea ?
<br>
<p><em>&gt; But quietly in the background there has been slow but steady progress
</em><br>
<em>&gt; in a variety of techniques generally known as Artificial
</em><br>
<em>&gt; Intelligence.  Glimpses of the progress appear in applications such
</em><br>
<em>&gt; as speech recognition, some expert systems and cars that can drive
</em><br>
<em>&gt; themselves unaided on freeways or rough desert tracks. The problem is
</em><br>
<em>&gt; far from being solved, but there are many brilliant minds working on
</em><br>
<em>&gt; it.
</em><br>
<p>Although many grants are given to researchers claiming those 
<br>
applciations to be relevant to ai, there's not necessarily anything 
<br>
that says that a car that drives itself is intelligent at all. There's 
<br>
the visual systems that are integrated into it, I suppose, but a lot of 
<br>
that has been stolen from biology when it comes to programs like 
<br>
pNEURON or GENESIS, so really that's just stealing from biology [and in 
<br>
which case, our search should go to biology, not to the foundations of 
<br>
computers, which I must admit my longing for ...].
<br>
<p><em>&gt; It might seem implausible that a computer could ever become truly
</em><br>
<em>&gt; intelligent.  After all, they aren't intelligent now.  But we have a
</em><br>
<em>&gt; solid existence proof that intelligence is possible — namely
</em><br>
<em>&gt; ourselves.  Unless one believes in magic then our intelligence must
</em><br>
<em>&gt; result from well defined electro chemical processes in our brains. 
</em><br>
<em>&gt; If those could be understood and simulated then you would have an
</em><br>
<em>&gt; intelligent machine.  But current results suggests that such a
</em><br>
<em>&gt; simulation is not necessary, there are many ways to build an
</em><br>
<em>&gt; intelligent machine.  It is difficult to predict just how hard it is
</em><br>
<em>&gt; to build an intelligent machine, but barring magic it is certainly
</em><br>
<em>&gt; possible.
</em><br>
<p>Bingo. Let me point out something that you have stated very, very well. 
<br>
It is /people/ that are intelligent. There's something about their 
<br>
brains, something about these systems that does something that we want 
<br>
to imitate and elaborate on. But we don't know what the hell it is. I 
<br>
don't want any of the games with saying it's pattern recognition or 
<br>
something silly like that, don't cite any pop psychology. What we know, 
<br>
as of now, is that the brain is doing something awesome, and that we 
<br>
want to figure out how to do it in other areas too. As for the &quot;well 
<br>
defined electro chemical processes&quot;, that's very, very vague for such 
<br>
an important concept (intelligence). There's more to it than that.
<br>
<p><a href="http://heybryan.org/mediawiki/index.php/Henry_Markram">http://heybryan.org/mediawiki/index.php/Henry_Markram</a>
<br>
<p>The brain is so much more than electrochem procs. I'm not saying that 
<br>
it's all relevant, but at the same time I'm saying that handwaving 
<br>
electrochem isn't the way to go about doing this ...
<br>
<p><em>&gt; Man's intelligence is intimately tied to his physical body.  The
</em><br>
<p>I'd argue Google.
<br>
<p><em>&gt; brain is very finite, cannot be physically extended or copied, takes
</em><br>
<p>I'd argue arms and legs re: extensions.
<br>
<p><em>&gt; What is certain is that an intelligence that was good at world
</em><br>
<em>&gt; domination would be good at world domination.   So if there were a
</em><br>
<em>&gt; large number artificial intelligences, and just one of them wanted to
</em><br>
<em>&gt; and was capable of dominating the world, then it would.  That is just
</em><br>
<p>I don't understand. Are you saying that because an ai really wants 
<br>
something, that it is somehow privledged to get it because of its 
<br>
nature as ai, whereas ai implemented in biological systems isn't going 
<br>
to get it? That makes little sense, and sounds like ancient vitalism to 
<br>
me. World domination is world domination, no matter whether you are ai 
<br>
or hu.
<br>
<p><em>&gt; Darwin's evolution taken to the next level.  The pen is mightier than
</em><br>
<p>This is not Darwin.
<br>
<p><em>&gt; the sword, and the best intelligence has the best pen.  It is also
</em><br>
<em>&gt; difficult to see why an AI would want humans around competing for
</em><br>
<em>&gt; resources and threatening the planet.
</em><br>
<p>Let's think a little broader than the planet. This is SL4, not SL1.
<br>
<p><em>&gt; The first question to be addressed is whether computer hardware has
</em><br>
<em>&gt; sufficient power to run an intelligent program if such a program
</em><br>
<em>&gt; could be written.
</em><br>
<p>The concept of power is meaningless when it comes to computer science. 
<br>
Instead, you have to consider computational complexity, executing time, 
<br>
or maybe even complexity classes (NP-hard, etc.). But power doesn't 
<br>
mean much. It doesn't matter if the ai runs for a few billion years or 
<br>
for a few seconds. Mainly the issue is that we want something that we 
<br>
can use within a reasonable amount of time so that we can analyze the 
<br>
data and see if it matches up with what it probably should be doing. So 
<br>
that we can debug the software. Debugging something that takes more 
<br>
than a few hours to run is a pain in the ass. Debugging something that 
<br>
runs over many years is even worse. So the alternative is to just get 
<br>
it correct the first time. But then this is assuming a cathedral-style, 
<br>
monolithic development effort from some sort of 'universal model of 
<br>
intelligence', when we don't even have that sort of model in the first 
<br>
place. All that we have is the human brain and we know it's doing 
<br>
something special. That's it. None of this statistical inference crap 
<br>
to get around it. Debugging our methodology of implementing this in 
<br>
other ways could be done if we accept that the implementation isn't 
<br>
necessarily going to be somebody sitting down at the terminal and 
<br>
typing out code [as much as I'd like to do just that].
<br>
<p><em>&gt; Our meat based brains have roughly 100 billion neurons.  Each neuron
</em><br>
<em>&gt; can have complex behavior which is still not well understood, and may
</em><br>
<em>&gt; have an average of 7,000 connections to other neurons.  Each neuron
</em><br>
<em>&gt; can operate concurrently with other neurons, which in theory could
</em><br>
<em>&gt; perform a staggering amount of computation.  However, neurons are
</em><br>
<em>&gt; relatively slow, with only roughly 200 firings per second, so they
</em><br>
<em>&gt; have to work concurrently to produce results in a timely manner.
</em><br>
<p>Actually, you might be interested in knowing that it has been shown that 
<br>
in the hu brain there are only maybe up to 100 neurons in a path from 
<br>
input to output, so there's lots of specialization of neurons and the 
<br>
pathways for signal processing and such. It makes for an interesting 
<br>
visualization of the brain. Just an idea.
<br>
<p><em>&gt;  That said, the computer can perform several billion operations per
</em><br>
<em>&gt; second, which is over a million times faster than neurons.   And
</em><br>
<em>&gt; specialized hardware and advanced architectures can perform many
</em><br>
<em>&gt; operations simultaneously.  Computers are also extremely accurate
</em><br>
<em>&gt; which is fortunate as they are also extremely sensitive to any
</em><br>
<em>&gt; errors.
</em><br>
<p>When computing an intelligence algorithm, how does one see an error? ;-)
<br>
<p>The models don't add up, in other words.
<br>
<p><em>&gt; Thus a computer that was ten thousand times faster than a desktop
</em><br>
<em>&gt; computer would probably be at least as computationally powerful as
</em><br>
<em>&gt; the human brain.   With specialized hardware it would not be
</em><br>
<em>&gt; difficult to build such a machine in the very near future.
</em><br>
<p>What the hell is power? Argh. See my commentary above.
<br>
<p><em>&gt; But current progress in artificial intelligence is rarely limited by
</em><br>
<em>&gt; the speed and power of modern computer hardware.  The current
</em><br>
<em>&gt; limitation is that we simply do not know how to write the software.
</em><br>
<p>No, we know how to program. You just don't know what to program. I do, 
<br>
but the results aren't going to look like what you're expecting them to 
<br>
look like (intelligence in the form of code on your blinky ssh tty). 
<br>
<p><em>&gt; The &quot;software&quot; for the human brain is ultimately encoded into our
</em><br>
<em>&gt; DNA.   What is amazing is that the entire human genome only contains
</em><br>
<p>Woah, hold on there. What the hell is software when it comes to the 
<br>
human brain? Are you trying to make an analogy to the mind? Are you 
<br>
trying to make an analogy to gene expression? There's already many 
<br>
databases concerning the genetic expression of different regions of the 
<br>
rat brain &lt; <a href="http://brain-maps.org/">http://brain-maps.org/</a> &gt; and the Allen Institute has 
<br>
recently begun funding a similar mapping expedition of the human brain 
<br>
in a slice-by-slice manner. But I doubt that this is what you mean. 
<br>
Perhaps you mean the internal perceptions of the running brain?
<br>
<p>I see that you mention gene expression later. You write:
<br>
<p><em>&gt;  Still, while babies are not born intelligent, it is clear that the
</em><br>
<em>&gt; core algorithms and structures that allow a baby to become
</em><br>
<em>&gt; intelligent are encoded in a very small amount of raw data.  There is
</em><br>
<em>&gt; just not enough room for some large and arbitrary program to be
</em><br>
<em>&gt; encoded in our DNA, even if the junk DNA turns out not to be junk. 
</em><br>
<em>&gt; This suggests that a very few clever insights might solve the problem
</em><br>
<em>&gt; of Artificial Intelligence very suddenly.
</em><br>
<p>Some good thoughts. However, let's consider evolution for a moment here. 
<br>
The problem that evolution was overcoming, in terms of natural 
<br>
selection, is not going to be necessarily represented in the proteins 
<br>
and the various structures that we see within the brain, but rather 
<br>
that these are the /byproducts/ of those situations. I'm saying that 
<br>
the design of the amorphous parallel nonlocalized systems of diffusion 
<br>
signaling and all sorts of interesting molecular processes aren't going 
<br>
to have a direct one-to-one correspondence with the problem space that 
<br>
was being selected against/for/whatever. There are some aspects of the 
<br>
human brain that are encoded in the genome that I suspect are important 
<br>
in terms of intelligence.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://heybryan.org/intense_world_syndrome.html">http://heybryan.org/intense_world_syndrome.html</a>
<br>
<p>However, translating that into &quot;straight up&quot; code that most ai theorists 
<br>
want, isn't necessarily going to happen. But I'd like to point out that 
<br>
the computational neuroscientists have been doing this already, 
<br>
especially with their microcolumnar simulations. I offer:
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://heybryan.org/mediawiki/index.php/Computational_neuroscience">http://heybryan.org/mediawiki/index.php/Computational_neuroscience</a>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://heybryan.org/mediawiki/Henry_Markram">http://heybryan.org/mediawiki/Henry_Markram</a>
<br>
<p>Markram has a team of 20 postdoc programmers working away at integrating 
<br>
all sorts of interesting code that models biology into his system. He 
<br>
has it running on a supercomputer, generating something like one 
<br>
terabyte per sec of raw output that he visualizes with a secondary 
<br>
supercomputer. ;-) It works.
<br>
<p><em>&gt; It also suggests that if a small improvements can account for the
</em><br>
<em>&gt; huge increase in intelligence from ape to human, then producing super
</em><br>
<em>&gt; human intelligence might not be much harder than producing human
</em><br>
<em>&gt; intelligence (i.e. the problem is not exponentially difficult).
</em><br>
<p>That's only true if you allow those same variables some relevance. For 
<br>
example, in Markram's models there's the issue of spillover from the 
<br>
synaptic junctions due to the decreased spacing between neurons in the 
<br>
inhibitory channels of microcolumns. How would you apply that in a case 
<br>
where you are *not* simulating neurons on a computer? I'd be interested 
<br>
in knowing, because I haven't see any models that can convert that 
<br>
very, very specific detail into any of the more loose/simplistic models 
<br>
that float around here and the AGI discussion lists.
<br>
<p><em>&gt; Great progress was made in artificial intelligence during the early
</em><br>
<em>&gt; years of software.  By the early 1970s computers could prove
</em><br>
<em>&gt; substantial theorems in mathematical logic,  solve differential
</em><br>
<em>&gt; equations better than most undergraduate mathematicians, and beat
</em><br>
<em>&gt; most people at chess.
</em><br>
<p>How is that related to intelligence? Except in that the people lied to 
<br>
us?
<br>
<p><em>&gt; A major problem in AI is to relate the symbolic internal world of the
</em><br>
<em>&gt; computer to the real world at large, which is full of noisy and
</em><br>
<em>&gt; inconsistent data.  &quot;Neural networks&quot; have an uncanny ability to
</em><br>
<em>&gt; learn complex relationships between a vector of observed inputs and a
</em><br>
<em>&gt; vector of known properties of the input.  They can also be given
</em><br>
<em>&gt; memory between inferences, and thus solve complex problems.  However,
</em><br>
<em>&gt; the models they learn are unintelligible arrays of numbers, and their
</em><br>
<em>&gt; utility for higher level reasoning and introspection is probably
</em><br>
<em>&gt; limited.  (Their relationship to real neurons is tenuous.)
</em><br>
<p>The symbolic grounding problem is largely one in that there needs to be 
<br>
grounding within the world itself in order to observe the results. 
<br>
Trying to peak in at the &quot;unintelligable arrays of numbers&quot; is evidence 
<br>
that people might be trying to do something stupid ... i.e., assuming 
<br>
that perceptions are going to be popping out at them &quot;Hey! I'm the 
<br>
basis of the understanding of a cat's respiratory system!&quot;. Heh.
<br>
<p><em>&gt; Having been unfashionable for many years, AI research is now big
</em><br>
<em>&gt; business.  The easy software problems have all been solved, adding
</em><br>
<em>&gt; intelligence may give the commercial edge.  Google has invested
</em><br>
<em>&gt; heavily — they want to understand documents at a deeper level than
</em><br>
<em>&gt; just their keywords.  Microsoft has also made substantial investments
</em><br>
<em>&gt; (particularly in Bayesian networks) — they want to understand Google.
</em><br>
<p>Just wait until Google installs a brain farm.
<br>
<p><em>&gt; One major driver will be the need for practical intelligence as
</em><br>
<em>&gt; robots leave the factory and start to interact with the real world. 
</em><br>
<p>No, that's more a 'driver' for people to come to terms with the problems 
<br>
and realize that they might be interested in working on them, it's 
<br>
nothing about the actuality of solving them, and whether or not robotic 
<br>
machinery of those sorts are the correct platforms for intelligence 
<br>
implementation, etc. Although I wouldn't mind messing around with such 
<br>
robotics, or even highly parallel multiple core systems on a bot. Cool.
<br>
<p><em>&gt; In particular cars can already drive themselves over rough desert
</em><br>
<em>&gt; tracks and down freeways.  We will see autonomous vehicles driving on
</em><br>
<em>&gt; our suburbs much sooner than later.  (Initially they might be hybrid,
</em><br>
<em>&gt; and monitored remotely from somewhere in India.)  Once robots start
</em><br>
<em>&gt; to mow grass, paint houses, explore Mars and lay bricks people may
</em><br>
<em>&gt; start to ask healthy questions as to the role of man.  Being
</em><br>
<em>&gt; unnecessary is dangerous.
</em><br>
<p>You're talking about physical manufacturing and mechanics, tasks that 
<br>
machines can already do. Intelligence isn't really needed for those 
<br>
things. In fact, it could be argued that people who do those things 
<br>
aren't practicising much intelligence, especially in the case of the 
<br>
houses that are automatically manufactured (Bucky died too soon), or 
<br>
explore Mars (cough, we have a bot up there, and it's only somewhat 
<br>
remotely operated, it needs to execute localized program from time to 
<br>
time). As for 'the role of man'. I don't know what this means. I 
<br>
suspect that you are assuming the typical scenario where man might be 
<br>
displaced by automated machines that are nonbiological. I consider this 
<br>
to be fixed by my request for you to consider more than our simple 
<br>
planet. Who cares if you are out of work? The machines are taking care 
<br>
of the necessities of life anyway, yes? Then what's the big deal? And 
<br>
if you want to go somewhere else, where you can do some work that 
<br>
robots are not doing at the moment, then go do it. It's not like 
<br>
economics will stop you -- there's no reason to keep current monetary 
<br>
systems really, especially when we consider singularities or F/OSS. And 
<br>
so on.
<br>
<p><em>&gt; Creationists are right to reject evolution.  For evolution consumes
</em><br>
<em>&gt; all that is good and noble in mankind and reduces it to base impulses
</em><br>
<em>&gt; that have simply been found to be effective for breeding children. 
</em><br>
<em>&gt; The one thing the we know about each of our millions of ancestors is
</em><br>
<em>&gt; that they all successfully had grandchildren.  Will you?
</em><br>
<p>No, there's more than just base impulses when it comes to evolution. 
<br>
Evolution is not entirely reductionist.
<br>
<p><em>&gt; Our sex drive produces children.
</em><br>
<p>Wrong.
<br>
<p><a href="http://en.wikipedia.org/wiki/Intercourse">http://en.wikipedia.org/wiki/Intercourse</a>
<br>
<p><em>&gt; Our love of our children ensures that we provide them with precious
</em><br>
<em>&gt; resources to thrive and breed.
</em><br>
<p>Wrong. Hundreds of millions, if not billions, of mothers are too poor to 
<br>
provide them precious resources to thrive and breed, no matter how much 
<br>
they might love their children. I don't want to dissect the rest of 
<br>
your paragraph. :(
<br>
<p><em>&gt; Nothing new above.  But it is interesting to speculate what
</em><br>
<em>&gt; motivations an artificial intelligence might have.  Would it inherit
</em><br>
<em>&gt; our noble goals?
</em><br>
<p>Who cares? Let's plan for two scenarios:
<br>
<p>1) Yes.
<br>
<p>2) No.
<br>
<p>Given #1: hurray?
<br>
<p>Given #2: guess you should try to accomplish those goals yourself.
<br>
<p><em>&gt; It is difficult to see a role for humans in this scenario.  Humans
</em><br>
<em>&gt; consume valuable resources, and could threaten the intelligence by
</em><br>
<em>&gt; destroying the planet.  Maybe a few might be left in isolated parts
</em><br>
<em>&gt; of the world.  But the intelligence would optimizes itself, why waste
</em><br>
<em>&gt; even 1% of the world's resources on man.  Certainly evolution has
</em><br>
<em>&gt; left no place on earth for any other man-like hominids.
</em><br>
<p>Geeze, if only there was something more than this planet ...
<br>
<p><em>&gt; If our computers threatened us, surely we could just turn them off? 
</em><br>
<em>&gt; That is easier said than done.
</em><br>
<p>You have any idea how easy it is to launch a nuclear warhead?
<br>
<p><em>&gt; The developers of the atomic bomb could not turn it off, even though
</em><br>
<em>&gt; some of them tried.
</em><br>
<p>You can stop it from working by using it.
<br>
<p><em>&gt; Further, the Internet has enabled criminals to create huge botnets of
</em><br>
<em>&gt; other people's computers that they can control.  The computer on your
</em><br>
<em>&gt; desk might be part of a botnet — it is very hard to know what a
</em><br>
<em>&gt; computer is thinking about.  Ordinary dumb botnets are very difficult
</em><br>
<em>&gt; to eliminate due to their distributed nature.  Imagine trying to
</em><br>
<em>&gt; control a truly intelligent botnet..
</em><br>
<p>&quot;Cap'n, just blow up the damn ship!&quot;
<br>
<p><em>&gt; But a botnet cannot be shot with a zap gun.  We live in the
</em><br>
<em>&gt; information age.
</em><br>
<p>You sir, need a nuclear zap gun. You want to really, really kill it, 
<br>
right?
<br>
<p><em>&gt; Presidents and dictators do not gain power through their own physical
</em><br>
<em>&gt; strength, but rather through their intelligence, drive and instincts.
</em><br>
<em>&gt;  Modern politicians already rely on sophisticated software to manage
</em><br>
<em>&gt; their campaigns and daily interactions.  Imagine if some of their
</em><br>
<em>&gt; software was truly intelligent.  Who would really be in control?
</em><br>
<p>Arguably who the hell is in control as it is now ? I'm not really sure 
<br>
control is an accurate framework to describe the peculiar situation.
<br>
<p><em>&gt; Just because an AI could dominate the world does not mean that it
</em><br>
<em>&gt; would want to.  But controlling one's environment (the world) is a
</em><br>
<em>&gt; subgoal of almost any other goal.  For example, to study the
</em><br>
<em>&gt; universe, or even to prolong human life, one needs to continue to
</em><br>
<em>&gt; exist, and to acquire and utilize resources to solve the given goal. 
</em><br>
<em>&gt; Allowing any competitor to kill the AI would defeat its ability to
</em><br>
<em>&gt; solve its base goal.
</em><br>
<p>I've already argued against ai-domination-scenarios above. Not that they 
<br>
are impossible, but that they are not the end of the world and hu 
<br>
alike.
<br>
<p><em>&gt; Philosophers have asked whether an artificial intelligence has real
</em><br>
<em>&gt; intelligence or is just simulating intelligence.  This is actually a
</em><br>
<em>&gt; non-question, because those that ask it cannot define what measurable
</em><br>
<em>&gt; property &quot;real&quot; intelligence has that simulated intelligence does not
</em><br>
<em>&gt; have.  It will be &quot;real&quot; enough if it dominates the world and
</em><br>
<em>&gt; destroys humanity.
</em><br>
<p>No, that &quot;real enough&quot; is re: any existential threat. That's completely 
<br>
different from the concept of intelligence. Whether or not it is 
<br>
intelligent is the issue ... not whether or not the result of death 
<br>
is ... sigh. There's so many complex strands of bullshit running 
<br>
through that paragraph of yours. It's not your fault, but I'm not 
<br>
prepared to go through it entirely. Let me try, but I can't guarantee 
<br>
anything here. Look: you are proposing that ai could end up with 
<br>
domination and death, and then you proceed to say that if the result is 
<br>
ending with domination or death and so on that then it was &quot;real&quot;, even 
<br>
though we're talking about *intelligence*, not about your inability to 
<br>
plan for existential threats.
<br>
<p><em>&gt; There are many doom's day scenarios.  Bio technologies, nano
</em><br>
<em>&gt; technologies, global warming, nuclear annihilation.  While these
</em><br>
<em>&gt; might be annoying, they are all within our normal understanding and
</em><br>
<em>&gt; some of humanity is likely to survive.  We also would have at least
</em><br>
<em>&gt; some time to understand and react to most them.  But intelligence is
</em><br>
<em>&gt; fundamental to our existence and its onset could be very fast.  How
</em><br>
<em>&gt; do you argue with a much more intelligent opponent?
</em><br>
<p>Stop arguing and just implement your damn solution *now*. Seriously. :-)
<br>
<p><em>&gt; (Biotechnology has been much over hyped as a threat.  We have been
</em><br>
<em>&gt; doing battle with microbes for billions of years, and our bodies are
</em><br>
<em>&gt; very good at fighting them.  It might also be possible to produce
</em><br>
<em>&gt; some increase human intelligence by tweaking the brain's
</em><br>
<em>&gt; biochemistry.  But again, evolution has also been trying to do this
</em><br>
<em>&gt; for a long time.  For a real intelligence explosion we need a
</em><br>
<em>&gt; technology that we really understand.  And that means digital
</em><br>
<em>&gt; computers.)
</em><br>
<p>You're assuming that evolution is goal-directed by saying that it has 
<br>
been trying to tweak intelligence to become even better at itself. That 
<br>
doesn't make sense at all and isn't Darwin. Not by a long shot.
<br>
<p><em>&gt; Trying to prevent people from building intelligent computers is like
</em><br>
<p>Trying to prevent people *period* is like trying to stop microbes.
<br>
<p><em>&gt; trying to stop the spread of knowledge.  Once Eve picks the apple it
</em><br>
<em>&gt; is very hard to put it back on the tree.  As we get get close to
</em><br>
<em>&gt; artificial intelligence capabilities, it would only take a small team
</em><br>
<em>&gt; of clever programmers anywhere in the world to push it over the line.
</em><br>
<p>No, again, programming isn't the hard part. We can do programming very, 
<br>
very well.
<br>
<p><em>&gt; But it is not so easy to build powerful new computer chips.  It takes
</em><br>
<p>What the hell is power in this context?
<br>
<p><em>&gt; large investments and large teams with many specialties from
</em><br>
<em>&gt; producing ultra pure silicon to developing extremely complex logical
</em><br>
<em>&gt; designs.  Extremely complex and precise machinery is required to
</em><br>
<p>Not really. There's guys with basements that are operating their own 
<br>
semiconductor fabrication installations. And when the industry was just 
<br>
starting out, like I mentioned above, people were using lenses from 
<br>
down at the shop to write on their masks and ethyl etches.
<br>
<p><em>&gt; build them.  Unlike programming, this is certainly not something that
</em><br>
<em>&gt; can be done in someone's garage.
</em><br>
<p>Hahaha.. I think I'll go cry now. Please stop ignoring me. Really. 
<br>
<p>^ Also, it's a good idea to not ignore the pioneers of the industry.
<br>
<p><em>&gt; We have a precedent in the control of nuclear fuel.  While far from
</em><br>
<em>&gt; perfect, we do have strong controls on the availability of bomb
</em><br>
<em>&gt; making materials, and they could be made stronger if the political
</em><br>
<em>&gt; will existed.  It is relatively easy to make an atomic bomb once one
</em><br>
<em>&gt; has enough plutonium or highly enriched uranium.  But making the fuel
</em><br>
<em>&gt; is much, much harder.  That is why we are alive today.
</em><br>
<p>Go check one of the recent threads on transhumantech. There's a story 
<br>
about how this isn't quite true. You can assemble the right resources 
<br>
from a variety of different companies and be completely legit.
<br>
<p><em>&gt; If someone produced a safe and affordable car powered by plutonium,
</em><br>
<em>&gt; would we welcome that as a solution to soaring fuel prices?  Of
</em><br>
<em>&gt; course not.  We would consider it far too dangerous to have plutonium
</em><br>
<em>&gt; scattered throughout society.
</em><br>
<p>Who is this 'we'?
<br>
<p><em>&gt; It is the goal of this paper to help raise awareness of the danger
</em><br>
<em>&gt; that computers pose.  If that can be raised to the level of nuclear
</em><br>
<em>&gt; bombs, then action might well be possible.
</em><br>
<p>Except that in your fear you forget the original Project Orion.
<br>
<p><em>&gt; So ideally we would try to reduce the power of new processors and
</em><br>
<em>&gt; destroy existing ones.
</em><br>
<p>What the hell is power? Have you ever bought a processor ?
<br>
<p><em>&gt; A 10 mega hertz processor running with 1 megabyte of memory is a
</em><br>
<em>&gt; thousand times weaker than current computers.
</em><br>
<p>Weaker? Since when does the quarts crystal frequency determine physical 
<br>
strength?
<br>
<p><em>&gt; Is the ability to have video games with very sexy graphics really
</em><br>
<em>&gt; worth the annihilation of humanity?
</em><br>
<p>You see, you're still assuming that we can't do multiply redundant 
<br>
backup systems, that earth is the only planet on which anything related 
<br>
to humans can be processed, and yet our presence on Mars shows 
<br>
completely otherwise. Have you ever read any science fiction? That 
<br>
might be a good introduction ...
<br>
<p><em>&gt; Yudkowsky proposed an alternate solution, namely that it might be
</em><br>
<em>&gt; possible to program a &quot;Friendly&quot; AI that will not hurt us.  If the
</em><br>
<em>&gt; very first AI was friendly, then it might be capable of preventing
</em><br>
<em>&gt; other unfriendly AIs from developing.  The first AI would have a head
</em><br>
<em>&gt; start on reprogramming itself, so no other AI would be able to catch
</em><br>
<em>&gt; it, at least initially.
</em><br>
<p>And if it's not? Let's work on those solutions too ... *cough*. What 
<br>
good does making policies against ebola do when you're bleeding out 
<br>
your ass?
<br>
<p><em>&gt; While a Friendly AI would be very nice, it is probably just wishful
</em><br>
<em>&gt; thinking.  There is simply nothing in it for the AI to be friendly to
</em><br>
<em>&gt; man.
</em><br>
<p>Why are you assuming incentives?
<br>
<p><em>&gt; The force of evolution is just too strong.
</em><br>
<p>What the hell?
<br>
<p><em>&gt;  The AI that is good at world domination is good at world domination.
</em><br>
<p>Tautologies are like tautologies.
<br>
<p><em>&gt; That said, there is no reason why limiting hardware should prevent
</em><br>
<em>&gt; research into friendly AI.  It just gives us more time.
</em><br>
<p>No, it just shoots ourselves in the foot. You don't stop anybody from 
<br>
making their own computers in their garages. This is what the whole 
<br>
freaking Homebrew Computer Club was about in the first place [indeed, 
<br>
they worked with some premanufactured components, but in reality those 
<br>
components started out in garages as well].
<br>
<p><em>&gt; As worms have evolved into apes, and apes to man, the evolution of
</em><br>
<em>&gt; man to an AI is just a natural process and something that could be
</em><br>
<em>&gt; celebrated rather than avoided.  Certainly it would probably only be
</em><br>
<em>&gt; a matter of a few centuries before modern man destroys the earth,
</em><br>
<em>&gt; whereas an artificial intelligence may be able to survive for
</em><br>
<em>&gt; millenia.
</em><br>
<p>Holy shit man, you don't understand evolution. Particularly the part 
<br>
about programming and &quot;man evolving into AI&quot;. What most people consider 
<br>
AI to be is something about programming. I truly doubt that a directed 
<br>
intelligent process, like programming, is a naturally occuring 
<br>
evolutionary process. 
<br>
<p><em>&gt; We know that all of our impulses are just simple consequences of
</em><br>
<em>&gt; evolution.  Love is an illusion, and all our endeavors are ultimately
</em><br>
<em>&gt; futile.  The Zen Buddhists are right — desires are illusions, their
</em><br>
<em>&gt; abandonment is required for enlightenment.
</em><br>
<p>Are you preaching?
<br>
<p><em>&gt; All very clever.  But I have two little daughters, whom I love very
</em><br>
<em>&gt; much and would do anything for.  That love may be a product of
</em><br>
<em>&gt; evolution, but it is real to me.  AI means their death, so it matters
</em><br>
<em>&gt; to me.  And so, I suspect, to the reader.
</em><br>
<p>There's nothing about ai that means their death. See above.
<br>
<p><em>&gt; It is of course possible that &quot;the Singularity&quot; will never happen. 
</em><br>
<em>&gt; That the problem of building an intelligent machine might just be too
</em><br>
<em>&gt; hard for man to solve.
</em><br>
<p>Have you considered sex?
<br>
<p><em>&gt; This paper aims to raise awareness, and to encourage real discussion
</em><br>
<em>&gt; as to the fate of humanity and whether that matters.
</em><br>
<p>You might be a newbie, and I might have been harsh on you. :)
<br>
<p>- Bryan
<br>
________________________________________
<br>
<a href="http://heybryan.org/">http://heybryan.org/</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="19070.html">Stuart Armstrong: "Re: [sl4] Re: More silly but friendly ideas"</a>
<li><strong>Previous message:</strong> <a href="19068.html">Thomas McCabe: "Re: [sl4] Re: Signaling after a singularity"</a>
<li><strong>In reply to:</strong> <a href="18982.html">Anthony Berglas: "Paper: Artificial Intelligence will Kill our Grandchildren"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19072.html">Anthony Berglas: "Re: [sl4] [Hplusroadmap] Paper: Artificial Intelligence will Kill our Grandchildren"</a>
<li><strong>Reply:</strong> <a href="19072.html">Anthony Berglas: "Re: [sl4] [Hplusroadmap] Paper: Artificial Intelligence will Kill our Grandchildren"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19069">[ date ]</a>
<a href="index.html#19069">[ thread ]</a>
<a href="subject.html#19069">[ subject ]</a>
<a href="author.html#19069">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:03 MDT
</em></small></p>
</body>
</html>
