<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Why I'm not more involved with academia</title>
<meta name="Author" content="Eliezer Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Why I'm not more involved with academia">
<meta name="Date" content="2004-10-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Why I'm not more involved with academia</h1>
<!-- received="Sat Oct 23 07:01:52 2004" -->
<!-- isoreceived="20041023130152" -->
<!-- sent="Sat, 23 Oct 2004 09:01:55 -0400" -->
<!-- isosent="20041023130155" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Why I'm not more involved with academia" -->
<!-- id="417A5643.5000301@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="5844e22f041022150847d1583b@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Why%20I'm%20not%20more%20involved%20with%20academia"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Oct 23 2004 - 07:01:55 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10072.html">Giu1i0 Pri5c0: "RE: Why SIAI is a nonprofit"</a>
<li><strong>Previous message:</strong> <a href="10070.html">Ben Goertzel: "RE: A funding suggestion: Solve The Riemann hypothesis ;)"</a>
<li><strong>In reply to:</strong> <a href="10060.html">Jeff Medina: "Re: SIAI: Donate Today and Tomorrow"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10074.html">Marieke Mooij: "AI and Biomed Tech"</a>
<li><strong>Reply:</strong> <a href="10074.html">Marieke Mooij: "AI and Biomed Tech"</a>
<li><strong>Reply:</strong> <a href="10078.html">Ben Goertzel: "RE: Why I'm not more involved with academia"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10071">[ date ]</a>
<a href="index.html#10071">[ thread ]</a>
<a href="subject.html#10071">[ subject ]</a>
<a href="author.html#10071">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Jeff Medina wrote:
<br>
<em>&gt; Robin Lee Powell wrote: &quot;IIRC, Eliezer is not allowed to put Ph.D.
</em><br>
<em>&gt; after his name.  That pretty much rules out this avenue of approach.&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; That absolutely *does not* rule out this avenue of approach.
</em><br>
<p>Correct.
<br>
<p><em>&gt; Many
</em><br>
<em>&gt; respected journals and conferences in the relevant areas are
</em><br>
<em>&gt; blind-reviewed (such that the academic credentials of the author of
</em><br>
<em>&gt; the paper is made irrelevant, because the author's identity &amp; other
</em><br>
<em>&gt; info is kept secret), and even among the many which are not quality
</em><br>
<em>&gt; submissions are never rejected or looked down upon simply due to the
</em><br>
<em>&gt; lack of a Ph.D. by the author.
</em><br>
<p>Papers which are *not* blind-reviewed show a *decided* bias toward known, 
<br>
prestigious researchers and institutions.  That is *why* some journals are 
<br>
blind-reviewed.  Most aren't, and it doesn't help that other journals are 
<br>
blind-reviewed if the one you want to target isn't.  I've read studies 
<br>
assessing the bias, but though I googled on &quot;effectiveness of peer review&quot; 
<br>
I failed to track them down.  I did find other interesting material 
<br>
including, to pick an arbitrary example, a study by Rothwell and Martyn 
<br>
(2000) showing that peer review in two neuroscience journals was not 
<br>
reproducible; that is, agreement between reviewers is not significantly 
<br>
greater than chance.  My recollection is that this result is widespread in 
<br>
studies of this kind.  I include this tidbit by way of saying that 
<br>
experimental study of peer review has produced surprising and alarming 
<br>
results, so be sure to check out peer-reviewed studies of peer review 
<br>
before praising its effectiveness.
<br>
<p>(Rothwell PM, Martyn CN. Reproducibility of peer review in clinical 
<br>
neuroscience. Is agreement between reviewers any greater than would be 
<br>
expected by chance alone? Brain 2000;123:1964-9.)
<br>
<p>This has nothing to do with the reason I don't write additional academic 
<br>
papers.  I just thought I'd mention it.
<br>
<p>**
<br>
<p>First, you'll note that I say &quot;write additional papers&quot;.  I allocated one 
<br>
month in early 2002 to write &quot;Levels of Organization in General 
<br>
Intelligence&quot; (LOGI) in the best academic style I could manage.  It 
<br>
actually took four months.  Since then the draft has been online at 
<br>
<a href="http://intelligence.org/LOGI/">http://intelligence.org/LOGI/</a>.  In 2005 this paper will finally appear in 
<br>
&quot;Artificial General Intelligence&quot;, eds. Goertzel and Pennachin, to be 
<br>
published by Springer-Verlag in 2005.  (The three-year delay was for the 
<br>
entire book, not my own paper; *I* turned in my homework on time.)
<br>
<p>The fact that none of the people plaguing me to write papers have even 
<br>
*noticed* &quot;Levels of Organization in General Intelligence&quot;, speaking 
<br>
instead as if I haven't written *any* papers, is indeed related to the 
<br>
reason I am not more involved with academia.
<br>
<p>I've come a long way over the eight years since 1996.  People said to me: 
<br>
Write up your ideas about AI in a web page.  In 1998 I did.  Then new 
<br>
people came along and they said:  You'll never get anywhere with this, no 
<br>
one will be interested enough to pay you to do this.  In 2000, thanks to 
<br>
Brian Atkins, the Singularity Institute started up.  Possibly that 
<br>
impressed a few people who never thought I'd get that far.  Then new people 
<br>
came along, to whom Eliezer had *always been* a part of the Singularity 
<br>
Institute, so it wasn't impressive, and they said:  No one will ever pay 
<br>
attention to you unless you do as we say and write some kind of paper 
<br>
targeted at academia and get it published.  In 2002 I did.  I didn't expect 
<br>
anyone to notice, and no one did, but the effort of writing the LOGI paper 
<br>
served to help me unify my ideas and force me to read relevant literature 
<br>
and therefore I account it a partial success.  And lo, the people said: 
<br>
What you really need, Eliezer, is to write some kind of paper targeted at 
<br>
academia.
<br>
<p>Someone always thinks there's just one more thing you need to do.  *That* 
<br>
never changes, no matter how many times you fulfill the request.  They just 
<br>
find something else for you to do.  Often it's something you've already 
<br>
done.  I wasn't puzzled by this.  I expected it.  Thus the particular 
<br>
things that I did were selected strictly on the basis of their needing 
<br>
doing, rather than to one-up naysayers.
<br>
<p>Case in point:  Dr. Eric Drexler and _Nanosystems_.
<br>
<p>Before:  Eric Drexler has no PhD and hasn't written up his ideas in great 
<br>
gory technical detail.  People tell him:  Eric, no one will pay attention 
<br>
to you if you don't have a PhD.  People tell him:  Eric, you need to write 
<br>
up your technical ideas in great gory detail in a way that a wide audience 
<br>
can understand.
<br>
<p>Eric spends six years writing _Nanosystems_ and making it presentable to 
<br>
any technical reader without demanding a specific background in chemistry, 
<br>
physics, or computer science.  Eric defends _Nanosystems_ as his thesis and 
<br>
receives the world's first PhD in nanotechnology from MIT.
<br>
<p>Afterward:  None of the naysayers read _Nanosystems_ or even mention it 
<br>
exists.  No one pays any more attention to Drexler than before.  They just 
<br>
shift their criterion to something else Eric hasn't done yet.  Often they 
<br>
indignantly proclaim that Drexler hasn't given any technical presentation 
<br>
of his ideas - complete indifference to the work already accomplished.  The 
<br>
same people who liked Drexler before still like him.  The kind of people 
<br>
who objected to Drexler before find something different to which to object.
<br>
<p>I suspect those who objected to nanotechnology did not say:  &quot;Hm... I have 
<br>
no idea whether I like this or not... but wait!  Drexler doesn't have a 
<br>
PhD!  Okay, now I've decided that nanotechnology is impossible and Drexler 
<br>
is scaring our children.&quot;  The causal sequence of events is more like, 
<br>
&quot;Eek!  Too weird!  Hm, it seems that I disbelieve in nanotechnology.  I 
<br>
wonder why I disbelieve in nanotechnology?  (Searches for reason.)  It must 
<br>
be because Drexler doesn't have a PhD, hey, yeah, that's it.&quot;  After 
<br>
Drexler got a PhD, exactly the same process took place, only the 
<br>
rationalization search terminated elsewhere.
<br>
<p>Drexler has a personality far better suited to academia than I'll ever be. 
<br>
&nbsp;&nbsp;He's humble.  He did everything by the book, the way he was supposed to. 
<br>
&nbsp;&nbsp;Academia... to put it bluntly, they spit in his face.  And Drexler had a 
<br>
vastly easier problem to explain, in a field with all the underlying 
<br>
physical equations established and agreed upon.  If Drexler didn't make it 
<br>
in academia there's no chance in hell that I could do so.  Friendly AI 
<br>
would be two orders of magnitude harder to sell to academia than molecular 
<br>
nanotechnology.  I pointed out that last part to Drexler, by the way; he 
<br>
agreed.  And come to think, while he didn't say a word to me against 
<br>
academia or the academic system, Dr. Eric Drexler is *not* on the list of 
<br>
people whose advice to me included getting a PhD.
<br>
<p>I don't want to sound like I'm criticizing Drexler's intelligence.  Drexler 
<br>
did not have Drexler's case to warn him.  Drexler's choices were different; 
<br>
he may have had nothing better to try than getting a PhD and spending six 
<br>
years writing a technical book.
<br>
<p>But people seem to be absurdly optimistic about how easy it is for the 
<br>
actors on stage to carry out the helpful advice shouted from the audience. 
<br>
&nbsp;&nbsp;Then again, as plenty of studies show, people are also absurdly 
<br>
optimistic about the course of their own lives - except for the severely 
<br>
depressed, who are sometimes properly calibrated with respect to outcomes, 
<br>
a phenomenon known as &quot;depressive realism&quot;.  (I am not making this up.) 
<br>
Part of the reason why people are absurdly optimistic is that they think: 
<br>
I'll just do X, and then everything will be all right!  Not:  I'll try to 
<br>
do X, it will take four times as long as I expect, I'll probably fail, and 
<br>
even if I succeed, only one in ten successes of this kind have as great an 
<br>
impact as the one I pleasantly imagined.
<br>
<p>I remember meeting Chris Phoenix of CRN at a Foresight Gathering, and Chris 
<br>
Phoenix spoke optimistically of the day when molecular manufacturing is 
<br>
proved possible, and all the naysayers have to admit it... and I said: 
<br>
&quot;Yes, Chris, we can look forward to the fine day when the naysayers are 
<br>
presented with a working example of mechanosynthesis, and they are finally 
<br>
forced to stand up and say, in unison: 'Oh, but that isn't *really* 
<br>
nanotechnology.'&quot;
<br>
<p>Did I get a Ph.D., nothing would change.  I'd just hear: oh, but you aren't 
<br>
an eminent scientist in the field, go write more papers.
<br>
<p>If I were the sort of person who chased all over the map - starting 
<br>
companies, getting PhDs, whatever - then I wouldn't be here in the first 
<br>
place.  My life would have happened to me while I was making other plans. 
<br>
Antoine de Saint-Exupéry:  &quot;Perfection is achieved, not when there is 
<br>
nothing left to add, but when there is nothing left to take away.&quot;  People 
<br>
overestimate conjunctive probabilities and underestimate disjunctive 
<br>
probabilities; they overestimate the chance of many things going right in 
<br>
sequence, underestimate the probability of a single thing going wrong.  The 
<br>
way to success is to remove everything from the plan that doesn't 
<br>
absolutely *have* to be there.  The way to have any chance at all of 
<br>
finishing on time is to do nothing that is not absolutely necessary.
<br>
<p>Is being a part of academia absolutely necessary to success?  I don't think 
<br>
so.  No one's told me to get a PhD in something because in-depth technical 
<br>
mastery of that subject is absolutely necessary to the creation of AI, and 
<br>
yet that is *supposed* to be what PhDs are about.  No one's said a word 
<br>
about learning or knowledge.  It's all about the impressiveness of some 
<br>
letters after your name.  I know I'm far from the first person to point out 
<br>
the massive failure of the educational system, but it remains just as huge 
<br>
a problem and just as horribly awry.  The failure doesn't go away just 
<br>
because someone has pointed it out before.
<br>
<p>To tackle AI I've had to learn, at one time or another, evolutionary 
<br>
psychology, evolutionary biology, population genetics, game theory, 
<br>
information theory, Bayesian probability theory, mathematical logic, 
<br>
functional neuroanatomy, computational neuroscience, anthropology, 
<br>
computing in single neurons, cognitive psychology, the cognitive psychology 
<br>
of categories, heuristics and biases, decision theory, visual neurology, 
<br>
linguistics, linear algebra, physics, category theory, and probably a dozen 
<br>
other fields I haven't thought of offhand.  Sometimes, as with evolutionary 
<br>
psychology, I know the field in enough depth to write papers in it.  Other 
<br>
times I know only the absolute barest embarassingly simple basics, as with 
<br>
category theory, which I picked up less than a month ago because I needed 
<br>
to read other papers written in the language of category theory.  But the 
<br>
point is that in academia, where crossbreeding two or three fields is 
<br>
considered daring and interdisciplinary, and where people have to achieve 
<br>
supreme depth in a single field in order to publish in its journals, that 
<br>
kind of broad background is pretty rare.
<br>
<p>I'm a competent computer programmer with strong C++, Java, and Python, and 
<br>
I can read a dozen other programming languages.
<br>
<p>I accumulated all that (except category theory) before I was twenty-five 
<br>
years old, which is still young enough to have revolutionary ideas.
<br>
<p>That's another thing academia doesn't do very well.  By the time people 
<br>
finish a Ph.D. in *one* field, they might be thirty years old, past their 
<br>
annus mirabilis years.  To do AI you need a dozen backgrounds and you need 
<br>
them when you're young.  Small wonder academia hasn't had much luck on AI. 
<br>
&nbsp;&nbsp;Academia places an enormous mountain of unnecessary inconveniences and 
<br>
little drains of time in the way of learning and getting the job done.  Do 
<br>
your homework, teach your classes, publish or perish, compose grant 
<br>
proposals, write project reviews, suck up to the faculty... I'm not saying 
<br>
it's all useless.  Someone has to teach classes.  But it is not absolutely 
<br>
necessary to solving the problem of Friendly AI.
<br>
<p>Nearly all academics are untrained in the way of rationality.  Not 
<br>
surprising; few academics are fifth-dan black belts and there are a lot 
<br>
more fifth-dan black belts than fifth-dan rationalists.  But if I were in 
<br>
academia I would be subject to the authority of those who were not Bayesian 
<br>
Masters.  In the art of rationality, one seeks to attain the perception 
<br>
that most of the things that appear to be reasons and arguments are not 
<br>
Bayesian.  Eliminate the distractions, silence the roar of cognitive noise, 
<br>
and you can finally see the small plain trails of genuine evidence.  One of 
<br>
my academic friends once asked me to look at a paper on decision theory; 
<br>
the paper described the conventional theory, presented a problem, and then 
<br>
proposed several different individual patches to the conventional theory 
<br>
and analyzed the patches individually, concluding that none of the 
<br>
solutions were satisfactory.  I replied by arguing that the conventional 
<br>
theory actually contained *two* independent foundational errors, which 
<br>
needed to be simultaneously refactored to solve the problem, and in fact, 
<br>
he needed to look at this whole problem a different way.  And the one said: 
<br>
&nbsp;&nbsp;But I have to take the traditional theory as a point of departure and 
<br>
then present changes to it, because that's what the reviewers will expect. 
<br>
&nbsp;&nbsp;And I said:  Okay, but for myself I don't have to give a damn about 
<br>
reviewers, and so I plan to go on using the solution with two simultaneous 
<br>
corrections.  That bias against two simultaneous changes, owing to the need 
<br>
to take the conventional theory as a point of departure, was justified as 
<br>
necessary by pointing to social forces instead of Bayesian forces.  That 
<br>
makes it a distraction.
<br>
<p>I refuse to accept that entire class of distractions.  As an independent 
<br>
scholar, I never have to give any reason for saying something or thinking 
<br>
something that points to social forces instead of the facts of the matter. 
<br>
&nbsp;&nbsp;I have the freedom to do the right thing, without the faintest bias 
<br>
toward the academically acceptable thing except insofar as the academically 
<br>
acceptable thing happens to be right.  I have the luxury of giving no more 
<br>
credence to an idea than the weight of Bayesian evidence calls for, even if 
<br>
the idea has become fixed in academia through any of the non-Bayesian 
<br>
processes that prevail there.
<br>
<p>Now, most of the time, I don't second-guess academia - certainly not in 
<br>
established fields with great weights of evidence, nor after learning just 
<br>
the basics of something.  Like I said, it's dangerous to be half a 
<br>
rationalist; if you learn the skill of challenging conventional ideas, 
<br>
you'd damn well better learn the skill of accepting conventional ideas, or 
<br>
end up worse off than before.  But sometimes, on the fringes, AI for 
<br>
example, people just make stuff up that sounds cool, and it becomes fixed 
<br>
because everyone repeats it.  Look at Freudian analysis: not one scrap of 
<br>
experimental evidence.  It was a major academic field, with peer-reviewed 
<br>
journals and everything, but not the faintest hint of science.  If that's 
<br>
the academic standard then academia's standards are too damn low.  Or 
<br>
sometimes the people in one field don't know about the results in another 
<br>
field, and they say things that are silly and get past the reviewers, 
<br>
because the people who could catch the mistake work in a different building 
<br>
of the college campus.  That likewise happens, *a lot*, in AI.
<br>
<p>It seems to me that the secret of effectiveness is refusing to be 
<br>
distracted.  At one point in my life I did permit myself to be 
<br>
distracted... by writing freelance programs, by planning to start a 
<br>
company... eventually I noticed that the only projects in my life that had 
<br>
ever done the slightest bit of good were the ones that were *directly* on 
<br>
track to the Singularity.  *Not* the distraction projects that I thought 
<br>
would provide resources or whatever, but the projects that were directly 
<br>
part of the critical path to AI.  In 1998 I took one month out of my 
<br>
all-important plots to accumulate Singularity resources to write &quot;Coding a 
<br>
Transhuman AI&quot;, and in the end CaTAI that was the only thing I did that 
<br>
year that actually mattered.  And that was very much the story of my life, 
<br>
until the day I finally snapped and decided to concentrate solely on the 
<br>
Singularity.  Today I refuse to be distracted.  Not by academia, not by 
<br>
technology companies, not by anything.  All I ask of myself is that I do 
<br>
this one thing, solve this one challenge of Friendly AI.
<br>
<p><em>&gt; 2.  If lacking a PhD really becomes a problem... well, why not get
</em><br>
<em>&gt; one? PhD students get living stipends and support for their research.
</em><br>
<em>&gt; So even being a PhD student may well put Eliezer in a better position
</em><br>
<em>&gt; to pursue his research than the current scenario allows. Further, if
</em><br>
<em>&gt; he (or anyone else involved) doesn't like the idea of being forced to
</em><br>
<em>&gt; take 2 years of coursework for the PhD, he could always pursue the PhD
</em><br>
<em>&gt; outside of the U.S., where PhDs are pure research degrees with no
</em><br>
<em>&gt; course requirements.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 3.  There are a couple of schools (e.g., The University of Technology,
</em><br>
<em>&gt; Sydney, in Australasia) that award PhDs by prior publication. After
</em><br>
<em>&gt; applying, you put together a portfolio of your research, and write an
</em><br>
<em>&gt; overarching paper that illustrates your contribution to the field of
</em><br>
<em>&gt; study, and if deemed PhD-level, you are granted a PhD. I've come
</em><br>
<em>&gt; across at least a few professors in the UK and elsewhere who have
</em><br>
<em>&gt; received their doctorates in this manner. (I've also seen quite a few
</em><br>
<em>&gt; professors with just Master's, but this falls back to point 2 above).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Of course, most people aren't aware of some or all of the above
</em><br>
<p>I wasn't aware.  Thanks.  If I don't need to spend eight years, that does 
<br>
shift the cost/benefit ratio.  But not far enough, I'm afraid.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10072.html">Giu1i0 Pri5c0: "RE: Why SIAI is a nonprofit"</a>
<li><strong>Previous message:</strong> <a href="10070.html">Ben Goertzel: "RE: A funding suggestion: Solve The Riemann hypothesis ;)"</a>
<li><strong>In reply to:</strong> <a href="10060.html">Jeff Medina: "Re: SIAI: Donate Today and Tomorrow"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10074.html">Marieke Mooij: "AI and Biomed Tech"</a>
<li><strong>Reply:</strong> <a href="10074.html">Marieke Mooij: "AI and Biomed Tech"</a>
<li><strong>Reply:</strong> <a href="10078.html">Ben Goertzel: "RE: Why I'm not more involved with academia"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10071">[ date ]</a>
<a href="index.html#10071">[ thread ]</a>
<a href="subject.html#10071">[ subject ]</a>
<a href="author.html#10071">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:49 MDT
</em></small></p>
</body>
</html>
