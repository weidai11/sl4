<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Military Friendly AI</title>
<meta name="Author" content="Brian Atkins (brian@posthuman.com)">
<meta name="Subject" content="Re: Military Friendly AI">
<meta name="Date" content="2002-06-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Military Friendly AI</h1>
<!-- received="Fri Jun 28 07:13:56 2002" -->
<!-- isoreceived="20020628131356" -->
<!-- sent="Thu, 27 Jun 2002 23:01:56 -0400" -->
<!-- isosent="20020628030156" -->
<!-- name="Brian Atkins" -->
<!-- email="brian@posthuman.com" -->
<!-- subject="Re: Military Friendly AI" -->
<!-- id="3D1BD1A4.E55D5B16@posthuman.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJEEAHCMAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Brian Atkins (<a href="mailto:brian@posthuman.com?Subject=Re:%20Military%20Friendly%20AI"><em>brian@posthuman.com</em></a>)<br>
<strong>Date:</strong> Thu Jun 27 2002 - 21:01:56 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4491.html">Samantha Atkins: "Re: Military Friendly AI"</a>
<li><strong>Previous message:</strong> <a href="4489.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>In reply to:</strong> <a href="4489.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4493.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4493.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4490">[ date ]</a>
<a href="index.html#4490">[ thread ]</a>
<a href="subject.html#4490">[ subject ]</a>
<a href="author.html#4490">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Brian Atkins wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; James Higgins wrote:
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; I would tend to worry very little if Ben was about to kick off a
</em><br>
<em>&gt; &gt; &gt; Singularity attempt, but I would worry very much if you, Eliezer, were.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; That's quite odd since last I checked Ben wasn't even interested in
</em><br>
<em>&gt; &gt; the idea of Friendliness until we invented it and started pointing out
</em><br>
<em>&gt; &gt; to SL4 exactly how important it is.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Quite the contrary, Brian. I -- like Pei Wang, Minsky, Peter Voss, and many
</em><br>
<em>&gt; other AI researchers -- have been thinking about Friendliness for many
</em><br>
<em>&gt; years.  Since Eliezer was in diapers -- and in Minsky's case, since before I
</em><br>
<em>&gt; or Eliezer were born!  These are not new ideas.  The term &quot;Friendly AI&quot; is
</em><br>
<em>&gt; due to Eli, so far as I know, but the concept certainly is not.
</em><br>
<p>I'll have to disagree, and say that many of the ideas in CFAI are AFAIK
<br>
novel. Give him a little credit for inventing more than just the term. And
<br>
as for all the thinking you and others did, I don't see that it produced
<br>
much in the way of results during the period before CFAI was published.
<br>
Where's all the prior art, and why aren't we debating that stuff here, and
<br>
comparing it to CFAI to see which is better?
<br>
<p><em>&gt; 
</em><br>
<em>&gt; Over the last 15 years, I have chosen to focus my research work, and my
</em><br>
<em>&gt; writing, on the creation of real AI, rather than on the Friendliness aspect
</em><br>
<em>&gt; specifically.  This is not because I consider Friendliness unimportant.  It
</em><br>
<em>&gt; is, rather, because -- unlike Eliezer -- I think that we don't yet know
</em><br>
<em>&gt; enough about AGI to make a really detailed, meaningful analysis of the
</em><br>
<em>&gt; Friendly AI issue.  I think it's good to think about it now, but it's
</em><br>
<em>&gt; premature to focus on it now.  I think we will be able to develop a real
</em><br>
<em>&gt; theory of Friendly AI only after some experience playing around with
</em><br>
<em>&gt; infrahuman AGI's that have a lot more general intelligence than any program
</em><br>
<em>&gt; now existing.
</em><br>
<p>Which tends to strike me as a dangerous approach.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; I believe my attitude toward Friendliness is typical of AGI researchers.
</em><br>
<p>Unfortunately, yes not many people seem to as careful as we all would like
<br>
when playing around with existential technologies.
<br>
<p><em>&gt; It's not that no one but Eliezer realizes the issue exists, or is
</em><br>
<em>&gt; important -- it's not that he brought the issue to the AI community's
</em><br>
<em>&gt; intention. It's rather that he's nearly the only one who believes it's
</em><br>
<em>&gt; possible to create a detailed theory of Friendliness *at this stage* prior
</em><br>
<em>&gt; to the existence of infrahuman AGI's with a decent level of general
</em><br>
<em>&gt; intelligence.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Personally, I think he's largely wrong on this; I think that his theory of
</em><br>
<em>&gt; Friendly AI is not all that valuable, and that it will look somewhat
</em><br>
<em>&gt; oversimplistic and naive, in hindsight, when we reach the point of having a
</em><br>
<em>&gt; powerful infrahuman AGI.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The idea of self-modifying AI causing exponentially increasing intelligence
</em><br>
<em>&gt; is also something AI researchers have been talking about for years -- Minsky
</em><br>
<em>&gt; since the 70's or earlier.  What distinguishes Eliezer is not his
</em><br>
<em>&gt; understanding of the long-term relevance of this issue, but the fact that
</em><br>
<em>&gt; he's one of very few AI researchers who thinks that this issue is worth
</em><br>
<em>&gt; paying a lot of attention to *now*.  Most AI researchers, rather, believe
</em><br>
<em>&gt; that only once we have an infrahuman AGI with a lot of intelligence, does it
</em><br>
<em>&gt; make sense to pay a lot of attention to intelligence-increasing
</em><br>
<em>&gt; self-modification.
</em><br>
<p>So can your position be summarized as: we'll build our AI, get it working at
<br>
some subhuman level, and then when we guess it needs it we'll stop running
<br>
it for a while until we figure out how to ensure &quot;Friendliness&quot;? I think
<br>
your protocol needs to be fleshed out for us further so we can feel more
<br>
comfortable with your plans.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; Now, no one has proved they know how to construct an AGI.  It is possible
</em><br>
<em>&gt; that Eliezer is correct that it makes sense to spend a lot of time on these
</em><br>
<em>&gt; issues *now*, before we have a decent infrahuman AGI.  But it is not right
</em><br>
<em>&gt; to claim that others don't understand these issues, or think they're
</em><br>
<em>&gt; serious, just because they think the task of creating a decent &quot;real AI&quot;
</em><br>
<em>&gt; should come temporally first.
</em><br>
<p>It sounds dangerous to me (and I guess others here) to build the AI first,
<br>
and let it run for some time without any special F features built in. How
<br>
will your protocol ensure that it does not take off, and if it does how
<br>
are we ensured it will turn out ok?
<br>
<p><em>&gt; 
</em><br>
<em>&gt; I note that, while Eli has been focusing on these topics, he has not made
</em><br>
<em>&gt; all that much observable progress on actually creating AGI.  He has
</em><br>
<em>&gt; performed a valuable service by bringing ideas like AI morality and AI
</em><br>
<em>&gt; self-modification to a segment of the population that was not familiar with
</em><br>
<em>&gt; them (mostly, members of the futurist community who are not AI researchers).
</em><br>
<em>&gt; But by making this choice as to how to spend his time, he has chosen not to
</em><br>
<em>&gt; progress as far on the AI design front as he could have otherwise.
</em><br>
<p>Can we please stop talking about Eli Eli Eli for a few minutes? Thank you
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; Not that it seems to have had much
</em><br>
<em>&gt; &gt; effect since he still has no plans that I know of to alter his rather
</em><br>
<em>&gt; &gt; dramatically risky seed AI experimentation protocol (basically not
</em><br>
<em>&gt; &gt; adding any Friendliness features until /after/ he decides that the
</em><br>
<em>&gt; &gt; AI has advanced enough) (he has a gut feel you see, and there's certainly
</em><br>
<em>&gt; &gt; no chance of a hard takeoff, and even if it did he's quite sure it would
</em><br>
<em>&gt; &gt; all turn out ok... trust him on it)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think that it is not possible to create a meaningful &quot;Friendly AI&quot; aspect
</em><br>
<em>&gt; to Novamente at this stage.  I am skeptical that it's possible to create a
</em><br>
<em>&gt; meaningful &quot;Friendly AI&quot; aspect to any AI architecture in advance, before
</em><br>
<em>&gt; one has a good understanding of the characteristics of the AI in action.
</em><br>
<p>Why not at least build in some kind of &quot;controlled ascent&quot; features?
<br>
<p><em>&gt; 
</em><br>
<em>&gt; I do trust my intuition that there is no chance of Novamente having a hard
</em><br>
<em>&gt; takeoff right now.  The damn design is only about 20% implemented!  We will
</em><br>
<em>&gt; know when we have a system that has some autonomous general intelligence,
</em><br>
<em>&gt; and at that point we will start putting Friendliness-oriented controls in
</em><br>
<em>&gt; the system.  Putting this sort of control into our system now would really
</em><br>
<em>&gt; just be silly -- pure window dressing.
</em><br>
<p>So at exactly what stages of the development do you plan to implement which
<br>
F features? You have some sort of protocol, right?
<br>
<p><em>&gt; 
</em><br>
<em>&gt; You may say &quot;Yeah, Ben, but you can't absolutely KNOW the system won't
</em><br>
<em>&gt; achieve a hard takeoff tomorrow.&quot;  No, I can't absolutely know that, and I
</em><br>
<em>&gt; can't absolutely know that I'm not really a gerbil dreaming I'm an AI
</em><br>
<em>&gt; scientist, either; nor that the universe won't spontaneously explode three
</em><br>
<em>&gt; seconds from now.  But there's such a thing as common sense.  There are a
</em><br>
<em>&gt; dozen other people who know the Novamente codebase, and every single one of
</em><br>
<em>&gt; them would agree: there is NO chance of Novamente as it is now, incomplete,
</em><br>
<em>&gt; achieving any kind of takeoff.  It does not have significantly more chance
</em><br>
<em>&gt; of doing so right now than Microsoft Windows does.  I am sure that if Eli
</em><br>
<em>&gt; saw the codebase as it now exists he would agree -- not that it's bad, it's
</em><br>
<em>&gt; just very incomplete.
</em><br>
<p>This discussion is not about right now, it is about later.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt;I guess it is because we go to the effort to
</em><br>
<em>&gt; &gt; put our plans out for public review and he sits in with the rest of the
</em><br>
<em>&gt; &gt; crowd picking them apart. At least we _have_ plans out for public
</em><br>
<em>&gt; &gt; review.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Eliezer has a much more detailed plan for AI friendliness than I do, but in
</em><br>
<em>&gt; my view it's sort of a &quot;castle in the air,&quot; because it's based on certain
</em><br>
<em>&gt; assumptions about how an AI will work, and Eliezer does not have a detailed
</em><br>
<em>&gt; design (let alone an implementation) for an AI fulfilling these assumptions.
</em><br>
<em>&gt; The whole theory may be meaningless, if it turns out it's not possible to
</em><br>
<em>&gt; make (or even thoroughly design) an AGI meeting the assumptions of the
</em><br>
<em>&gt; theory.
</em><br>
<p>If your design is incapable of supporting such features, and you have been
<br>
unable to come up with your own seemingly impregnable way to keep your AI
<br>
&quot;Friendly&quot; throughout its development into superintelligence, then maybe
<br>
we should be getting worried?
<br>
<p>I assume that if you get your working infrahuman AI, and are unable to
<br>
come up with a bulletproof way of keeping it &quot;Friendly&quot;, you will turn it
<br>
off? How will you judge whether or not it is safe to continue allowing it
<br>
to grow?
<br>
<p><em>&gt; 
</em><br>
<em>&gt; Also on that page you will find a link to an essay I wrote on &quot;AI Morality&quot;.
</em><br>
<em>&gt; (Eliezer and some others pointed out some minor flaws in that paper, which I
</em><br>
<em>&gt; have not yet found time to correct, but it still basically represents my
</em><br>
<em>&gt; views.)  I do not give a detailed theory of Friendly AI comparable to
</em><br>
<em>&gt; Eliezer's there, but I do explain generally how I expect AI morality to
</em><br>
<em>&gt; work, and discuss some of the issues I have with Eliezer's ideas on Friendly
</em><br>
<em>&gt; AI.  I stress that this is something I've thought about &quot;in the background&quot;
</em><br>
<em>&gt; for a long time, but NOT something that has been a major focus of my work
</em><br>
<em>&gt; lately, because of my believe that the right way to do Friendly AI will only
</em><br>
<em>&gt; be determinable via substantial experimentation with early-stage infrahuman
</em><br>
<em>&gt; AGI's.
</em><br>
<p>Well do you think it's worth our trouble to read it? If so I'd like to see
<br>
some discussion about it (perhaps Eliezer will allow you to repost the flaws
<br>
he saw in it) since I don't recall any threads regarding it (if I've
<br>
forgotten, someone please give me a URL to the archives, thanks).
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; How about we set July for picking Ben's plan apart. After all he is far
</em><br>
<em>&gt; &gt; closer to completion (he claims) than anyone else, yet few people here
</em><br>
<em>&gt; &gt; seem to have anywhere near as good a grasp of his ideas compared to
</em><br>
<em>&gt; &gt; SIAI's.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Disclaimer: this post is not intended to start any kind of us vs. them
</em><br>
<em>&gt; &gt; phenomena. It exists simply to point out a perceived important difference
</em><br>
<em>&gt; &gt; in the amount of critical discussion regarding the two
</em><br>
<em>&gt; &gt; organizations' plans.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Regarding picking my ideas on Friendly AI apart, that sounds like a fun
</em><br>
<em>&gt; discussion!  However, I will be on vacation from July 1-11 (though I will
</em><br>
<em>&gt; check e-mail occasionally); hence I suggest to postpone a long and detailed
</em><br>
<em>&gt; thread on this until mid-July when I get back.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Regarding picking the Novamente AI design apart, unfortunately a really
</em><br>
<em>&gt; detailed thread on that will have to wait until sometime in 2003, when the
</em><br>
<em>&gt; book comes out.  There is a lot of depth there, much more than most of the
</em><br>
<em>&gt; readers of the first draft saw (due to the flaws of the first draft), and a
</em><br>
<em>&gt; detailed discussion of the design among a group who doesn't *know* the
</em><br>
<em>&gt; details of the design, is unlikely to be productive.
</em><br>
<em>&gt; 
</em><br>
<p>I agree, at this time I'm more interested in discussing your FAI ideas and
<br>
experiment protocol. We can take a break while you're gone.
<br>
<p>I just started reading your AI Morality paper, I'm sure I'll have more
<br>
comments later, but this part is a bit scary I guess to everyone here who
<br>
is afraid of the initial AI programmers having too much control over the
<br>
AI's final state:
<br>
<p>&nbsp;&nbsp;&quot;But intuitively, I feel that an AGI with these values is going to be a
<br>
&nbsp;&nbsp;&nbsp;positive force in the universe – where by “positive” I mean “in accordance
<br>
&nbsp;&nbsp;&nbsp;with Ben Goertzel’s value system”.&quot;
<br>
<p><a href="http://www.goertzel.org/dynapsyc/2002/AIMorality.htm">http://www.goertzel.org/dynapsyc/2002/AIMorality.htm</a>
<br>
<pre>
-- 
Brian Atkins
Singularity Institute for Artificial Intelligence
<a href="http://www.intelligence.org/">http://www.intelligence.org/</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4491.html">Samantha Atkins: "Re: Military Friendly AI"</a>
<li><strong>Previous message:</strong> <a href="4489.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>In reply to:</strong> <a href="4489.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4493.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4493.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4490">[ date ]</a>
<a href="index.html#4490">[ thread ]</a>
<a href="subject.html#4490">[ subject ]</a>
<a href="author.html#4490">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
