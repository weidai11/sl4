<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Fwd: Re: How Kurzweil lost the Singularity</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Fwd: Re: How Kurzweil lost the Singularity">
<meta name="Date" content="2002-06-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Fwd: Re: How Kurzweil lost the Singularity</h1>
<!-- received="Wed Jun 19 15:07:27 2002" -->
<!-- isoreceived="20020619210727" -->
<!-- sent="Wed, 19 Jun 2002 15:05:13 -0400" -->
<!-- isosent="20020619190513" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Fwd: Re: How Kurzweil lost the Singularity" -->
<!-- id="3D10D5E9.6010105@pobox.com" -->
<!-- charset="iso-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Fwd:%20Re:%20How%20Kurzweil%20lost%20the%20Singularity"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed Jun 19 2002 - 13:05:13 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4104.html">Eliezer S. Yudkowsky: "Re: How Kurzweil lost the Singularity"</a>
<li><strong>Previous message:</strong> <a href="4102.html">Eliezer S. Yudkowsky: "Re: Fwd: Eclectic Pseudoplague"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4104.html">Eliezer S. Yudkowsky: "Re: How Kurzweil lost the Singularity"</a>
<li><strong>Reply:</strong> <a href="4104.html">Eliezer S. Yudkowsky: "Re: How Kurzweil lost the Singularity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4103">[ date ]</a>
<a href="index.html#4103">[ thread ]</a>
<a href="subject.html#4103">[ subject ]</a>
<a href="author.html#4103">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<p><strong>attached mail follows:</strong><hr><p>
Anand,
<br>
<p>Yes, I would like to respond to Eliezer.  You're welcome to post this
<br>
response on your mailing list:
<br>
<p>I think that Eliezer is misunderstanding my statements, intentions, and
<br>
efforts.  First of all, he states &quot;Kurzweil's entire *being* is directed
<br>
toward predicting the Singularity - *not* nudging the Singularity in any
<br>
direction.&quot;  The fact is that the bulk of my efforts are involved in
<br>
technology creation efforts, with only a portion devoted to talking and
<br>
writing about technology.  I do believe that as technologists, we have an
<br>
ethical responsibility to apply our efforts in ways that will promote
<br>
positive human values, albeit that we don't always have a consensus on what
<br>
those are.  Most of my efforts have been devoted to developing technology
<br>
for persons with disabilities, and towards enhancing human expression in
<br>
areas such as music, and I do give a high priority to considering the impact
<br>
that technologies I'm involved in creating will have on society.
<br>
<p>I am familiar with Eliezer's efforts at defining and articulating ways that
<br>
we can promote what he calls &quot;friendly AI,&quot; and I applaud his concern and
<br>
efforts in this direction.  By itself, I don't believe that such efforts are
<br>
sufficient, and Eliezer would probably agree with this.  I don't think that
<br>
we have enough knowledge today to define a reliable strategy to assuring
<br>
that AI (or other advanced technologies) will remain &quot;friendly,&quot; but the
<br>
dialogue on how to achieve this is certainly worthwhile and not premature.
<br>
It's an effort we will need to maintain and intensify, particularly as we
<br>
get closer.  I have said many times that these technologies are advancing on
<br>
many fronts, and I believe that a critical aspect of assuring that these
<br>
future technologies are helpful rather than harmful is that everyone
<br>
consider and apply ethical issues in every project and in every decision.
<br>
There's no one &quot;magic bullet&quot; strategy that is going to assure that we avoid
<br>
catastrophic downside scenarios. I do agree, however, that it is not too
<br>
early to define these downsides and to develop multiple strategies towards
<br>
this end.
<br>
<p>So in summary I believe that Eliezer's efforts in this direction are
<br>
important and worthwhile.  However, he is not correct that I am unconcerned
<br>
with this critical issue.  I've said on many ocassions that it's the number
<br>
one challenge facing our civilization in the 21st century.
<br>
<p>Ray Kurzweil
<br>
<p><p><p><p><p>-----Original Message-----
<br>
From: Anand AI [mailto:<a href="mailto:trans_humanism@msn.com?Subject=Re:%20Fwd:%20Re:%20How%20Kurzweil%20lost%20the%20Singularity">trans_humanism@msn.com</a>]
<br>
Sent: Sunday, June 16, 2002 4:53 AM
<br>
To: <a href="mailto:ray@kurzweilai.net?Subject=Re:%20Fwd:%20Re:%20How%20Kurzweil%20lost%20the%20Singularity">ray@kurzweilai.net</a>
<br>
Cc: <a href="mailto:sentience@pobox.com?Subject=Re:%20Fwd:%20Re:%20How%20Kurzweil%20lost%20the%20Singularity">sentience@pobox.com</a>
<br>
Subject: Fwd: How Kurzweil lost the Singularity
<br>
<p><p>Ray,
<br>
<p>I thought you may be interested in reading, and possibly replying, to this
<br>
SL4 (mailing list) message by Yudkowsky. The SL4 replies to this message can
<br>
be found at <a href="http://sysopmind.com/archive-sl4/current/">http://sysopmind.com/archive-sl4/current/</a>, under the header of
<br>
this email's subject.
<br>
<p>Permission has been received to forward this message. In doing so, I do not
<br>
necessarily agree with the opinions expressed within it.
<br>
<p>Best wishes,
<br>
<p>Anand
<br>
<p>----- Original Message -----
<br>
From: &quot;Eliezer S. Yudkowsky&quot; &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20Fwd:%20Re:%20How%20Kurzweil%20lost%20the%20Singularity">sentience@pobox.com</a>&gt;
<br>
To: &quot;SL4&quot; &lt;<a href="mailto:sl4@sysopmind.com?Subject=Re:%20Fwd:%20Re:%20How%20Kurzweil%20lost%20the%20Singularity">sl4@sysopmind.com</a>&gt;
<br>
Sent: Saturday, June 15, 2002 11:39 AM
<br>
Subject: How Kurzweil lost the Singularity
<br>
<p>Eliezer Yudkowsky wrote:
<br>
<em>&gt;
</em><br>
<em>&gt;Ben Goertzel wrote:
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt; Kurzweil, however, IS putting effort into helping people understand the
</em><br>
<em>&gt;  &gt; Singularity.
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt; And I'm sure that part of his motivation for doing this, is a desire to
</em><br>
<em>&gt;  &gt; nudge the Singularity in a better direction.  A direction not too
</em><br>
<em>&gt;  &gt; thoroughly polluted by peoples' fear and uncomprehension.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;Ben, to the best of my ability to understand it, Kurzweil's entire *being*
</em><br>
<em>&gt;is directed toward predicting the Singularity - *not* nudging the
</em><br>
<em>&gt;Singularity in any direction.  On every occasion in which I have spoken to
</em><br>
<em>&gt;Kurzweil, the concept of influencing the Singularity in any way is met
</em><br>
<em>&gt;with blank incomprehension.  As far as Kurzweil is concerned, he wins
</em><br>
<em>&gt;the argument when he convinces the audience that the Singularity will
</em><br>
<em>&gt;happen.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Any conceptual model of the Singularity that allows for individual actions
</em><br>
<em>&gt;to accelerate or influence the Singularity is seen by Kurzweil as a
</em><br>
<em>&gt;weakness in the argument, because it appears to argue that &quot;the
</em><br>
<em>&gt;Singularity requires individuals to do such-and-such.&quot;  Kurzweil will
</em><br>
<em>&gt;always argue for the creation of AI based on neuroanatomical modeling
</em><br>
<em>&gt;of all cortical areas, and will never admit that a general understanding
</em><br>
<em>&gt;of intelligence is necessary or even that it could speed up the process,
</em><br>
<em>&gt;because in the current scientific environment it is easier for Kurzweil to
</em><br>
<em>&gt;defend the proposition that neurocomputational modeling is possible than
</em><br>
<em>&gt;it is for Kurzweil to defend the proposition that an understanding of
</em><br>
<em>&gt;intelligence is possible.  As for the idea that &quot;We can do this using
</em><br>
<em>&gt;neurocomputational modeling, and therefore the Singularity is provably
</em><br>
<em>&gt;possible, but an understanding of intelligence may allow us to build AI
</em><br>
<em>&gt;earlier without reverse-engineering the brain&quot; - why, that's too complex
</em><br>
<em>&gt;for Kurzweil to explain on television. So it doesn't get said.  It doesn't
</em><br>
<em>&gt;get defended.  Ever.  It's easier for Kurzweil to present a model of the
</em><br>
<em>&gt;Singularity in which *only* reverse-engineering plays a role, and so his
</em><br>
<em>&gt;thoughts appear to have conformed to the worldview that will let him
</em><br>
<em>&gt;win arguments in the current memetic environment.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Kurzweil has, deliberately or inadvertantly, accepted constraints upon his
</em><br>
<em>&gt;thinking which prohibit his model from corresponding to reality, and which
</em><br>
<em>&gt;prohibit him from accepting any role for individual action in the
</em><br>
<em>&gt;Singularity.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Kurzweil believes in the inevitability of his curves, not in activism.
</em><br>
<em>&gt;Kurzweil wants to believe in the benevolence and inevitability of the
</em><br>
<em>&gt;Singularity and any argument of the form &quot;You can do X and it will improve
</em><br>
<em>&gt;your chances of (a Singularity) / (a positive Singularity)&quot; appears to him
</em><br>
<em>&gt;to be a vulnerability in his argument:  &quot;The Singularity *could* (go
</em><br>
<em>&gt;wrong) / (not happen) if not-X.&quot;  Kurzweil will therefore argue against
</em><br>
<em>&gt;it.  Kurzweil's entire worldview prohibits the possibility of Singularity
</em><br>
<em>&gt;activism.
</em><br>
<em>&gt;
</em><br>
<em>&gt;In fact, having watched Kurzweil debate Vinge, I've come to the conclusion
</em><br>
<em>&gt;that Kurzweil's worldview prohibits Kurzweil from arriving at any real
</em><br>
<em>&gt;understanding of the basic nature of the Singularity.  Over the course of
</em><br>
<em>&gt;my personal interaction with Kurzweil, I've seen him say two really
</em><br>
<em>&gt;bizarre things.  One was during the recent chat with Vinge, when Kurzweil
</em><br>
<em>&gt;predicted superhuman AI intelligence in 2029, followed shortly thereafter
</em><br>
<em>&gt;by the statement that the Singularity &quot;would not begin to tear the fabric
</em><br>
<em>&gt;of human understanding until 2040&quot;.  The second really bizarre thing I've
</em><br>
<em>&gt;heard Kurzweil say was at his SIG at the recent Foresight Gathering, when
</em><br>
<em>&gt;I asked why AIs thinking at million-to-one speeds wouldn't speed up the
</em><br>
<em>&gt;development of technology, and he said &quot;Well, that's another reason to
</em><br>
<em>&gt;expect Moore's Law to remain on course.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt;These statements are so absolutely bizarre that, after pondering what
</em><br>
<em>&gt;Kurzweil could have been thinking, I've come to the conclusion that what
</em><br>
<em>&gt;Kurzweil calls the &quot;Singularity&quot; is what we would call &quot;the ordinary
</em><br>
<em>&gt;progress of technology.&quot;  In Kurzweil's world, the Grinding Gears of
</em><br>
<em>&gt;Industry churn out AI, superhuman AI, uploading, brain-computer interfaces
</em><br>
<em>&gt;and so on, but these developments do not affect the nature of
</em><br>
<em>&gt;technological progress except insofar as they help to maintain Kurzweil's
</em><br>
<em>&gt;curves *exactly on track*.  What we, and Vinge, call the &quot;Singularity&quot; are
</em><br>
<em>&gt;the events that grow out of transhuman intelligence however and wherever
</em><br>
<em>&gt;it arises; industry is of interest to us only insofar as it leads up to
</em><br>
<em>&gt;that point.  What Kurzweil calls the &quot;Singularity&quot; is the inevitable,
</em><br>
<em>&gt;inexorable, and entirely ordinary progress of technology, which, in
</em><br>
<em>&gt;Kurzweil's world, *causes* developments such as transhumanity, but is not
</em><br>
<em>&gt;*changed* by transhumanity except in the same ways that industry has been
</em><br>
<em>&gt;changed by previous technological developments.
</em><br>
<em>&gt;
</em><br>
<em>&gt;What Kurzweil is selling, under the brand name of the &quot;Singularity&quot;, is
</em><br>
<em>&gt;the idea that technological progress will continue to go on exactly as it
</em><br>
<em>&gt;has done over the last century, and that the inexorable grinding of the
</em><br>
<em>&gt;gears of industry will eventually churn out luxuries such as
</em><br>
<em>&gt;superintelligent AIs, brain-computer interfaces, inloading, uploading,
</em><br>
<em>&gt;transhuman servants, and so on.  The gears of industry will then continue
</em><br>
<em>&gt;grinding at exactly the same pace, producing more and better
</em><br>
<em>&gt;superintelligent AIs, more and better transhumans, and so on.  Kurzweil's
</em><br>
<em>&gt;timeline for Moore's Law continues unblinkingly from &quot;Human-equivalent
</em><br>
<em>&gt;brainpower costs $1000&quot; to &quot;1000 brainpower costs $1000&quot; a decade
</em><br>
<em>&gt;later.  Kurzweil is not defending what we would regard as the Singularity;
</em><br>
<em>&gt;he is defending the idea of ordinary technological progress.  As part of
</em><br>
<em>&gt;defending the inevitability and desirability of the Turning Gears of
</em><br>
<em>&gt;Industry, Kurzweil also defends the idea that the Gears of Industry will
</em><br>
<em>&gt;churn out transhuman technologies, and the idea that the transhuman
</em><br>
<em>&gt;technologies churned out by the Gears of Industry are safe, desirable
</em><br>
<em>&gt;luxuries.  It so happens that one of the branches of Kurzweil's
</em><br>
<em>&gt;worldview - the production of transhuman intelligence - is known to us
</em><br>
<em>&gt;as the &quot;Singularity&quot;.  But Kurzweil's worldview does not contain any of
</em><br>
<em>&gt;our beliefs about the consequences and nature of transhuman intelligence.
</em><br>
<em>&gt;
</em><br>
<em>&gt;On the whole, Kurzweil's actions are probably a net benefit to the
</em><br>
<em>&gt;Singularity.  Kurzweil is promoting a safe, sanitized, comparatively
</em><br>
<em>&gt;unalarming, optimized-for-defensibility meme, under the brand name of
</em><br>
<em>&gt;&quot;Singularity&quot;, which bears a surface resemblance to the real concept of
</em><br>
<em>&gt;the Singularity as created by Vernor Vinge and preserved here.  People
</em><br>
<em>&gt;who become interested in Kurzweil's pseudo-Singularity may go on to
</em><br>
<em>&gt;google on &quot;Singularity&quot; and subsequently end up at the Singularity
</em><br>
<em>&gt;Institute.  People who learn to love transhumanity as a consequence of
</em><br>
<em>&gt;the Inexorable Gears of Industry may choose to take on transhumanity as
</em><br>
<em>&gt;a personal goal.
</em><br>
<em>&gt;
</em><br>
<em>&gt;But:
</em><br>
<em>&gt;
</em><br>
<em>&gt;1)  Kurzweil's positive effects on the Singularity are an accident.
</em><br>
<em>&gt;Unless he is being deliberately dishonest, the positive consequences of
</em><br>
<em>&gt;his actions are unintended consequences.
</em><br>
<em>&gt;
</em><br>
<em>&gt;2)  Despite his much greater potential to make a difference, it currently
</em><br>
<em>&gt;seems that Kurzweil will go on playing the role of a celebrity
</em><br>
<em>&gt;spokesperson, nothing more.  His outlook prohibits him from seeing the
</em><br>
<em>&gt;possibility of influencing the Singularity in any way.
</em><br>
<em>&gt;
</em><br>
<em>&gt;3)  Kurzweil's model is wrong enough that I cannot ethically help spread
</em><br>
<em>&gt;it.  Kurzweil is providing a safe, sanitized, easily digestible view of
</em><br>
<em>&gt;something that is *not* ordinary.  He is not being dishonest, but it would
</em><br>
<em>&gt;be dishonest for *me* to help spread ideas that I know to be attractive
</em><br>
<em>&gt;but untrue.
</em><br>
<em>&gt;
</em><br>
<em>&gt;At present Kurzweil is neither using his resources to accelerate the
</em><br>
<em>&gt;Singularity (in his capacity as an entrepreneur), nor even urging others
</em><br>
<em>&gt;to do so (in his capacity as an author).  I therefore question whether we
</em><br>
<em>&gt;should be lined up around the block to congratulate Kurzweil on his
</em><br>
<em>&gt;altruism, until he either (a) calls in his next book for college students
</em><br>
<em>&gt;to enter Singularity-related professions or (b) throws a few bucks the way
</em><br>
<em>&gt;of neurocomputational modeling research.  Right now Kurzweil appears to
</em><br>
<em>&gt;be a man with an idea that he believes is true.  So he writes books about
</em><br>
<em>&gt;it, speaks publicly about it, uses his celebrity status to promote it, and
</em><br>
<em>&gt;in turn gains greater prestige and celebrity status as the idea comes to
</em><br>
<em>&gt;be associated with him.  In this, Kurzweil is no different from anyone
</em><br>
<em>&gt;else with an idea.  This does not make Kurzweil a bad person, but it
</em><br>
<em>&gt;doesn't make him Gandhi either.  And it does not mean that Kurzweil is
</em><br>
<em>&gt;out to accelerate or improve the Singularity, either directly or
</em><br>
<em>&gt;indirectly.
</em><br>
<em>&gt;
</em><br>
<em>&gt;We are people with a cause, and our cause bears a vague resemblance to
</em><br>
<em>&gt;Kurzweil's idea, but we would be in error to try and see Kurzweil as a man
</em><br>
<em>&gt;with a cause.  Currently, Kurzweil is a man with an idea.  I wish I knew
</em><br>
<em>&gt;how to nudge people with ideas into becoming people with causes.
</em><br>
<em>&gt;
</em><br>
<em>&gt;--
</em><br>
<em>&gt;Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
</em><br>
<em>&gt;Research Fellow, Singularity Institute for Artificial Intelligence
</em><br>
<p><p><p>_________________________________________________________________
<br>
Join the world’s largest e-mail service with MSN Hotmail.
<br>
<a href="http://www.hotmail.com">http://www.hotmail.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4104.html">Eliezer S. Yudkowsky: "Re: How Kurzweil lost the Singularity"</a>
<li><strong>Previous message:</strong> <a href="4102.html">Eliezer S. Yudkowsky: "Re: Fwd: Eclectic Pseudoplague"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4104.html">Eliezer S. Yudkowsky: "Re: How Kurzweil lost the Singularity"</a>
<li><strong>Reply:</strong> <a href="4104.html">Eliezer S. Yudkowsky: "Re: How Kurzweil lost the Singularity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4103">[ date ]</a>
<a href="index.html#4103">[ thread ]</a>
<a href="subject.html#4103">[ subject ]</a>
<a href="author.html#4103">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
