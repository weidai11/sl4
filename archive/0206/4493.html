<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Military Friendly AI</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Military Friendly AI">
<meta name="Date" content="2002-06-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Military Friendly AI</h1>
<!-- received="Fri Jun 28 07:14:00 2002" -->
<!-- isoreceived="20020628131400" -->
<!-- sent="Thu, 27 Jun 2002 21:50:59 -0600" -->
<!-- isosent="20020628035059" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Military Friendly AI" -->
<!-- id="LAEGJLOGJIOELPNIOOAJEEAKCMAA.ben@goertzel.org" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="3D1BD1A4.E55D5B16@posthuman.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Military%20Friendly%20AI"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Thu Jun 27 2002 - 21:50:59 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4494.html">Samantha Atkins: "Re: Ben vs. Ben"</a>
<li><strong>Previous message:</strong> <a href="4492.html">Brian Atkins: "Ben vs. Ben"</a>
<li><strong>In reply to:</strong> <a href="4490.html">Brian Atkins: "Re: Military Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4496.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4496.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4501.html">James Higgins: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4560.html">Brian Atkins: "Re: Military Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4493">[ date ]</a>
<a href="index.html#4493">[ thread ]</a>
<a href="subject.html#4493">[ subject ]</a>
<a href="author.html#4493">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi,
<br>
<p><em>&gt; I'll have to disagree, and say that many of the ideas in CFAI are AFAIK
</em><br>
<em>&gt; novel. Give him a little credit for inventing more than just the term. And
</em><br>
<em>&gt; as for all the thinking you and others did, I don't see that it produced
</em><br>
<em>&gt; much in the way of results during the period before CFAI was published.
</em><br>
<em>&gt; Where's all the prior art, and why aren't we debating that stuff here, and
</em><br>
<em>&gt; comparing it to CFAI to see which is better?
</em><br>
<p>The &quot;prior art&quot; (if you can apply that term to speculative philosophy; it's
<br>
normally used for concrete technology) is scattered all over the place, in
<br>
writings that were focused mainly on other things and mentioned Friendly AI
<br>
topics only peripherally.  Digging it all up would be a major task.
<br>
Personally, I can recall some books where such things were discussed, but
<br>
not the page numbers etc.
<br>
<p>One pretty old reference where such ideas were discussed was Val Turchin's
<br>
book The Phenomenon of Science, published in Russia in the late 60's, and in
<br>
the US in the early 70's.  He coined the nice term &quot;human plankton&quot; to refer
<br>
to how insignificant we would appear from the future perspective of
<br>
superintelligent machines with synergetic group minds.   His philosophy of
<br>
what we call the Singularity centered around the notion of the &quot;Metasystem
<br>
transition&quot; wherein what used to be autonomous become components in a
<br>
higher-level system, and the nexus of control passes to a higher level...
<br>
<p><p><em>&gt; &gt;I think we will be able to develop a real
</em><br>
<em>&gt; &gt; theory of Friendly AI only after some experience playing around with
</em><br>
<em>&gt; &gt; infrahuman AGI's that have a lot more general intelligence than
</em><br>
<em>&gt; any program
</em><br>
<em>&gt; &gt; now existing.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Which tends to strike me as a dangerous approach.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I believe my attitude toward Friendliness is typical of AGI researchers.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Unfortunately, yes not many people seem to as careful as we all would like
</em><br>
<em>&gt; when playing around with existential technologies.
</em><br>
<p>There are dangers involved of course.
<br>
<p>However, it may be even MORE dangerous to fool oneself into believing one
<br>
has adequately grappled with the Friendliness issue prior to creating an
<br>
infrahuman AGI.
<br>
<p>Perhaps it is less dangerous to be honest with oneself about the fact that,
<br>
prior to having some serious infrahuman AGI's to experiment with, it's just
<br>
possible to make a serious theory of Friendly AI...
<br>
<p><p><em>&gt; So can your position be summarized as: we'll build our AI, get it
</em><br>
<em>&gt; working at
</em><br>
<em>&gt; some subhuman level, and then when we guess it needs it we'll stop running
</em><br>
<em>&gt; it for a while until we figure out how to ensure &quot;Friendliness&quot;? I think
</em><br>
<em>&gt; your protocol needs to be fleshed out for us further so we can feel more
</em><br>
<em>&gt; comfortable with your plans.
</em><br>
<p>Whether it will be necessary to stop running it for a while, or not, will
<br>
depend on the situation.
<br>
<p>If the intelligence in the system is growing very fast, then yes, this will
<br>
be necessary.  If the intelligence is growing slowly, then to stop it may
<br>
not be necessary.
<br>
<p>I should add that in Novamente we *do* have a Friendly goal system, it's
<br>
just a different kind than Eli's.  Novamente has a number of goals, a goal
<br>
heterarchy rather than a goal hierarchy.  Friendliness is an important one
<br>
of its goals, but it's not wired in as the supergoal.  Then Friendly
<br>
behavior is intended to be taught to the system via interaction in a shared
<br>
perceptual environment, just as CFAI suggests.
<br>
<p>So, the big difference between Novamente's Friendly goal system and the one
<br>
Eliezer proposes, is that he suggests a hierarchical goal system with
<br>
Friendliness at the top is best, whereas I suggest a heterarchical goal
<br>
system with Friendliness as one element of the heterarchy, is the only thing
<br>
compatible with the natural dynamics of intelligence.
<br>
<p>However, I am not *sure* this is right, and I'd like to experiment with both
<br>
heterarchical and hierarchical goal systems at an appropriate point in time.
<br>
<p>It is possible to implement an Eliezer-style Friendly goal hierarchy with
<br>
Friendliness at the top, inside Novamente.  My conjecture is that this goal
<br>
system will be less stable over time than a heterarchical goal system with
<br>
Friendliness as one element among many.  However, this conjecture could be
<br>
wrong -- I don't have a good enough theory of Novamente dynamics to tell
<br>
right now.  This is the sort of question I think has to be discovered
<br>
through experiment, not through proud theoretical declamations....
<br>
<p><p><em>&gt; It sounds dangerous to me (and I guess others here) to build the AI first,
</em><br>
<em>&gt; and let it run for some time without any special F features built in. How
</em><br>
<em>&gt; will your protocol ensure that it does not take off, and if it does how
</em><br>
<em>&gt; are we ensured it will turn out ok?
</em><br>
<p>Here's the thing... as clarified in the previous paragraphs I just typed, we
<br>
*do* have a Friendliness goal built in, we're just sure yet what the best
<br>
way is to do this.  And we're not willing to fool ourselves that we *are*
<br>
sure what the best way is....
<br>
<p>Compared to e.g. Peter Voss's A2I2 system, our approach is far closer to
<br>
Eli's, because Peter's system is neural-nettish and is not the sort of
<br>
system that one *can* explicitly supply with a Friendliness goal.  But Peter
<br>
holds far more strongly than me to the opinion that it's &quot;too early&quot; to
<br>
seriously consider the Friendliness issue.  He's just less argumentative
<br>
than me so he's keeping relatively quiet about this view on this list,
<br>
although he's a member ;_)
<br>
<p>As you and Eli and I discussed in a private e-mail, we do plan to put a
<br>
&quot;failsafe&quot; mechanism into Novamente to halt a potential unsupervised hard
<br>
takeoff -- eventually, when we consider there to be a significantly &gt; 0 risk
<br>
of this happening.
<br>
<p>The task of detecting a rapid intelligence increase within Novamente is not
<br>
an easy one, and creating and tuning useful measures of intelligence
<br>
increase is going to be a big research topic, which we will have to explore
<br>
through systematic experimentation *once we have a fully implemented system
<br>
with a moderate degree of intelligence*.  In the Novamente manuscript (the
<br>
new version) there is a whole section on intelligence measurement, and it's
<br>
not an easy topic.  Measuring the intelligence of an AGI effectively
<br>
requires a measurement system of some intelligence in itself, because
<br>
intelligence is a fairly  multidimensional concept when you really delve
<br>
into it.
<br>
<p>So, I think we don't even know how to build a good failsafe mechanism for
<br>
Novamente or any other AI yet.  We will only know that when we know how to
<br>
measure the intelligence of an AGI effectively, and we will only know *this*
<br>
based on experimentation with AGI's smarter than the ones we have now.
<br>
<p>yeah, you can create a standard test of puzzles for the system to solve, but
<br>
I have little faith that these things measure intelligence well.  There are
<br>
so many different kinds of intelligence... and then how do you measure how
<br>
hard the system is actually trying to solve the puzzle, as opposed to
<br>
thinking about other things of interest to it?  These are not impossible
<br>
problems, but they're not trivial, and it's hard to solve them definitively
<br>
in the absence of a working AGI.
<br>
<p><p><em>&gt;
</em><br>
<em>&gt; So at exactly what stages of the development do you plan to
</em><br>
<em>&gt; implement which
</em><br>
<em>&gt; F features? You have some sort of protocol, right?
</em><br>
<p>Once the entire system is implemented, and we have begun to teach it by
<br>
chatting with it through our Experiential Interactive Learning interface,
<br>
and exposing it to external data in a &quot;free access&quot; manner rather than
<br>
simply by feeding it test datasets.
<br>
<p><p><em>&gt; If your design is incapable of supporting such features, and you have been
</em><br>
<em>&gt; unable to come up with your own seemingly impregnable way to keep your AI
</em><br>
<em>&gt; &quot;Friendly&quot; throughout its development into superintelligence, then maybe
</em><br>
<em>&gt; we should be getting worried?
</em><br>
<p><p>Actually, Novamente is capable of supporting a CFAI style friendly goal
<br>
system *initially*, I just doubt that such a goal system will be stable thru
<br>
long-term Novamente dynamics....  I think that a heterarchical friendly goal
<br>
system will be more stable within novamente... but this is just conjecture
<br>
right now...
<br>
<p><em>&gt;
</em><br>
<em>&gt; I assume that if you get your working infrahuman AI, and are unable to
</em><br>
<em>&gt; come up with a bulletproof way of keeping it &quot;Friendly&quot;, you will turn it
</em><br>
<em>&gt; off?
</em><br>
<p>Not necessarily, this will be a hard decision if it comes to that.
<br>
<p>It may be that what we learn is that there is NO bulletproof way to make an
<br>
AGI Friendly... just like there is no bulletproof way to make a human
<br>
Friendly....  It is possible that the wisest course is to go ahead and let
<br>
an AGI evolve even though one knows one is not 100% guaranteed of
<br>
Friendliness.  This would be a tough decision to come to, but not an
<br>
impossible one, in my view.
<br>
<p><em>&gt; Well do you think it's worth our trouble to read it? If so I'd like to see
</em><br>
<em>&gt; some discussion about it (perhaps Eliezer will allow you to
</em><br>
<em>&gt; repost the flaws
</em><br>
<em>&gt; he saw in it) since I don't recall any threads regarding it (if I've
</em><br>
<em>&gt; forgotten, someone please give me a URL to the archives, thanks).
</em><br>
<p>I think it's worth your while to read it, sure.  And there was a brief
<br>
thread on it a while back.
<br>
<p><em>&gt; I just started reading your AI Morality paper, I'm sure I'll have more
</em><br>
<em>&gt; comments later, but this part is a bit scary I guess to everyone here who
</em><br>
<em>&gt; is afraid of the initial AI programmers having too much control over the
</em><br>
<em>&gt; AI's final state:
</em><br>
<em>&gt;
</em><br>
<em>&gt;   &quot;But intuitively, I feel that an AGI with these values is going to be a
</em><br>
<em>&gt;    positive force in the universe – where by “positive” I mean
</em><br>
<em>&gt; “in accordance
</em><br>
<em>&gt;    with Ben Goertzel’s value system”.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; <a href="http://www.goertzel.org/dynapsyc/2002/AIMorality.htm">http://www.goertzel.org/dynapsyc/2002/AIMorality.htm</a>
</em><br>
<p><p>There is no escaping the subjectivity of  morality, Brian.
<br>
<p>The reason I put that phrase in there is, I know that to *some* people,
<br>
anything that may lead to the obsolescence of humanity is intrinsically
<br>
negative.  (Because they believe, e.g., that humans are God's chosen
<br>
creatures... that uploads will not have souls... etc.).  To these people
<br>
even a Friendly AGI would be a negative force in the universe.
<br>
<p>Eliezer's approach to Friendliness relies on his own personal morals as
<br>
well.  his are pretty similar to mine; for instance, he thinks that
<br>
preserving lives forever is a good thing.  My wife, who believes in
<br>
reincarnation, disagrees with me and Eli on this -- according to her moral
<br>
standards, ending death goes against the natural cycle of karma and is thus
<br>
probably not a good thing....
<br>
<p>Perhaps I should refer to some standard &quot;transhumanist moral code&quot; rather
<br>
than my own personal code, but I didn't want to make up a moral code and
<br>
just define is at the standard &quot;transhumanist moral code.&quot;
<br>
<p>In fact though I strongly suspect that an AGI with the values I suggest in
<br>
that essay, would be a positive force according to the moral standards of
<br>
nearly all transhumanists...
<br>
<p>-- Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4494.html">Samantha Atkins: "Re: Ben vs. Ben"</a>
<li><strong>Previous message:</strong> <a href="4492.html">Brian Atkins: "Ben vs. Ben"</a>
<li><strong>In reply to:</strong> <a href="4490.html">Brian Atkins: "Re: Military Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4496.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4496.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4501.html">James Higgins: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4560.html">Brian Atkins: "Re: Military Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4493">[ date ]</a>
<a href="index.html#4493">[ thread ]</a>
<a href="subject.html#4493">[ subject ]</a>
<a href="author.html#4493">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
