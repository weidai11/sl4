<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Military Friendly AI</title>
<meta name="Author" content="James Higgins (jameshiggins@earthlink.net)">
<meta name="Subject" content="RE: Military Friendly AI">
<meta name="Date" content="2002-06-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Military Friendly AI</h1>
<!-- received="Fri Jun 28 14:37:33 2002" -->
<!-- isoreceived="20020628203733" -->
<!-- sent="Fri, 28 Jun 2002 11:33:45 -0700" -->
<!-- isosent="20020628183345" -->
<!-- name="James Higgins" -->
<!-- email="jameshiggins@earthlink.net" -->
<!-- subject="RE: Military Friendly AI" -->
<!-- id="4.3.2.7.2.20020628110024.01cbe720@mail.earthlink.net" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJEEAKCMAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> James Higgins (<a href="mailto:jameshiggins@earthlink.net?Subject=RE:%20Military%20Friendly%20AI"><em>jameshiggins@earthlink.net</em></a>)<br>
<strong>Date:</strong> Fri Jun 28 2002 - 12:33:45 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4502.html">James Higgins: "RE: Ben vs. Ben"</a>
<li><strong>Previous message:</strong> <a href="4500.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>In reply to:</strong> <a href="4493.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4522.html">Eliezer S. Yudkowsky: "FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4522.html">Eliezer S. Yudkowsky: "FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4525.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4501">[ date ]</a>
<a href="index.html#4501">[ thread ]</a>
<a href="subject.html#4501">[ subject ]</a>
<a href="author.html#4501">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
At 09:50 PM 6/27/2002 -0600, Ben Goertzel wrote:
<br>
<em>&gt; &gt; &gt;I think we will be able to develop a real
</em><br>
<em>&gt; &gt; &gt; theory of Friendly AI only after some experience playing around with
</em><br>
<em>&gt; &gt; &gt; infrahuman AGI's that have a lot more general intelligence than
</em><br>
<em>&gt; &gt; any program
</em><br>
<em>&gt; &gt; &gt; now existing.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Which tends to strike me as a dangerous approach.
</em><br>
<p>I can see Ben's point, however.  Eliezer's Friendliness designs do have a 
<br>
reasonable potential be useless.  The basic concepts of Friendliness are 
<br>
good and useful, but detailed designs, does seem a bit 
<br>
pointless.  Espicially if there were only one or very few detailed 
<br>
designs.  Systems designed to implement an AI are almost certainly all 
<br>
going to be significantly different.  Because, collectively as a race, we 
<br>
have little idea of what we're doing yet.  So everyone is going in 
<br>
different directions hoping they find the new world.  Once someone spots it 
<br>
we'll have a much better idea of how such things actually work.
<br>
<p>Note that I am not, in any way, saying that work on Friendliness is useless 
<br>
nor is the existence of the Singularity Institute irrelevant.  On a 
<br>
theoretical basis such work may prove helpful and should by all means be 
<br>
continued.  It will help guide the Friendliness system(s) designed once we 
<br>
have some idea how they will work (and in what kind of system).
<br>
<p><em>&gt; &gt; So can your position be summarized as: we'll build our AI, get it
</em><br>
<em>&gt; &gt; working at
</em><br>
<em>&gt; &gt; some subhuman level, and then when we guess it needs it we'll stop running
</em><br>
<em>&gt; &gt; it for a while until we figure out how to ensure &quot;Friendliness&quot;? I think
</em><br>
<em>&gt; &gt; your protocol needs to be fleshed out for us further so we can feel more
</em><br>
<em>&gt; &gt; comfortable with your plans.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Whether it will be necessary to stop running it for a while, or not, will
</em><br>
<em>&gt;depend on the situation.
</em><br>
<em>&gt;
</em><br>
<em>&gt;If the intelligence in the system is growing very fast, then yes, this will
</em><br>
<em>&gt;be necessary.  If the intelligence is growing slowly, then to stop it may
</em><br>
<em>&gt;not be necessary.
</em><br>
<p>I very much hope that you would make certain the fail safe was implemented 
<br>
by this time (discussed by Ben below).
<br>
<p><em>&gt; &gt; It sounds dangerous to me (and I guess others here) to build the AI first,
</em><br>
<em>&gt; &gt; and let it run for some time without any special F features built in. How
</em><br>
<em>&gt; &gt; will your protocol ensure that it does not take off, and if it does how
</em><br>
<em>&gt; &gt; are we ensured it will turn out ok?
</em><br>
<em>&gt;
</em><br>
<em>&gt;Here's the thing... as clarified in the previous paragraphs I just typed, we
</em><br>
<em>&gt;*do* have a Friendliness goal built in, we're just sure yet what the best
</em><br>
<em>&gt;way is to do this.  And we're not willing to fool ourselves that we *are*
</em><br>
<em>&gt;sure what the best way is....
</em><br>
<p>This sounds like a realistic view of the problem.  The system does have 
<br>
some basic friendliness implemented.  But it isn't clear how best to 
<br>
implement the whole Friendliness systems and, thus, would be a waste of 
<br>
time at this point (since at best it is very likely be useless and, at 
<br>
worst, could bog down or throw off the entire system).
<br>
<p>The Friendliness goals are already in place, then?
<br>
<p><em>&gt;Compared to e.g. Peter Voss's A2I2 system, our approach is far closer to
</em><br>
<em>&gt;Eli's, because Peter's system is neural-nettish and is not the sort of
</em><br>
<em>&gt;system that one *can* explicitly supply with a Friendliness goal.  But Peter
</em><br>
<em>&gt;holds far more strongly than me to the opinion that it's &quot;too early&quot; to
</em><br>
<em>&gt;seriously consider the Friendliness issue.  He's just less argumentative
</em><br>
<em>&gt;than me so he's keeping relatively quiet about this view on this list,
</em><br>
<em>&gt;although he's a member ;_)
</em><br>
<em>&gt;
</em><br>
<em>&gt;As you and Eli and I discussed in a private e-mail, we do plan to put a
</em><br>
<em>&gt;&quot;failsafe&quot; mechanism into Novamente to halt a potential unsupervised hard
</em><br>
<em>&gt;takeoff -- eventually, when we consider there to be a significantly &gt; 0 risk
</em><br>
<em>&gt;of this happening.
</em><br>
<p>Good.  I'd suggest you start with a simple fail safe soon and gradually 
<br>
improve / expand on it as the software base grows.  As we discussed (via 
<br>
private email) building frameworks as you go end up being much easier and 
<br>
less error prone if you start early.
<br>
<p><em>&gt;So, I think we don't even know how to build a good failsafe mechanism for
</em><br>
<em>&gt;Novamente or any other AI yet.  We will only know that when we know how to
</em><br>
<em>&gt;measure the intelligence of an AGI effectively, and we will only know *this*
</em><br>
<em>&gt;based on experimentation with AGI's smarter than the ones we have now.
</em><br>
<p>Well, getting a basic fail safe system in sooner should also help you learn 
<br>
how to do this better in the long-run.
<br>
<p><em>&gt; &gt; I assume that if you get your working infrahuman AI, and are unable to
</em><br>
<em>&gt; &gt; come up with a bulletproof way of keeping it &quot;Friendly&quot;, you will turn it
</em><br>
<em>&gt; &gt; off?
</em><br>
<em>&gt;
</em><br>
<em>&gt;Not necessarily, this will be a hard decision if it comes to that.
</em><br>
<em>&gt;
</em><br>
<em>&gt;It may be that what we learn is that there is NO bulletproof way to make an
</em><br>
<em>&gt;AGI Friendly... just like there is no bulletproof way to make a human
</em><br>
<em>&gt;Friendly....  It is possible that the wisest course is to go ahead and let
</em><br>
<em>&gt;an AGI evolve even though one knows one is not 100% guaranteed of
</em><br>
<em>&gt;Friendliness.  This would be a tough decision to come to, but not an
</em><br>
<em>&gt;impossible one, in my view.
</em><br>
<p>I think everyone who is realistic believes this is a possibility, no matter 
<br>
how much we hate that fact.  However, it must be made absolutely certain 
<br>
that this is the case.  And, if it is, some AI designs would still be less 
<br>
risky than others (I would think).  If anything even close to this looks 
<br>
likely you better be getting opinions of hundreds or thousands of relevant 
<br>
experts.  Or I'll come kick yer ass.  ;)  Seriously.
<br>
<p>What happens if you get Novamenta working as an AI, it is proven that 
<br>
Friendliness can not be guaranteed and it looks like your design is 
<br>
somewhat more risky that the ideal system.  Lets say your AI has a 4% 
<br>
chance (totally arbitrary, just for illustration) of turning out 
<br>
un-friendly if allowed to proceed.  And a group of responsible experts (not 
<br>
crack pots, not government appointed, not-self interested parties, etc) 
<br>
strongly believe a different design could lower the risk to 3%.  Lets say 
<br>
you'd have to scrap 70% of your code base and logic to implement the other 
<br>
design and it would take you several years to do this.
<br>
<p>What is the trade-off point between risk and time?
<br>
<p>What if another team was further ahead on this other design than yours?
<br>
<p><em>&gt; &gt; Well do you think it's worth our trouble to read it? If so I'd like to see
</em><br>
<em>&gt; &gt; some discussion about it (perhaps Eliezer will allow you to
</em><br>
<em>&gt; &gt; repost the flaws
</em><br>
<em>&gt; &gt; he saw in it) since I don't recall any threads regarding it (if I've
</em><br>
<em>&gt; &gt; forgotten, someone please give me a URL to the archives, thanks).
</em><br>
<em>&gt;
</em><br>
<em>&gt;I think it's worth your while to read it, sure.  And there was a brief
</em><br>
<em>&gt;thread on it a while back.
</em><br>
<p>I actually remember the thread (though, unfortunately, not so much of 
<br>
details), so I know its out there somewhere...
<br>
<p><em>&gt; &gt; I just started reading your AI Morality paper, I'm sure I'll have more
</em><br>
<em>&gt; &gt; comments later, but this part is a bit scary I guess to everyone here who
</em><br>
<em>&gt; &gt; is afraid of the initial AI programmers having too much control over the
</em><br>
<em>&gt; &gt; AI's final state:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;   &quot;But intuitively, I feel that an AGI with these values is going to be a
</em><br>
<em>&gt; &gt;    positive force in the universe ­ where by “positive” I mean
</em><br>
<em>&gt; &gt; “in accordance
</em><br>
<em>&gt; &gt;    with Ben Goertzel’s value system”.&quot;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; <a href="http://www.goertzel.org/dynapsyc/2002/AIMorality.htm">http://www.goertzel.org/dynapsyc/2002/AIMorality.htm</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt;There is no escaping the subjectivity of  morality, Brian.
</em><br>
<p>A true fact.  And part of the reason why I'd like to see more than one 
<br>
person be responsible for defining this morality.
<br>
<p><em>&gt;The reason I put that phrase in there is, I know that to *some* people,
</em><br>
<em>&gt;anything that may lead to the obsolescence of humanity is intrinsically
</em><br>
<em>&gt;negative.  (Because they believe, e.g., that humans are God's chosen
</em><br>
<em>&gt;creatures... that uploads will not have souls... etc.).  To these people
</em><br>
<em>&gt;even a Friendly AGI would be a negative force in the universe.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Eliezer's approach to Friendliness relies on his own personal morals as
</em><br>
<em>&gt;well.  his are pretty similar to mine; for instance, he thinks that
</em><br>
<em>&gt;preserving lives forever is a good thing.  My wife, who believes in
</em><br>
<em>&gt;reincarnation, disagrees with me and Eli on this -- according to her moral
</em><br>
<em>&gt;standards, ending death goes against the natural cycle of karma and is thus
</em><br>
<em>&gt;probably not a good thing....
</em><br>
<p>Another good reason why morality should not be decided by a single 
<br>
individual.  Eliezer or Ben's morality may not allow death, thus severely 
<br>
going against Ben's wife's morals.  Ben's wife's morals, however, would not 
<br>
prevent any deaths, and thus would go strongly against Eliezer's and Ben's 
<br>
(and mine).  So maybe preventing deaths except where the individual does 
<br>
not want this protection is the best answer.  But it takes more than one 
<br>
viewpoint to even see this questions.
<br>
<p>James Higgins
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4502.html">James Higgins: "RE: Ben vs. Ben"</a>
<li><strong>Previous message:</strong> <a href="4500.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>In reply to:</strong> <a href="4493.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4522.html">Eliezer S. Yudkowsky: "FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4522.html">Eliezer S. Yudkowsky: "FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4525.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4501">[ date ]</a>
<a href="index.html#4501">[ thread ]</a>
<a href="subject.html#4501">[ subject ]</a>
<a href="author.html#4501">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
