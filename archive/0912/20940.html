<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))</title>
<meta name="Author" content="M.>h (m.transhumanist@gmail.com)">
<meta name="Subject" content="Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))">
<meta name="Date" content="2009-12-11">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))</h1>
<!-- received="Fri Dec 11 00:12:04 2009" -->
<!-- isoreceived="20091211071204" -->
<!-- sent="Fri, 11 Dec 2009 08:11:46 +0100" -->
<!-- isosent="20091211071146" -->
<!-- name="M.>h" -->
<!-- email="m.transhumanist@gmail.com" -->
<!-- subject="Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))" -->
<!-- id="A8199DE5-11A1-46B7-A89C-7A3C4F17A628@gmail.com" -->
<!-- charset="UTF-8" -->
<!-- inreplyto="814549.21992.qm@web51908.mail.re2.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> M.>h (<a href="mailto:m.transhumanist@gmail.com?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))"><em>m.transhumanist@gmail.com</em></a>)<br>
<strong>Date:</strong> Fri Dec 11 2009 - 00:11:46 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="20941.html">Stathis Papaioannou: "Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<li><strong>Previous message:</strong> <a href="20939.html">Matt Mahoney: "Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<li><strong>In reply to:</strong> <a href="20932.html">Matt Mahoney: "Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20901.html">Stathis Papaioannou: "Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20940">[ date ]</a>
<a href="index.html#20940">[ thread ]</a>
<a href="subject.html#20940">[ subject ]</a>
<a href="author.html#20940">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Am 07.12.2009 um 21:09 schrieb Matt Mahoney &lt;<a href="mailto:matmahoney@yahoo.com?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">matmahoney@yahoo.com</a>&gt;:
<br>
<p><em>&gt; &quot;M.&gt;h&quot; wrote:
</em><br>
<em>&gt; &gt; ... sorry, but i do not get the whole problem.
</em><br>
<em>&gt;
</em><br>
<em>&gt; You understand that I can't define something which doesn't exist.  
</em><br>
<em>&gt; When I say qualia or self awareness or that little person inside  
</em><br>
<em>&gt; your mind that observes the world through your senses, most people  
</em><br>
<em>&gt; know what I mean.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Let me put it this way. Consider the AI program that observed  
</em><br>
<em>&gt; everything you did for the last several years until it became so  
</em><br>
<em>&gt; good at predicting your behavior that none of your friends or  
</em><br>
<em>&gt; relatives could distinguish it from you in a Turing test  
</em><br>
<em>&gt; environment. Unfortunately the building containing the only copy of  
</em><br>
<em>&gt; the program is on fire.  It is just you and the computer in a room  
</em><br>
<em>&gt; rapidly filling with smoke. There is just enough time either for you  
</em><br>
<em>&gt; to get out, allowing the only copy to be destroyed, or for you to  
</em><br>
<em>&gt; upload a copy of the program to a remote site over the internet with  
</em><br>
<em>&gt; your last dying breath. Which do you do?
</em><br>
<p>... from a rational point i would
<br>
a) think that copy and me are the same
<br>
b) assume that saving the copy is a better long-term strategy
<br>
<p>... however i would nonetheless feel strange because if dying i would  
<br>
(probably) recognise it as my *own* death, because i will (probably)  
<br>
not perceive what the copy does but what i do. a distructive upload  
<br>
would not have these divergences, or?
<br>
<p>cheers,
<br>
<p>miriam
<br>
<p><em>&gt;
</em><br>
<em>&gt; If you got out, then you must believe in this nonexistent thing.  
</em><br>
<em>&gt; Otherwise you would logically conclude that by preserving your  
</em><br>
<em>&gt; memories in a form that can be backed up, that you become immortal.  
</em><br>
<em>&gt; Furthermore, you have the opportunity to enhance your intelligence  
</em><br>
<em>&gt; and your environment by running on more powerful computers and  
</em><br>
<em>&gt; embodied in better robots in the future. Why would you ever allow  
</em><br>
<em>&gt; one copy to be destroyed now and the only other copy in a few decades?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Sorry for my ambiguous use of the word &quot;you&quot;.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; -- Matt Mahoney, <a href="mailto:matmahoney@yahoo.com?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">matmahoney@yahoo.com</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; From: &quot;M.&gt;h&quot; &lt;<a href="mailto:m.transhumanist@gmail.com?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">m.transhumanist@gmail.com</a>&gt;
</em><br>
<em>&gt; To: &quot;<a href="mailto:sl4@sl4.org?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">sl4@sl4.org</a>&quot; &lt;<a href="mailto:sl4@sl4.org?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">sl4@sl4.org</a>&gt;
</em><br>
<em>&gt; Sent: Mon, December 7, 2009 1:58:00 AM
</em><br>
<em>&gt; Subject: Re: Copying nonsense (was Re: [sl4] Uploading (was : goals  
</em><br>
<em>&gt; of AI))
</em><br>
<em>&gt;
</em><br>
<em>&gt; ... sorry, but i do not get the whole problem. even if a clone of me  
</em><br>
<em>&gt; would walk up to me right here and now, having sufficiently enough  
</em><br>
<em>&gt; of my memories and claiming to have my 'identity', i would not care  
</em><br>
<em>&gt; if this 'double' would not use my resources (e.g. credit card) and  
</em><br>
<em>&gt; bureaucrats would leave me alone!
</em><br>
<em>&gt;
</em><br>
<em>&gt; cheers,
</em><br>
<em>&gt;
</em><br>
<em>&gt; miriam
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Am 06.12.2009 um 22:12 schrieb Thomas Buckner &lt;<a href="mailto:tcbevolver@yahoo.com?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">tcbevolver@yahoo.com</a>&gt;:
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; From: Matt Mahoney &lt;<a href="mailto:matmahoney@yahoo.com?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">matmahoney@yahoo.com</a>&gt;
</em><br>
<em>&gt;&gt; To: <a href="mailto:sl4@sl4.org?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">sl4@sl4.org</a>
</em><br>
<em>&gt;&gt; Sent: Sun, December 6, 2009 2:44:53 PM
</em><br>
<em>&gt;&gt; Subject: Re: Copying nonsense (was Re: [sl4] Uploading (was : goals  
</em><br>
<em>&gt;&gt; of AI))
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Rewot Fetterkey wrote:
</em><br>
<em>&gt;&gt; &gt; Can you clarify that? How, exactly, is consciousness nonexistent?
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; By consciousness, I mean that which makes you different from a  
</em><br>
<em>&gt;&gt; philosophical zombie as described in <a href="http://en.wikipedia.org/wiki/Philosophical_zombie">http://en.wikipedia.org/wiki/Philosophical_zombie</a>
</em><br>
<em>&gt;&gt; But by definition, a zombie is not distinguishable from you at all.  
</em><br>
<em>&gt;&gt; I really don't know how much more clear the logic could be.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; The problem arises because all animals, including those that have  
</em><br>
<em>&gt;&gt; no concept of death, have evolved a fear of those things that can  
</em><br>
<em>&gt;&gt; kill them. Humans do have such a concept, which we associate with a  
</em><br>
<em>&gt;&gt; lack of conscious experience. So we all desperately want to  
</em><br>
<em>&gt;&gt; preserve this thing that does not exist. We can't help it. We are  
</em><br>
<em>&gt;&gt; programmed that way.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; One way to deal with this conflict is to argue that the zombie  
</em><br>
<em>&gt;&gt; argument is wrong and create ever more convoluted arguments to  
</em><br>
<em>&gt;&gt; refute it. My preferred approach is as follows:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; 1. I believe that I have conscious experience. (I am programmed to).
</em><br>
<em>&gt;&gt; 2. I know that conscious experience does not exist.. (Logic  
</em><br>
<em>&gt;&gt; irrefutably says so).
</em><br>
<em>&gt;&gt; 3. I realize that 1 and 2 are inconsistent. I leave it at that.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; -- Matt Mahoney, <a href="mailto:matmahoney@yahoo.com?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">matmahoney@yahoo.com</a>
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; I'm with Daniel Dennet on this: the P-zombie is (to paraphrase an  
</em><br>
<em>&gt;&gt; earlier poster)  2+2 = 5. Purely hypothetical, a character in a  
</em><br>
<em>&gt;&gt; gendankenexperiment, The Man Who Wasn't There. In practice, any  
</em><br>
<em>&gt;&gt; creature with a human brain that could say &quot;Ouch, that hurt&quot; has an  
</em><br>
<em>&gt;&gt; internal process isomorphic to what we experience as consciousness.  
</em><br>
<em>&gt;&gt; Please see my post of a few hours ago on the Edge talk.  <a href="http://www.edge.org/3rd_culture/dehaene09/dehaene09_index.html">http://www.edge.org/3rd_culture/dehaene09/dehaene09_index.html</a>
</em><br>
<em>&gt;&gt; Consciousness in the human brain is a global pattern of activation  
</em><br>
<em>&gt;&gt; and we now have methods of scanning and can say whether that  
</em><br>
<em>&gt;&gt; pattern appears or not. This scanning has been applied to comatose/ 
</em><br>
<em>&gt;&gt; vegetative patients. From Dr. Dehaene's talk:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; &quot;Let me just give you a very basic idea about the test. We  
</em><br>
<em>&gt;&gt; stimulate the patient with five tones. The first four tones are  
</em><br>
<em>&gt;&gt; identical, but the fifth can be different. So you hear something  
</em><br>
<em>&gt;&gt; like dit-dit-dit-dit-tat. When you do this, a very banal  
</em><br>
<em>&gt;&gt; observation, dating back 25 years, is that the brain reacts to the  
</em><br>
<em>&gt;&gt; different tone at the end. That reaction, which is called mismatch  
</em><br>
<em>&gt;&gt; negativity, is completely automatic. You get it even in coma, in  
</em><br>
<em>&gt;&gt; sleep, or when you do not attend to the stimulus. It's a non- 
</em><br>
<em>&gt;&gt; conscious response.
</em><br>
<em>&gt;&gt; Following it, however, there is also, typically, a later brain  
</em><br>
<em>&gt;&gt; response called the P3. This is exactly the large-scale global  
</em><br>
<em>&gt;&gt; response that we found in our previous experiments, that must be  
</em><br>
<em>&gt;&gt; specifically associated with consciousness.
</em><br>
<em>&gt;&gt;  (snip)
</em><br>
<em>&gt;&gt; The P3 response (a marker is absent in coma patients. It is also  
</em><br>
<em>&gt;&gt; gone in most vegetative state patients â€” but it remains present in 
</em><br>
<em>&gt;&gt;  most minimally conscious patients. It is always present in locked 
</em><br>
<em>&gt;&gt; -in patients and in any other conscious subject.&quot;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Consciousness, according to Dr. Dehaene's findings, is how the  
</em><br>
<em>&gt;&gt; human brain gets around certain limitations of being an analog  
</em><br>
<em>&gt;&gt; computer. If you've read Eliezer Yudkowsky's posts you'll know that  
</em><br>
<em>&gt;&gt; his approach to AGI would not necessarily call for the AGI to be  
</em><br>
<em>&gt;&gt; conscious in the sense we understand. I recall he said &quot;I'm not  
</em><br>
<em>&gt;&gt; looking for the AGI to be a new drinking buddy, at least not at  
</em><br>
<em>&gt;&gt; first&quot; or words close to that. Paramount, to him, is that the AGI  
</em><br>
<em>&gt;&gt; be Friendly, and not damage us intentionally or otherwise. While  
</em><br>
<em>&gt;&gt; the human brain is a kind of analog computer, and much research is  
</em><br>
<em>&gt;&gt; now afoot to emulate it on digital computers, our minds are not  
</em><br>
<em>&gt;&gt; exactly computer programs. They are certainly not fungible programs  
</em><br>
<em>&gt;&gt; running on a general computing machine, but rather embedded in the  
</em><br>
<em>&gt;&gt; structure. The mind is not fungible unless the neural structure is  
</em><br>
<em>&gt;&gt; made fungible, which may or may not ever be possible.
</em><br>
<em>&gt;&gt; To sum up, there's no real-world way a zombie could react as if  
</em><br>
<em>&gt;&gt; conscious, using human brain architecture, without being conscious.  
</em><br>
<em>&gt;&gt; Unless you believe in magic. And the subject of zombies, even if  
</em><br>
<em>&gt;&gt; such could exist, probably doesn't really apply to the problems of  
</em><br>
<em>&gt;&gt; building an AGI.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Tom Buckner
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="20941.html">Stathis Papaioannou: "Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<li><strong>Previous message:</strong> <a href="20939.html">Matt Mahoney: "Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<li><strong>In reply to:</strong> <a href="20932.html">Matt Mahoney: "Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20901.html">Stathis Papaioannou: "Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20940">[ date ]</a>
<a href="index.html#20940">[ thread ]</a>
<a href="subject.html#20940">[ subject ]</a>
<a href="author.html#20940">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:05 MDT
</em></small></p>
</body>
</html>
