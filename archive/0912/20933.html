<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))</title>
<meta name="Author" content="Luke (wlgriffiths@gmail.com)">
<meta name="Subject" content="Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))">
<meta name="Date" content="2009-12-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))</h1>
<!-- received="Mon Dec  7 13:56:55 2009" -->
<!-- isoreceived="20091207205655" -->
<!-- sent="Mon, 7 Dec 2009 15:56:22 -0500" -->
<!-- isosent="20091207205622" -->
<!-- name="Luke" -->
<!-- email="wlgriffiths@gmail.com" -->
<!-- subject="Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))" -->
<!-- id="12902e900912071256q562f253bq85a1b39d8589e50@mail.gmail.com" -->
<!-- charset="windows-1252" -->
<!-- inreplyto="814549.21992.qm@web51908.mail.re2.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Luke (<a href="mailto:wlgriffiths@gmail.com?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))"><em>wlgriffiths@gmail.com</em></a>)<br>
<strong>Date:</strong> Mon Dec 07 2009 - 13:56:22 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="20934.html">Matt Mahoney: "Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<li><strong>Previous message:</strong> <a href="20932.html">Matt Mahoney: "Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<li><strong>In reply to:</strong> <a href="20932.html">Matt Mahoney: "Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20934.html">Matt Mahoney: "Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<li><strong>Reply:</strong> <a href="20934.html">Matt Mahoney: "Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20933">[ date ]</a>
<a href="index.html#20933">[ thread ]</a>
<a href="subject.html#20933">[ subject ]</a>
<a href="author.html#20933">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&quot;If you got out, then you must believe in this nonexistent thing.&quot;
<br>
<p>Actually, in that situation you'd probably be primarily motivated by a
<br>
desire not to experience your flesh being burned.
<br>
<p>Even a zen master will duck if you throw a molotov cocktail at him.  That's
<br>
the nature of the flesh.
<br>
<p>More generally, re: this entire conversation.  Just accept the fact that if
<br>
you make a conscious copy of yourself you'll both feel the continuity.
<br>
<p>That continuity is itself an illusion.  The past doesn't exist, except in
<br>
memory.  &quot;In&quot; memory - not a spatial relationship.
<br>
<p>&nbsp;- Luke
<br>
<p>On Mon, Dec 7, 2009 at 3:09 PM, Matt Mahoney &lt;<a href="mailto:matmahoney@yahoo.com?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">matmahoney@yahoo.com</a>&gt; wrote:
<br>
<p><em>&gt; &quot;M.&gt;h&quot; wrote:
</em><br>
<em>&gt; &gt; ... sorry, but i do not get the whole problem.
</em><br>
<em>&gt;
</em><br>
<em>&gt; You understand that I can't define something which doesn't exist. When I
</em><br>
<em>&gt; say qualia or self awareness or that little person inside your mind that
</em><br>
<em>&gt; observes the world through your senses, most people know what I mean.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Let me put it this way. Consider the AI program that observed everything
</em><br>
<em>&gt; you did for the last several years until it became so good at predicting
</em><br>
<em>&gt; your behavior that none of your friends or relatives could distinguish it
</em><br>
<em>&gt; from you in a Turing test environment. Unfortunately the building containing
</em><br>
<em>&gt; the only copy of the program is on fire.  It is just you and the computer in
</em><br>
<em>&gt; a room rapidly filling with smoke. There is just enough time either for you
</em><br>
<em>&gt; to get out, allowing the only copy to be destroyed, or for you to upload a
</em><br>
<em>&gt; copy of the program to a remote site over the internet with your last dying
</em><br>
<em>&gt; breath. Which do you do?
</em><br>
<em>&gt;
</em><br>
<em>&gt; If you got out, then you must believe in this nonexistent thing. Otherwise
</em><br>
<em>&gt; you would logically conclude that by preserving your memories in a form that
</em><br>
<em>&gt; can be backed up, that you become immortal. Furthermore, you have the
</em><br>
<em>&gt; opportunity to enhance your intelligence and your environment by running on
</em><br>
<em>&gt; more powerful computers and embodied in better robots in the future. Why
</em><br>
<em>&gt; would you ever allow one copy to be destroyed now and the only other copy in
</em><br>
<em>&gt; a few decades?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Sorry for my ambiguous use of the word &quot;you&quot;.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; -- Matt Mahoney, <a href="mailto:matmahoney@yahoo.com?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">matmahoney@yahoo.com</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; ------------------------------
</em><br>
<em>&gt; *From:* &quot;M.&gt;h&quot; &lt;<a href="mailto:m.transhumanist@gmail.com?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">m.transhumanist@gmail.com</a>&gt;
</em><br>
<em>&gt; *To:* &quot;<a href="mailto:sl4@sl4.org?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">sl4@sl4.org</a>&quot; &lt;<a href="mailto:sl4@sl4.org?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">sl4@sl4.org</a>&gt;
</em><br>
<em>&gt; *Sent:* Mon, December 7, 2009 1:58:00 AM
</em><br>
<em>&gt;
</em><br>
<em>&gt; *Subject:* Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of
</em><br>
<em>&gt; AI))
</em><br>
<em>&gt;
</em><br>
<em>&gt; ... sorry, but i do not get the whole problem. even if a clone of me would
</em><br>
<em>&gt; walk up to me right here and now, having sufficiently enough of my memories
</em><br>
<em>&gt; and claiming to have my 'identity', i would not care if this 'double' would
</em><br>
<em>&gt; not use my resources (e.g. credit card) and bureaucrats would leave me
</em><br>
<em>&gt; alone!
</em><br>
<em>&gt;
</em><br>
<em>&gt; cheers,
</em><br>
<em>&gt;
</em><br>
<em>&gt; miriam
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Am 06.12.2009 um 22:12 schrieb Thomas Buckner &lt;<a href="mailto:tcbevolver@yahoo.com?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">tcbevolver@yahoo.com</a>&gt;:
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; ------------------------------
</em><br>
<em>&gt; *From:* Matt Mahoney &lt;<a href="mailto:matmahoney@yahoo.com?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">matmahoney@yahoo.com</a>&gt;
</em><br>
<em>&gt; *To:* &lt;<a href="mailto:sl4@sl4.org?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">sl4@sl4.org</a>&gt;<a href="mailto:sl4@sl4.org?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">sl4@sl4.org</a>
</em><br>
<em>&gt; *Sent:* Sun, December 6, 2009 2:44:53 PM
</em><br>
<em>&gt; *Subject:* Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of
</em><br>
<em>&gt; AI))
</em><br>
<em>&gt;
</em><br>
<em>&gt; Rewot Fetterkey wrote:
</em><br>
<em>&gt; &gt; Can you clarify that? How, exactly, is consciousness nonexistent?
</em><br>
<em>&gt;
</em><br>
<em>&gt; By consciousness, I mean that which makes you different from a
</em><br>
<em>&gt; philosophical zombie as described in &lt;<a href="http://en.wikipedia.org/wiki/Philosophical_zombie">http://en.wikipedia.org/wiki/Philosophical_zombie</a>&gt;
</em><br>
<em>&gt; <a href="http://en.wikipedia.org/wiki/Philosophical_zombie">http://en.wikipedia.org/wiki/Philosophical_zombie</a>
</em><br>
<em>&gt; But by definition, a zombie is not distinguishable from you at all. I
</em><br>
<em>&gt; really don't know how much more clear the logic could be.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The problem arises because all animals, including those that have no
</em><br>
<em>&gt; concept of death, have evolved a fear of those things that can kill them.
</em><br>
<em>&gt; Humans do have such a concept, which we associate with a lack of conscious
</em><br>
<em>&gt; experience. So we all desperately want to preserve this thing that does not
</em><br>
<em>&gt; exist. We can't help it. We are programmed that way.
</em><br>
<em>&gt;
</em><br>
<em>&gt; One way to deal with this conflict is to argue that the zombie argument is
</em><br>
<em>&gt; wrong and create ever more convoluted arguments to refute it. My preferred
</em><br>
<em>&gt; approach is as follows:
</em><br>
<em>&gt;
</em><br>
<em>&gt; 1. I believe that I have conscious experience. (I am programmed to).
</em><br>
<em>&gt; 2. I know that conscious experience does not exist.. (Logic irrefutably
</em><br>
<em>&gt; says so).
</em><br>
<em>&gt; 3. I realize that 1 and 2 are inconsistent. I leave it at that.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; -- Matt Mahoney, &lt;<a href="mailto:matmahoney@yahoo.com?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">matmahoney@yahoo.com</a>&gt;<a href="mailto:matmahoney@yahoo.com?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">matmahoney@yahoo.com</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt; I'm with Daniel Dennet on this: the P-zombie is (to paraphrase an earlier
</em><br>
<em>&gt; poster) 2+2 = 5. Purely hypothetical, a character in a gendankenexperiment,
</em><br>
<em>&gt; The Man Who Wasn't There. In practice, any creature with a human brain that
</em><br>
<em>&gt; could say &quot;Ouch, that hurt&quot; has an internal process isomorphic to what we
</em><br>
<em>&gt; experience as consciousness. Please see my post of a few hours ago on the
</em><br>
<em>&gt; Edge talk.  &lt;<a href="http://www.edge.org/3rd_culture/dehaene09/dehaene09_index.html">http://www.edge.org/3rd_culture/dehaene09/dehaene09_index.html</a>&gt;
</em><br>
<em>&gt; <a href="http://www.edge.org/3rd_culture/dehaene09/dehaene09_index.html">http://www.edge.org/3rd_culture/dehaene09/dehaene09_index.html</a>
</em><br>
<em>&gt; Consciousness in the human brain is a global pattern of activation and we
</em><br>
<em>&gt; now have methods of scanning and can say whether that pattern appears or
</em><br>
<em>&gt; not. This scanning has been applied to comatose/vegetative patients. From
</em><br>
<em>&gt; Dr. Dehaene's talk:
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;Let me just give you a very basic idea about the test. We stimulate the
</em><br>
<em>&gt; patient with five tones. The first four tones are identical, but the fifth
</em><br>
<em>&gt; can be different. So you hear something like dit-dit-dit-dit-tat. When you
</em><br>
<em>&gt; do this, a very banal observation, dating back 25 years, is that the brain
</em><br>
<em>&gt; reacts to the different tone at the end. That reaction, which is called
</em><br>
<em>&gt; mismatch negativity, is completely automatic. You get it even in coma, in
</em><br>
<em>&gt; sleep, or when you do not attend to the stimulus. It's a non-conscious
</em><br>
<em>&gt; response.
</em><br>
<em>&gt; Following it, however, there is also, typically, a later brain response
</em><br>
<em>&gt; called the P3. This is exactly the large-scale global response that we found
</em><br>
<em>&gt; in our previous experiments, that must be specifically associated with
</em><br>
<em>&gt; consciousness.
</em><br>
<em>&gt;  (snip)
</em><br>
<em>&gt; The P3 response (a marker is absent in coma patients. It is also gone in
</em><br>
<em>&gt; most vegetative state patients — but it remains present in most minimally
</em><br>
<em>&gt; conscious patients. It is always present in locked-in patients and in any
</em><br>
<em>&gt; other conscious subject.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Consciousness, according to Dr. Dehaene's findings, is how the human brain
</em><br>
<em>&gt; gets around certain limitations of being an analog computer. If you've read
</em><br>
<em>&gt; Eliezer Yudkowsky's posts you'll know that his approach to AGI would not
</em><br>
<em>&gt; necessarily call for the AGI to be conscious in the sense we understand. I
</em><br>
<em>&gt; recall he said &quot;I'm not looking for the AGI to be a new drinking buddy, at
</em><br>
<em>&gt; least not at first&quot; or words close to that. Paramount, to him, is that the
</em><br>
<em>&gt; AGI be Friendly, and not damage us intentionally or otherwise. While the
</em><br>
<em>&gt; human brain is a kind of analog computer, and much research is now afoot to
</em><br>
<em>&gt; emulate it on digital computers, our minds are not exactly computer
</em><br>
<em>&gt; programs. They are certainly not fungible programs running on a general
</em><br>
<em>&gt; computing machine, but rather embedded in the structure. The mind is not
</em><br>
<em>&gt; fungible unless the neural structure is made fungible, which may or may not
</em><br>
<em>&gt; ever be possible.
</em><br>
<em>&gt; To sum up, there's no real-world way a zombie could react as if conscious,
</em><br>
<em>&gt; using human brain architecture, without being conscious. Unless you believe
</em><br>
<em>&gt; in magic. And the subject of zombies, even if such could exist, probably
</em><br>
<em>&gt; doesn't really apply to the problems of building an AGI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Tom Buckner
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="20934.html">Matt Mahoney: "Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<li><strong>Previous message:</strong> <a href="20932.html">Matt Mahoney: "Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<li><strong>In reply to:</strong> <a href="20932.html">Matt Mahoney: "Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20934.html">Matt Mahoney: "Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<li><strong>Reply:</strong> <a href="20934.html">Matt Mahoney: "Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20933">[ date ]</a>
<a href="index.html#20933">[ thread ]</a>
<a href="subject.html#20933">[ subject ]</a>
<a href="author.html#20933">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:05 MDT
</em></small></p>
</body>
</html>
