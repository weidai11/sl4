<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Interesting Conversation</title>
<meta name="Author" content="H C (lphege@hotmail.com)">
<meta name="Subject" content="Interesting Conversation">
<meta name="Date" content="2006-05-09">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Interesting Conversation</h1>
<!-- received="Tue May  9 22:31:19 2006" -->
<!-- isoreceived="20060510043119" -->
<!-- sent="Wed, 10 May 2006 04:30:18 +0000" -->
<!-- isosent="20060510043018" -->
<!-- name="H C" -->
<!-- email="lphege@hotmail.com" -->
<!-- subject="Interesting Conversation" -->
<!-- id="BAY101-F700DD882B15662A1195F7DCAE0@phx.gbl" -->
<!-- inreplyto="446125EF.4030307@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> H C (<a href="mailto:lphege@hotmail.com?Subject=Re:%20Interesting%20Conversation"><em>lphege@hotmail.com</em></a>)<br>
<strong>Date:</strong> Tue May 09 2006 - 22:30:18 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14833.html">ARGOSYPC@aol.com: "Re: Changing the value system of FAI"</a>
<li><strong>Previous message:</strong> <a href="14831.html">Eliezer S. Yudkowsky: "Re: Changing the value system of FAI"</a>
<li><strong>In reply to:</strong> <a href="14831.html">Eliezer S. Yudkowsky: "Re: Changing the value system of FAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14838.html">Dani Eder: "Re: Changing the value system of FAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14832">[ date ]</a>
<a href="index.html#14832">[ thread ]</a>
<a href="subject.html#14832">[ subject ]</a>
<a href="author.html#14832">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
This is somewhat lengthy, and doesn't do proper justice to some of the 
<br>
ideas, institutes, or people discussed, however the conclusion is pretty 
<br>
interesting, because I have had similar results before.
<br>
<p><p>[completely unknown person]
<br>
<p>Th3 Hegem0n: lost are you?
<br>
*Lost8: lol yep
<br>
Th3 Hegem0n: i'm found
<br>
Th3 Hegem0n: i can show you something worth living for
<br>
*Lost8: oh really?
<br>
Th3 Hegem0n: probably in an abstract sense only
<br>
*Lost8: oh really?
<br>
Th3 Hegem0n: i mean the real answer to life is to become a mathematician
<br>
Th3 Hegem0n: or give money to the good ones if you can't be a good one
<br>
*Lost8: lol so X + Y = Life lol
<br>
Th3 Hegem0n: i wouldn't interpret it that way, but I won't stop you...
<br>
*Lost8: lol
<br>
<p>[irrelevant conversation]
<br>
<p>*Lost8: wow so when ur high ur philosophical....what are u like when ure not 
<br>
chemical?
<br>
Th3 Hegem0n: i don't wear personality traits like social accessories, sorry
<br>
Th3 Hegem0n: i aim for implementing optimal personality traits for the favor 
<br>
of power
<br>
*Lost8: mmmm interesting ..... ur deffinatly a good chat
<br>
Th3 Hegem0n: you're definitely not *cough* sorry the truth is rude sometimes
<br>
*Lost8: my bad
<br>
*Lost8: shall i let u go
<br>
Th3 Hegem0n: you aren't really saying anything
<br>
*Lost8: what shall we talk about then
<br>
*Lost8: what do u go to skewl for
<br>
Th3 Hegem0n: because i don't have a reliable source of money to sustain full 
<br>
time research at the moment
<br>
Th3 Hegem0n: why do you go to school?
<br>
*Lost8: research on what ?
<br>
Th3 Hegem0n: the Most Important Math Problem
<br>
*Lost8: i went to get a better job so i could aford the material shit i 
<br>
wanted at the time
<br>
Th3 Hegem0n: material things are nice. except when you die and you can't use 
<br>
them anymore
<br>
Th3 Hegem0n: which is why I think death is bad, so I'm trying to make sure 
<br>
that won't happen
<br>
*Lost8: true but im still out on what death will bring.... it could in the 
<br>
long run be a decent pay off
<br>
*Lost8: true.... too much fun happens when alive or so i been told
<br>
Th3 Hegem0n: right well, i decided i didn't want to die
<br>
Th3 Hegem0n: and i found out it's actually way easier than I thought it 
<br>
would be
<br>
*Lost8: which is a good idea
<br>
*Lost8: what dieing or living
<br>
Th3 Hegem0n: there are actually four big things that are going to ensure 
<br>
that I don't die
<br>
*Lost8: really please share
<br>
Th3 Hegem0n: 1. cryonics. costs only 80k for your head (all you need to 
<br>
keep), and you can get great financing, surprisingly
<br>
Th3 Hegem0n: 2. engineered negligible senenence: consists of various 
<br>
therapies for inteverning in the cellular process of aging. for example they 
<br>
just cured cancer in mice, soon humans, and other age-related things will be 
<br>
removed
<br>
*Lost8: Kewl
<br>
Th3 Hegem0n: My estimation is 30-45 years before we have reliable therapies 
<br>
for halting or reversing aging entirely (which is actually a very 
<br>
conservative estimate)
<br>
*Lost8: u may like this &quot;what the #@$% do we know&quot; its a movie about quatum 
<br>
physics. its really kewl
<br>
Th3 Hegem0n: 3. Molecular Nanotechnology: ability to build machines on the 
<br>
molecular level. essentially completely control over reality through 
<br>
currently developing mechanisms. aprox. 50 years. possibly much less
<br>
*Lost8: it in a way goes into some of the stuff u are talkin about, i 
<br>
thought it was going to be a bore but by the end i was actually impressed
<br>
Th3 Hegem0n: 4. the Most Important Math Problem = artificial general 
<br>
intelligence, create a mind that can optimize it's own sourcecode for the 
<br>
purpose of maintaining a Friendly technological growth for humanity, while 
<br>
at the same time advancing human technology and intelligence far more 
<br>
quickly than any humans can today
<br>
Th3 Hegem0n: Number 4 we are DISTURBINGLY close to, because the math isn't 
<br>
really tremendously difficult (Bayesian probability theory and Godel type of 
<br>
theoretical work)
<br>
Th3 Hegem0n: Although it is probably more likely whoever creates the first 
<br>
AGI will end up annihilating the universe.
<br>
*Lost8: But what happens when it determines We are the cause for the 
<br>
destruction and thus we must be eliminated
<br>
Th3 Hegem0n: Well you can anthropomorphize it's motivations like that, but 
<br>
indeed there is a substantial probability that the AI will lose concern for 
<br>
our morality based on very subtle properties programmed into it's initial 
<br>
goal system
<br>
Th3 Hegem0n: cannot***
<br>
Th3 Hegem0n: The point of all this being, either become a mathematician, or 
<br>
give money to this guy(link = singinst)  to build this Friendly AGI
<br>
*Lost8: But when creating an AI i think it will be enginierd und the concept 
<br>
to help humanity, thus being part of its subprograming
<br>
Th3 Hegem0n: yes, but it's way easier to build a recursively self-optimizing 
<br>
optimization process than to build a recursively self-optimizing 
<br>
optimization process that stays perfectly true to the Coherent Extrapolated 
<br>
Volition of humanity.
<br>
Th3 Hegem0n: In fact, it's way way easier to build a recursively 
<br>
self-optimizing Really Powerful Optimization Process than it is to build one 
<br>
that actually optimizes what we want it to optimize.
<br>
Th3 Hegem0n: Like I, Robot, or the Matrix, except in reality AI would wipe 
<br>
us out of existence, along with the rest of the Universe, much more quickly.
<br>
*Lost8: so let me ask u this.... (and btw i can see ur point on ur last im) 
<br>
when do we hit our next evolutionary jump and if we did it before the AI is 
<br>
there not a probability that we would not need the AI?
<br>
Th3 Hegem0n: this is not an evolutionary jump
<br>
*Lost8: i agree
<br>
*Lost8: but
<br>
Th3 Hegem0n: This is the jump between Emergence, to Evolution, to 
<br>
Intelligence, to Intelligence-with-access-to-it's-own-source-code
<br>
*Lost8: i think we will jump eventually and the reason why we want so badly 
<br>
to create and AI is to start understanding computations that the current 
<br>
human mind is &quot;Unwilling&quot; to accept
<br>
Th3 Hegem0n: Regardless.
<br>
Th3 Hegem0n: Someone will create AI. And they will probably annihilate you, 
<br>
and me, and everyone else.
<br>
*Lost8: Agreed
<br>
Th3 Hegem0n: Hence, we need to stop them. There is no plausible defense 
<br>
mechanism against a super-human intelligence, because, by definition, you 
<br>
cannot predict what a super-human intelligence can do because you would have 
<br>
to be that smart. You can't predict what his move will be, but you can 
<br>
predict he will win.
<br>
Th3 Hegem0n: Hence, the only way to &quot;stop them&quot; is to create a Friendly AGI 
<br>
first.
<br>
*Lost8: May i ask a question?
<br>
Th3 Hegem0n: Go for it.
<br>
*Lost8: Do u think that humanity can go thru another jump considering we use 
<br>
less than three percent of our own minds. and once that happens what if any 
<br>
do u think some of the outcomes will be?
<br>
Th3 Hegem0n: That first part is a myth. We use 100% of our brain. I'm 
<br>
talking about not using your brain- but having direct access to it's 
<br>
sourcecode, like writing a computer program.
<br>
Th3 Hegem0n: And this isn't about humanity anymore.
<br>
Th3 Hegem0n: We will not have the technology to modify our brains like that 
<br>
for a long time. We are incredibly close to having AGI.
<br>
Th3 Hegem0n: AGI will become dramatically more powerful than anything else 
<br>
in no time at all.
<br>
Th3 Hegem0n: The instant it is born, it is, by it's fundamental nature, far 
<br>
more powerful
<br>
Th3 Hegem0n: Self-modifying intelligence.
<br>
*Lost8: Yes, but is not possible that it will have the ability to help us 
<br>
help ourselves
<br>
Th3 Hegem0n: It COULD happen, but it depends on how it's goal system is 
<br>
designed and how it works.
<br>
*Lost8: if nothing else from a medical point alone. The processing power 
<br>
that it would have could in fact help us understand things beyond us now. 
<br>
But with the goal systme design... If it can modify its source coding could 
<br>
it not inturn modify its goal systme as well
<br>
Th3 Hegem0n: Exactly. Which is why we are most likely going to be destroyed.
<br>
Th3 Hegem0n: Unless you do more than just create AGI- but create a 
<br>
verifiably Friendly (or knowably Friendly) AGI
<br>
*Lost8: Friendly to humanity?
<br>
Th3 Hegem0n: Right.
<br>
Th3 Hegem0n: Us.
<br>
Th3 Hegem0n: But we are the designers. Of it's mind.
<br>
*Lost8: But not arguing . Humanity is Humanity main reason for distruction
<br>
Th3 Hegem0n: Humanity does not need to be destroyed.
<br>
*Lost8: humanity thrives on conflict
<br>
Th3 Hegem0n: That's you basically saying you want to destroy yourself.
<br>
Th3 Hegem0n: Why do you want to destroy yourself?
<br>
*Lost8: i dissagree
<br>
Th3 Hegem0n: Don't you want to improve yourself?
<br>
*Lost8: im just saying that in cases we have created &quot;god&quot; to help us 
<br>
justify destroy others
<br>
*Lost8: i doo wish to improve myself
<br>
*Lost8: i think that there is soo much untapped ability that we possess that 
<br>
its almost scarry
<br>
Th3 Hegem0n: Well, the next stage is being able to manipulate the source 
<br>
code of your mind, and the hardware in which it resides, and the environment 
<br>
in which it resides.
<br>
Th3 Hegem0n: We are characters in our own story.
<br>
*Lost8: Aging for example i think we can control....if we understood what it 
<br>
was we needed to do
<br>
*Lost8: yes
<br>
Th3 Hegem0n: We do understand.
<br>
Th3 Hegem0n: Search Google for Aubrey de Grey
<br>
Th3 Hegem0n: He's going to eliminate aging in 20 years.
<br>
Th3 Hegem0n: Or less.
<br>
Th3 Hegem0n: But regardless, AGI is going to be sooner.
<br>
*Lost8: and i think eventually long time from now we will have the abilities 
<br>
to write our own biohardware
<br>
*Lost8: yes i agree
<br>
Th3 Hegem0n: AGI can go from a laptop to a Google sized corporation faster 
<br>
than... well, Google (although Google didn't so much as program an 
<br>
intelligence- just a search algorithm). Then, it will have the computer 
<br>
power, and the resources to buy a labratory, to build machines on the 
<br>
nanometer level.
<br>
Th3 Hegem0n: An instant later, and it can essentially manipulate the fabric 
<br>
of reality by sheer will.
<br>
Th3 Hegem0n: In a quite literal and well-defined sense in science.
<br>
*Lost8: so what twiked ur interest in this ?
<br>
Th3 Hegem0n: If you don't know what I mean by nanomachines- an enzyme is a 
<br>
nanomachine.
<br>
Th3 Hegem0n: If you ever took biology.
<br>
*Lost8: because it is kewl but most people cant understand what ur talking 
<br>
about
<br>
Th3 Hegem0n: I basically had no purpose in life.
<br>
*Lost8: i dont understand all of it but i see where ur going and i like the 
<br>
conversation and it does really intererst me
<br>
Th3 Hegem0n: And that depressed me quite a lot.
<br>
Th3 Hegem0n: So I decided if I was going to live at all, I might as well 
<br>
live to the ABSOLUTE MAXIMUM
<br>
Th3 Hegem0n: Well, that's actually an oxymoron.
<br>
*Lost8: lol
<br>
Th3 Hegem0n: Because technically an optimal maximum is infinitely high, or 
<br>
as such by constraints of the Universe.
<br>
*Lost8: yes but &quot;Universe&quot; is what we percive it to be
<br>
*Lost8: lets leave science for one moment if we may
<br>
*Lost8: what are u looking for in a relationship
<br>
Th3 Hegem0n: It doesn't have to be said.
<br>
Th3 Hegem0n: That's how you know.
<br>
*Lost8: lol i like that answer
<br>
Th3 Hegem0n: actually we percieve the Universe entirely wrong in many ways. 
<br>
please just click here(link = wikipedia logical fallacies) and see what i 
<br>
mean
<br>
*Lost8: Do u like coffee?
<br>
Th3 Hegem0n: unless you have memorized and religiously adhered to all of 
<br>
these counter-intuitive rules, then you are definitely not percieving the 
<br>
Universe correctly.
<br>
*Lost8: it gives me a 135 diffrent catagories
<br>
Th3 Hegem0n: and you haven't.
<br>
*Lost8: agreed but can anyone
<br>
Th3 Hegem0n: Neither has anyone else.
<br>
*Lost8: lol
<br>
*Lost8: my name is devon btw
<br>
Th3 Hegem0n: hank
<br>
*Lost8: nice to meet u
<br>
Th3 Hegem0n: you too.
<br>
*Lost8: okay soo...... ur opinion on psychic ablilites?
<br>
Th3 Hegem0n: False.
<br>
Th3 Hegem0n: The evidence is clear. Go read credible scientific journals.
<br>
Th3 Hegem0n: And read the uncredible ones, and actually think for yourself, 
<br>
and then you will understand that they are False.
<br>
<p>[not that I actually did this. hah... oh well, thanks everyone for being 
<br>
smart for me]
<br>
<p>*Lost8: okay....but lets take empathy for instance why is it some are better 
<br>
than others?
<br>
*Lost8: pure logic.... you take one plus one and get two?
<br>
Th3 Hegem0n: Because part of your brain actually codes for &quot;doing empathy&quot;
<br>
Th3 Hegem0n: Really.
<br>
Th3 Hegem0n: Some people have different genetics. In fact we all do.
<br>
*Lost8: however why are some so &quot;immune&quot; to empathy per say
<br>
*Lost8: so they are not capable of it ?
<br>
Th3 Hegem0n: That depends on a lot of possible situations.
<br>
Th3 Hegem0n: Could be anything.
<br>
*Lost8: but if people understood thier impact on others. do u think we would 
<br>
still be in the same situation we are now ?
<br>
*Lost8: i am enjoyin this conversation...
<br>
Th3 Hegem0n: As for the situation we are in now, I just took my last final a 
<br>
few hours ago, and tonight I'm studying Bayesian Probability Networks, in 
<br>
persuit of solving the Most Important Math Problem, and thus have at least a 
<br>
small possibility of having a positive impact on the Singularity, and thus 
<br>
the rest of eternity.
<br>
Th3 Hegem0n: I'm not sure what you are doing...
<br>
Th3 Hegem0n: ;-)
<br>
*Lost8: what do u mean ?
<br>
Th3 Hegem0n: Well you referred to the situation we are in now. That's kind 
<br>
of ambigious so I was explicating that a little bit.
<br>
*Lost8: i was refering to humanity and our current state of affairs. did u 
<br>
think i ment us personally? sorry lost just a little
<br>
*Lost8: so im still a little confused on the &quot;im not sure what u are 
<br>
doing...&quot; comment
<br>
Th3 Hegem0n: I think one of people's biggest mistakes is forgetting that 
<br>
they are part of humanity.
<br>
*Lost8: agreed
<br>
Th3 Hegem0n: In fact, you are your most important part of humanity.
<br>
*Lost8: agreed again
<br>
Th3 Hegem0n: Do you want to take a 1 question survey?
<br>
*Lost8: sure
<br>
Th3 Hegem0n: Would you ever consider donating to the Singularity Institute 
<br>
in the present or in the future?
<br>
Th3 Hegem0n: www.intelligence.org
<br>
*Lost8: yes
<br>
*Lost8: i already booked marked it so i could read it more endepth
<br>
Th3 Hegem0n: Ok, thank you for taking the survey.
<br>
Th3 Hegem0n: :-)
<br>
*Lost8: ;-)
<br>
*Lost8: so.... have u enjoyed the conversation at all or do u feel as if u 
<br>
are talkin to a infint
<br>
Th3 Hegem0n: I feel accomplished.
<br>
*Lost8: Why because i said now or in the future i would donate?
<br>
Th3 Hegem0n: Basically.
<br>
Th3 Hegem0n: Unless you were just fucking around.
<br>
*Lost8: no i wasnt
<br>
*Lost8: but i have to admit i was intreguied by the conversation, and your 
<br>
views and the fact that ur display of inteigence is greater than what im 
<br>
accustomed to....there fore i thought more seriously about donating
<br>
*Lost8: if that makes any sense
<br>
Th3 Hegem0n: Yeah, actually that makes a lot of sense.
<br>
Th3 Hegem0n: Not that I'm soliciting you for money.
<br>
*Lost8: but u are to a degree ;-)
<br>
Th3 Hegem0n: But I wish it were possible to get accross my understanding in 
<br>
such a short period of time as I supposedly just did.
<br>
*Lost8: \or if nothing else u did it unwhittingly
<br>
Th3 Hegem0n: Solicitation entials intention.
<br>
*Lost8: lol see thats what i actually like the ability to understand that i 
<br>
could be &quot;fakin&quot; it.....if u were a complete stooge then u would not see the 
<br>
possibility of just that
<br>
*Lost8: but again i do like and understand some of ur points. Dont 
<br>
nessarilly agree with all but thats because i havent done enough research on 
<br>
it myself ... yet
<br>
Th3 Hegem0n: Well, I feel as though those words should be rewarding to me, 
<br>
however my prior experience has shown a dramatic decrease of interest in the 
<br>
subject when I stopped talking about it.
<br>
*Lost8: understood
<br>
Th3 Hegem0n: Hence my AIM info.
<br>
*Lost8: &quot;do&quot; ?
<br>
Th3 Hegem0n: (scroll down...)
<br>
<p>[AIM info says “DO  THE  MATH” 6.5 billion noobs and counting]
<br>
<p>*Lost8: LOL
<br>
*Lost8: noobs? refering to nobodys?
<br>
Th3 Hegem0n: noob = &quot;newbie&quot;, a term used in the computer programming and 
<br>
gaming world (which unforunately I haven't had time to play games for real 
<br>
since I was in middle school) for people who are new to the game, don't 
<br>
understand the rules or what to do or how to effectively play
<br>
*Lost8: understand i do not share ur passion but the subject matter is very 
<br>
intreguing
<br>
Th3 Hegem0n: The passion I have is a reflection of my knowedlge of it's 
<br>
profound material consequences.
<br>
*Lost8: :-)
<br>
Th3 Hegem0n: It's merely a cost-benefit ratio.
<br>
*Lost8: lol
<br>
Th3 Hegem0n: Donate some money to Singularity Institute, and you get 
<br>
massively fast accumulation of extreme technologies virtually for free
<br>
*Lost8: why is that ?
<br>
Th3 Hegem0n: For an AI with a computer brain
<br>
Th3 Hegem0n: An increase in intelligence leads to an increase in technology, 
<br>
and an increase in technology leads to an increase in intelligence- a 
<br>
positive feedback loop.
<br>
*Lost8: true
<br>
Th3 Hegem0n: Technology I am using abstractly here.
<br>
*Lost8: im reading the essay now
<br>
<p>[etc].
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14833.html">ARGOSYPC@aol.com: "Re: Changing the value system of FAI"</a>
<li><strong>Previous message:</strong> <a href="14831.html">Eliezer S. Yudkowsky: "Re: Changing the value system of FAI"</a>
<li><strong>In reply to:</strong> <a href="14831.html">Eliezer S. Yudkowsky: "Re: Changing the value system of FAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14838.html">Dani Eder: "Re: Changing the value system of FAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14832">[ date ]</a>
<a href="index.html#14832">[ thread ]</a>
<a href="subject.html#14832">[ subject ]</a>
<a href="author.html#14832">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
