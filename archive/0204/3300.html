<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: [silk] The Future of Artificial Intelligence (fwd)</title>
<meta name="Author" content="Eugen Leitl (eugen@leitl.org)">
<meta name="Subject" content="[silk] The Future of Artificial Intelligence (fwd)">
<meta name="Date" content="2002-04-08">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>[silk] The Future of Artificial Intelligence (fwd)</h1>
<!-- received="Mon Apr 08 02:46:46 2002" -->
<!-- isoreceived="20020408084646" -->
<!-- sent="Mon, 8 Apr 2002 08:27:11 +0200 (CEST)" -->
<!-- isosent="20020408062711" -->
<!-- name="Eugen Leitl" -->
<!-- email="eugen@leitl.org" -->
<!-- subject="[silk] The Future of Artificial Intelligence (fwd)" -->
<!-- id="Pine.LNX.4.33.0204080827070.31078-100000@hydrogen.leitl.org" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eugen Leitl (<a href="mailto:eugen@leitl.org?Subject=Re:%20[silk]%20The%20Future%20of%20Artificial%20Intelligence%20(fwd)"><em>eugen@leitl.org</em></a>)<br>
<strong>Date:</strong> Mon Apr 08 2002 - 00:27:11 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3301.html">Ben Houston: "RE: PAPER: Levels of Organization in General Intelligence"</a>
<li><strong>Previous message:</strong> <a href="3299.html">Justin Corwin: "Santa's Workshop Details"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3300">[ date ]</a>
<a href="index.html#3300">[ thread ]</a>
<a href="subject.html#3300">[ subject ]</a>
<a href="author.html#3300">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
---------- Forwarded message ----------
<br>
Date: Mon, 08 Apr 2002 07:51:34 +0530
<br>
From: Udhay Shankar N &lt;<a href="mailto:udhay@pobox.com?Subject=Re:%20[silk]%20The%20Future%20of%20Artificial%20Intelligence%20(fwd)">udhay@pobox.com</a>&gt;
<br>
Reply-To: <a href="mailto:silklist@lists.vipul.net?Subject=Re:%20[silk]%20The%20Future%20of%20Artificial%20Intelligence%20(fwd)">silklist@lists.vipul.net</a>
<br>
To: <a href="mailto:silklist@lists.vipul.net?Subject=Re:%20[silk]%20The%20Future%20of%20Artificial%20Intelligence%20(fwd)">silklist@lists.vipul.net</a>
<br>
Subject: [silk] The Future of Artificial Intelligence
<br>
<p>The Future of Artificial Intelligence
<br>
<p>Courtesy of New Scientist Magazine
<br>
<p>Dr. Mark Humphrys
<br>
University of Edinburgh
<br>
<p>Artificial Intelligence (AI) is a perfect example of how sometimes science
<br>
moves more slowly than we would have predicted. In the first flush of
<br>
enthusiasm at the invention of computers it was believed that we now finally
<br>
had the tools with which to crack the problem of the mind, and within years
<br>
we would see a new race of intelligent machines. We are older and wiser now.
<br>
The first rush of enthusiasm is gone, the computers that impressed us so
<br>
much back then do not impress us now, and we are soberly settling down to
<br>
understand how hard the problems of AI really are.
<br>
<p>What is AI? In some sense it is engineering inspired by biology. We look at
<br>
animals, we look at humans and we want to be able to build machines that do
<br>
what they do. We want machines to be able to learn in the way that they
<br>
learn, to speak, to reason and eventually to have consciousness. AI is
<br>
engineering but, at this stage, is it also science? Is it, for example,
<br>
modeling in cognitive science? We would like to think that is both
<br>
engineering and science but the contributions that is has made to cognitive
<br>
science so far are perhaps weaker than the contributions that biology has
<br>
given to the engineering.
<br>
<p>The confused history of AI
<br>
<p>Looking back at the history of AI, we can see that perhaps it began at the
<br>
wrong end of the spectrum. If AI had been tackled logically, it would
<br>
perhaps have begun as an artificial biology, looking at living things and
<br>
saying &quot;Can we model these with machines?&quot;. The working hypothesis would
<br>
have been that living things are physical systems so let's try and see where
<br>
the modeling takes us and where it breaks down. Artificial biology would
<br>
look at the evolution of physical systems in general, development from
<br>
infant to adult, self-organization, complexity and so on. Then, as a
<br>
subfield of that, a sort of artificial zoology that looks at sensorimotor
<br>
behavior, vision and navigation, recognizing, avoiding and manipulating
<br>
objects, basic, pre-linguistic learning and planning, and the simplest forms
<br>
of internal representations of external objects. And finally, as a further
<br>
subfield of this, an artificial psychology that looks at human behavior
<br>
where we deal with abstract reasoning, language, speech and social culture,
<br>
and all those philosophical conundrums like consciousness, free will and so
<br>
forth.
<br>
<p>That would have been a logical progression and is what should have happened.
<br>
But what did happen was that what people thought of as intelligence was the
<br>
stuff that impresses us. Our peers are impressed by things like doing
<br>
complex mathematics and playing a good chess game. The ability to walk, in
<br>
contrast, doesn't impress anyone. You can't say to your friends, &quot;Look, I
<br>
can walk&quot;, because your friends can walk too.
<br>
<p>So all those problems that toddlers grapple with every day were seen as
<br>
unglamorous, boring, and probably pretty easy anyway. The really hard
<br>
problems, clearly, were things demanding abstract thought, like chess and
<br>
mathematical theorem proving. Everyone ignored the animal and went straight
<br>
to the human, and the adult human too, not even the child human. And this is
<br>
what `AI' has come to mean - artificial adult human intelligence. But what
<br>
has happened over the last 40-50 years - to the disappointment of all those
<br>
who made breathless predictions about where AI would go - is that things
<br>
such as playing chess have turned out to be incredibly easy for computers,
<br>
whereas learning to walk and learning to get around in the world without
<br>
falling over has proved to be unbelievably difficult.
<br>
<p>And it is not as if we can ignore the latter skills and just carry on with
<br>
human-level AI. It has proved very difficult to endow machines with `common
<br>
sense', emotions and those other intangibles which seem to drive much
<br>
intelligent human behavior, and it does seem that these may come more from
<br>
our long history of interactions with the world and other humans than from
<br>
any abstract reasoning and logical deduction. That is, the animal and child
<br>
levels may be the key to making really convincing, well-rounded forms of
<br>
intelligence, rather than the intelligence of chess-playing machines like
<br>
Deep Blue, which are too easy to dismiss as `mindless'.
<br>
<p>In retrospect, the new view makes sense. It took 3 billion years of
<br>
evolution to produce apes, and then only another 2 million years or so for
<br>
languages and all the things that we are impressed by to appear. That's
<br>
perhaps an indication that once you've got the mobile, tactile monkey, once
<br>
you've got the Homo erectus, those human skills can evolve fairly quickly.
<br>
It may be a fairly trivial matter for language and reasoning to evolve in a
<br>
creature which can already find its way around the world.
<br>
<p>The new AI, and the new optimism That's certainly what the history of AI has
<br>
served to bear out. As a result, there has been a revolution in the field
<br>
which goes by names such as Artificial Life (AL) and Adaptive Behavior,
<br>
trying to re-situate AI within the context of an artificial biology and
<br>
zoology (respectively). The basic philosophy is that we need much more
<br>
understanding of the animal substrates of human behavior before we can
<br>
fulfil the dreams of AI in replicating convincing well-rounded intelligence.
<br>
<p>(Incidentally, the reader should note that the terminology is in chaos, as
<br>
fields re-group and re-define themselves. For example, I work on artificial
<br>
zoology but describe myself casually as doing AI. This chaos can, however,
<br>
be seen as a healthy sign of a field which has not yet stabilized. Any young
<br>
scientist with imagination should realize that these are the kind of fields
<br>
to get into. Who wants to be in a field where everything was solved long
<br>
ago?)
<br>
<p>So AI is not dead, but re-grouping, and is still being driven, as always, by
<br>
testable scientific models. Discussions on philosophical questions, such as
<br>
`What is life?' or `What is intelligence?', change little over the years.
<br>
There have been numerous attempts, from Roger Penrose to Gerald Edelman, to
<br>
disprove AI (show that it is impossible) but none of these attempted
<br>
revolutions has yet gathered much momentum. This is not just because of lack
<br>
of agreement with their philosophical analysis (although there is plenty of
<br>
that), but also perhaps because they fail to provide an alternative paradigm
<br>
in which we can do science. Progress, as is normal in science, comes from
<br>
building things and running experiments, and the flow of new and strange
<br>
machines from AI laboratories is not remotely exhausted. On the contrary, it
<br>
has been recently invigorated by the new biological approach.
<br>
<p>In fact, the old optimism has even been resurrected. Professor Kevin Warwick
<br>
of the University of Reading has recently predicted that the new approach
<br>
will lead to human-level AI in our lifetimes. But I think we have learned
<br>
our lesson on that one. I, and many like me in new AI, imagine that this is
<br>
still Physics before Newton, that the field might have a good one or two
<br>
hundred years left to run. The reason is that there is no obvious way of
<br>
getting from here to there - to human-level intelligence from the rather
<br>
useless robots and brittle software programs that we have nowadays. A long
<br>
series of conceptual breakthroughs are needed, and this kind of thinking is
<br>
very difficult to timetable. What we are trying to do in the next generation
<br>
is essentially to find out what are the right questions to ask.
<br>
<p>It may never happen (but not for the reasons you think)
<br>
<p>I think that people who are worried about robots taking over the world
<br>
should go to a robotics conference and watch these things try to walk. They
<br>
fall over, bump into walls and end up with their legs thrashing or wheels
<br>
spinning in the air. I'm told that in this summer's Robotic Football
<br>
competition, the losing player scored all five goals - 2 against the
<br>
opposing robot, and 3 against himself. The winner presumably just fell over.
<br>
<p>Robots are more helpless than threatening. They are really quite sweet. I
<br>
was in the MIT robotics laboratory once looking at Cog, Rodney Brooks'
<br>
latest robot. Poor Cog has no legs. He is a sort of humanoid, a torso stuck
<br>
on a stand with arms, grippers, binocular vision and so on. I saw Cog on a
<br>
Sunday afternoon in a darkened laboratory when everyone had gone home and I
<br>
felt sorry for him which I know is mad. But it was Sunday afternoon and no
<br>
one was going to come and play with him. If you consider the gulf between
<br>
that and what most animals experience in their lives, surrounded by a tribe
<br>
of fellow infants and adults, growing up with parents who are constantly
<br>
with them and constantly stimulating them, then you understand the
<br>
incredibly limited kind of life that artificial systems have.
<br>
<p>The argument I am developing is that there may be limits to AI, not because
<br>
the hypothesis of `strong AI' is false, but for more mundane reasons. The
<br>
argument, which I develop further on my website, is that you can't expect to
<br>
build single isolated AI's, alone in laboratories, and get anywhere. Unless
<br>
the creatures can have the space in which to evolve a rich culture, with
<br>
repeated social interaction with things that are like them, you can't really
<br>
expect to get beyond a certain stage. If we work up from insects to dogs to
<br>
Homo erectus to humans, the AI project will I claim fall apart somewhere
<br>
around the Homo erectus stage because of our inability to provide them with
<br>
a real cultural environment. We cannot make millions of these things and
<br>
give them the living space in which to develop their own primitive
<br>
societies, language and cultures. We can't because the planet is already
<br>
full. That's the main argument, and the reason for the title of this talk.
<br>
<p>So what will happen?
<br>
<p>So what will happen? What will happen over the next thirty years is that
<br>
will see new types of animal-inspired machines that are more `messy' and
<br>
unpredictable than any we have seen before. These machines will change over
<br>
time as a result of their interactions with us and with the world. These
<br>
silent, pre-linguistic, animal-like machines will be nothing like humans but
<br>
they will gradually come to seem like a strange sort of animal. Machines
<br>
that learn, familiar to researchers in labs for many years, will finally
<br>
become mainstream and enter the public consciousness.
<br>
<p>What category of problems could animal-like machines address? The kind of
<br>
problems we are going to see this approach tackle will be problems that are
<br>
somewhat noise and error resistant and that do not demand abstract
<br>
reasoning. A special focus will be behavior that is easier to learn than to
<br>
articulate - most of us know how to walk but we couldn't possibly tell
<br>
anyone how we do it. Similarly with grasping objects and other such skills.
<br>
These things involve building neural networks, filling in state-spaces and
<br>
so on, and cannot be captured as a set of rules that we speak in language.
<br>
You must experience the dynamics of your own body in infancy and thrash
<br>
about until the changing internal numbers and weights start to converge on
<br>
the correct behavior. Different bodies mean different dynamics. And robots
<br>
that can learn to walk can learn other sensorimotor skills that we can
<br>
neither articulate nor perform ourselves.
<br>
<p>What are examples of these type of problems? Well, for example, there are
<br>
already autonomous lawnmowers that will wander around gardens all afternoon.
<br>
The next step might be autonomous vacuum cleaners inside the house (though
<br>
clutter and stairs present immediate problems for wheeled robots). There are
<br>
all sorts of other uses for artificial animals in areas where people find
<br>
jobs dangerous or tedious - land-mine clearance, toxic waste clearance,
<br>
farming, mining, demolition, finding objects and robotic exploration, for
<br>
example. Any jobs done currently or traditionally by animals would be a
<br>
focus. We are familiar already from the Mars Pathfinder and other examples
<br>
that we can send autonomous robots not only to inhospitable places, but also
<br>
send them there on cheap one-way `suicide' missions. (Of course, no machine
<br>
ever `dies', since we can restore its mind in a new body on earth after the
<br>
mission.)
<br>
<p>Whether these type of machines may have a future in the home is an
<br>
interesting question. If it ever happens, I think it will be because the
<br>
robot is treated as a kind of pet, so that a machine roaming the house is
<br>
regarded as cute rather than creepy. Machines that learn tend to develop an
<br>
individual, unrepeatable character which humans can find quite attractive.
<br>
There are already a few games in software - such as the Windows-based game
<br>
Creatures, and the little Tamagotchi toys - whose personalities people can
<br>
get very attached to. A major part of the appeal is the unique, fragile and
<br>
unrepeatable nature of the software beings you interact with. If your
<br>
Creature dies, you may never be able to raise another one like it again.
<br>
Machines in the future will be similar, and the family robot will after a
<br>
few years be, like a pet, literally irreplaceable.
<br>
<p>What will hold things up? There are many things that could hold up progress
<br>
but hardware is the one that is staring us in the face at the moment. Nobody
<br>
is going to buy a robotic vacuum cleaner that costs £5000 no matter how many
<br>
big cute eyes are painted on it or even if it has a voice that says, &quot;I love
<br>
you&quot;. Many conceptual breakthroughs will be needed to create artificial
<br>
animals. The major theoretical issue to be solved is probably
<br>
representation: what is language and how do we classify the world. We say
<br>
`That's a table' and so on for different objects, but what does an insect
<br>
do, what is going on in an insect's head when it distinguishes objects in
<br>
the world, what information is being passed around inside, what kind of data
<br>
structures are they using. Each robot will have to learn an internal
<br>
language customized for its sensorimotor system and the particular
<br>
environmental niche in which it finds itself. It will have to learn this
<br>
internal language on its own, since any representations we attempt to impose
<br>
on it, coming from a different sensorimotor world, will probably not work.
<br>
<p>Predictions
<br>
<p>Finally, what will be the impact on society of animal-like machines? Let's
<br>
make a few predictions that I will later look back and laugh at.
<br>
<p>First, family robots may be permanently connected to wireless family
<br>
intranets, sharing information with those who you want to know where you
<br>
are. You may never need to worry if your loved ones are alright when they
<br>
are late or far away, because you will be permanently connected to them.
<br>
Crime may get difficult if all family homes are full of half-aware, loyal
<br>
family machines. In the future, we may never be entirely alone, and if the
<br>
controls are in the hands of our loved ones rather than the state, that may
<br>
not be such a bad thing.
<br>
<p>Slightly further ahead, if some of the intelligence of the horse can be put
<br>
back into the automobile, thousands of lives could be saved, as cars become
<br>
nervous of their drunk owners, and refuse to get into positions where they
<br>
would crash at high speed. We may look back in amazement at the carnage
<br>
tolerated in this age, when every western country had road deaths equivalent
<br>
to a long, slow-burning war. In the future, drunks will be able to use cars,
<br>
which will take them home like loyal horses. And not just drunks, but
<br>
children, the old and infirm, the blind, all will be empowered.
<br>
<p>Eventually, if cars were all (wireless) networked, and humans stopped
<br>
driving altogether, we might scrap the vast amount of clutter all over our
<br>
road system - signposts, markings, traffic lights, roundabouts, central
<br>
reservations - and return our roads to a soft, sparse, eighteenth-century
<br>
look. All the information - negotiation with other cars, traffic and route
<br>
updates - would come over the network invisibly. And our towns and
<br>
countryside would look so much sparser and more peaceful.
<br>
<p>Conclusion
<br>
<p>I've been trying to give an idea of how artificial animals could be useful,
<br>
but the reason that I'm interested in them is the hope that artificial
<br>
animals will provide the route to artificial humans. But the latter is not
<br>
going to happen in our lifetimes (and indeed may never happen, at least not
<br>
in any straightforward way).
<br>
<p>In the coming decades, we shouldn't expect that the human race will become
<br>
extinct and be replaced by robots. We can expect that classical AI will go
<br>
on producing more and more sophisticated applications in restricted
<br>
domains - expert systems, chess programs, Internet agents - but any time we
<br>
expect common sense we will continue to be disappointed as we have been in
<br>
the past. At vulnerable points these will continue to be exposed as `blind
<br>
automata'. Whereas animal-based AI or AL will go on producing stranger and
<br>
stranger machines, less rationally intelligent but more rounded and whole,
<br>
in which we will start to feel that there is somebody at home, in a strange
<br>
animal kind of way. In conclusion, we won't see full AI in our lives, but we
<br>
should live to get a good feel for whether or not it is possible, and how it
<br>
could be achieved by our descendants.
<br>
________________________________________________________________________
<br>
<p>&quot;Consider for a moment any beauty in the name Ralph.&quot;
<br>
-Frank Zappa, in an interview with Joan Rivers who had just asked him why 
<br>
he gave his
<br>
children such odd names.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3301.html">Ben Houston: "RE: PAPER: Levels of Organization in General Intelligence"</a>
<li><strong>Previous message:</strong> <a href="3299.html">Justin Corwin: "Santa's Workshop Details"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3300">[ date ]</a>
<a href="index.html#3300">[ thread ]</a>
<a href="subject.html#3300">[ subject ]</a>
<a href="author.html#3300">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:38 MDT
</em></small></p>
</body>
</html>
